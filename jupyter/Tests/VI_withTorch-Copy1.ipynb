{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2d8f72-376b-4f7b-b0e0-d1b70bafe8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "from matplotlib import pyplot as plt\n",
    "import GPE_ensemble as GPE\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from  torch.distributions import normal\n",
    "\n",
    "yobs = 8.0\n",
    "sigma2 = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd90672-e817-4cd8-8c99-611507c7f5b4",
   "metadata": {},
   "source": [
    "First we'll learn how to get the variational posterior for $x | y$ for a single value of y. We have\n",
    "$$y= 4x-x^2/2+N(0,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350d8cd-93ad-464d-aceb-d676326f8bd2",
   "metadata": {},
   "source": [
    "## Variational inference\n",
    "\n",
    "Maximize the ELBO\n",
    "$$ELBO(q) = E[\\log p(y | x)] − KL (q(x)|| p(x)).$$\n",
    "\n",
    "We'll use $q(x) = N(m, s^2)$ as the variational family\n",
    "In this case, the ELBO can be computed exactly, but its a pain to do. So I've approximated the E[log p(y | x)] with a Monte Carlo sum.\n",
    "$\\Phi=(m, \\log s^2)$ are the variational parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e7d49-6e46-4465-8d7f-297c6fd1edae",
   "metadata": {},
   "source": [
    "KL (q(x) || p(x)) = E[log q(x)] − E[log p(x)]\n",
    "where expectations are with respect to q.\n",
    "\n",
    "Note, if $p(x) \\propto 1$, then $E\\log p(x)$ does not depend on $\\phi$ so can be ignored\n",
    "If $q(x) =N(m, \\sigma^2)$, then $E[\\log q(x)] = -0.5 \\log(2 \\pi \\sigma^2) - 1/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525b9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p=3\n",
    "\n",
    "rl = -10\n",
    "ru=20\n",
    "\n",
    "obs_error = 0.001\n",
    "\n",
    "x=torch.linspace(rl,ru,p)\n",
    "\n",
    "b=0.5\n",
    "\n",
    "def lin(x):\n",
    "    y = b*x\n",
    "    return y\n",
    "\n",
    "y = lin(x) \n",
    "\n",
    "plt.plot(x[:,None],y[:,None],'o')\n",
    "\n",
    "emulator = GPE.ensemble(x[:,None],y[:,None],mean_func=\"constant\",training_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcf85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4739272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(m,log_s2,x,emulator,y,prior_mean,prior_cov,obs_error):\n",
    "    param=[m,log_s2]\n",
    "    L=torch.zeros((x.shape[0],x.shape[0]))\n",
    "    mu = torch.tensor((param[0:x.shape[0]]))\n",
    "    L=L.diagonal_scatter(torch.exp(torch.tensor(param[x.shape[0]:2*x.shape[0]])),0)\n",
    "    #L[1,0]=param[6]\n",
    "   # L[2,0]=param[7]\n",
    "   # L[2,1] =param[8]\n",
    "    covar = torch.matmul(L,L.T)\n",
    "    z=x*torch.exp(log_s2/2.)+m\n",
    "    \n",
    "    z=z.T\n",
    "    \n",
    "    mc_int = torch.sum(emulator.ensemble_log_likelihood_obs_error(z,y,obs_error)+torch.log(x_prior(z,prior_mean,prior_cov)).squeeze())\n",
    "        #mc_int +=-np.log(np.sum(((emulator.predict(z.iloc[[i]]).detach().numpy()-y.values)**2)))+np.log(x_prior(z.iloc[[i]],prior_mean,prior_cov))\n",
    "        #mc_int += (np.sum(np.log(emulator.ensemble_likelihood(z.iloc[[i]],y)))+np.log(x_prior(z.iloc[[i]],prior_mean,prior_cov)))\n",
    "    \n",
    "    lb = mc_int/x.shape[1] - q_prior(covar)\n",
    "    #print(mc_int/x.shape[1])\n",
    "    #print(-q_prior(covar))\n",
    "    #print(np.mean(z,axis=0))\n",
    "    #print(-lb)\n",
    "    return -lb\n",
    "\n",
    "def x_prior(x,mean,cov):\n",
    "\n",
    "    #var = scipy.stats.multivariate_normal(mean=mean, cov=cov)\n",
    "    #val1 = var.pdf(x)\n",
    "    dist = normal.Normal(loc=mean, scale=torch.sqrt(cov))\n",
    "    val = torch.exp(dist.log_prob(x))\n",
    "    return val\n",
    "\n",
    "def q_prior(covar):\n",
    "    qp = -(covar.shape[0]/2)*(1+torch.log(torch.tensor(2*torch.pi)))-0.5*torch.log(torch.linalg.det(covar))\n",
    "    return qp\n",
    "\n",
    "def f_likelihood(x,y,f,sigma2):\n",
    "    \n",
    "    likelihood_manual=-0.5*((f(x) - y)**2)/(sigma2)- 0.5*torch.log(2*np.pi)-0.5*torch.log(sigma2)\n",
    "    return likelihood_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa68f655-7ebf-4b2f-b77a-cb88572bcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(log_var):   \n",
    "    return(-0.5*np.log(2.0*math.pi)-0.5*log_var-0.5)\n",
    "\n",
    "def f(x):\n",
    "    return(4.*x-0.5*torch.pow(x,2.))\n",
    "# How do we define functions? x needs declaring?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ZtoX(m, log_s2):\n",
    "    #reparameterization trick\n",
    "    return(Z*torch.exp(log_s2/2.)+m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c5ab3e-19a1-4afb-bd58-5a6c8ed37629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_dnorm(x, mean, var):\n",
    "    return(-(x-mean).pow(2)/(2.*var)-0.5*np.log(2*math.pi*var))\n",
    "    # checked - could used built-in torch dnorm\n",
    "\n",
    "\n",
    "\n",
    "def Eloglike(m,log_s2):\n",
    "    X= ZtoX(m, log_s2)\n",
    "    \n",
    "    #print(X)\n",
    "    #Rewrite with f\n",
    "    loglikes = log_dnorm(f(X), yobs, sigma2) # observation likelihood\n",
    "    #-(yobs- 4.*X+0.5*X.pow(2)).pow(2)    #f(ZtoX(phi)),2.)\n",
    "    \n",
    "    return(loglikes.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db3cdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean=torch.tensor([0])\n",
    "prior_cov = torch.tensor([5])\n",
    "obs_error = 0.001\n",
    "y_cal = torch.tensor([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120e0a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 -1.8625075817108154 m= 5.586243629455566 s2= 0.432686984539032\n",
      "199 -1.6973435878753662 m= 5.673906326293945 s2= 0.233049213886261\n",
      "299 -1.4293887615203857 m= 5.672605991363525 s2= 0.15942224860191345\n",
      "399 -1.2216346263885498 m= 5.673786163330078 s2= 0.12142302840948105\n",
      "499 -1.0688527822494507 m= 5.672574043273926 s2= 0.09783735126256943\n",
      "599 -0.9157168865203857 m= 5.673327922821045 s2= 0.08193442225456238\n",
      "699 -0.7988768815994263 m= 5.674513816833496 s2= 0.07047479599714279\n",
      "799 -0.6766518354415894 m= 5.6742448806762695 s2= 0.061844710260629654\n",
      "899 -0.5798617601394653 m= 5.673141956329346 s2= 0.055114634335041046\n",
      "999 -0.4827641248703003 m= 5.670708656311035 s2= 0.049722831696271896\n",
      "1099 -0.4049488306045532 m= 5.6749491691589355 s2= 0.04528465121984482\n",
      "1199 -0.3211008310317993 m= 5.673573017120361 s2= 0.041534990072250366\n",
      "1299 -0.2508798837661743 m= 5.672875881195068 s2= 0.0383896604180336\n",
      "1399 -0.17959368228912354 m= 5.672743797302246 s2= 0.03569386899471283\n",
      "1499 -0.11806690692901611 m= 5.67447566986084 s2= 0.033332888036966324\n",
      "1599 -0.05440974235534668 m= 5.674718856811523 s2= 0.03125077486038208\n",
      "1699 -0.0009026527404785156 m= 5.674211025238037 s2= 0.02942822128534317\n",
      "1799 0.050957679748535156 m= 5.673460483551025 s2= 0.027804823592305183\n",
      "1899 0.10082435607910156 m= 5.673549652099609 s2= 0.02636738307774067\n",
      "1999 0.15243220329284668 m= 5.67431116104126 s2= 0.02505040355026722\n",
      "2099 0.19958257675170898 m= 5.6731791496276855 s2= 0.023861918598413467\n",
      "2199 0.23757481575012207 m= 5.674587726593018 s2= 0.022784437984228134\n",
      "2299 0.2821645736694336 m= 5.674602031707764 s2= 0.021798089146614075\n",
      "2399 0.3243134021759033 m= 5.6734209060668945 s2= 0.020898228511214256\n",
      "2499 0.3620457649230957 m= 5.674013137817383 s2= 0.02007165178656578\n",
      "2599 0.39866161346435547 m= 5.67329216003418 s2= 0.019304215908050537\n",
      "2699 0.4346315860748291 m= 5.673798561096191 s2= 0.018596617504954338\n",
      "2799 0.4711649417877197 m= 5.674774169921875 s2= 0.01793918013572693\n",
      "2899 0.5018925666809082 m= 5.674197673797607 s2= 0.017327284440398216\n",
      "2999 0.5397624969482422 m= 5.674050807952881 s2= 0.016750896349549294\n",
      "3099 0.5702779293060303 m= 5.674708843231201 s2= 0.01621275767683983\n",
      "3199 0.6021983623504639 m= 5.67425537109375 s2= 0.015704257413744926\n",
      "3299 0.6271820068359375 m= 5.674197196960449 s2= 0.015232320874929428\n",
      "3399 0.6591567993164062 m= 5.673704147338867 s2= 0.014786181971430779\n",
      "3499 0.6870729923248291 m= 5.673915863037109 s2= 0.01436611171811819\n",
      "3599 0.7123949527740479 m= 5.674243927001953 s2= 0.01396952848881483\n",
      "3699 0.7379026412963867 m= 5.672821044921875 s2= 0.013590727932751179\n",
      "3799 0.7692432403564453 m= 5.673821926116943 s2= 0.013237434439361095\n",
      "3899 0.7921807765960693 m= 5.674518585205078 s2= 0.012897578999400139\n",
      "3999 0.8156325817108154 m= 5.674481391906738 s2= 0.01257628109306097\n",
      "4099 0.8397138118743896 m= 5.673227787017822 s2= 0.01226694043725729\n",
      "4199 0.8645257949829102 m= 5.674282073974609 s2= 0.011975414119660854\n",
      "4299 0.887235164642334 m= 5.673048496246338 s2= 0.011697981506586075\n",
      "4399 0.9088807106018066 m= 5.673551559448242 s2= 0.011432873085141182\n",
      "4499 0.9320297241210938 m= 5.673544406890869 s2= 0.01118046697229147\n",
      "4599 0.952625036239624 m= 5.674816131591797 s2= 0.010936954990029335\n",
      "4699 0.9741735458374023 m= 5.6747894287109375 s2= 0.010706044733524323\n",
      "4799 0.9936561584472656 m= 5.67531156539917 s2= 0.010484118014574051\n",
      "4899 1.0143203735351562 m= 5.673409938812256 s2= 0.010270733386278152\n",
      "4999 1.0324175357818604 m= 5.6739630699157715 s2= 0.01006479561328888\n",
      "5099 1.0519826412200928 m= 5.674321174621582 s2= 0.009868877939879894\n",
      "5199 1.0731072425842285 m= 5.674402236938477 s2= 0.009679972194135189\n",
      "5299 1.092207670211792 m= 5.674326419830322 s2= 0.00949740782380104\n",
      "5399 1.109142541885376 m= 5.674071788787842 s2= 0.009322318248450756\n",
      "5499 1.1288261413574219 m= 5.673962593078613 s2= 0.009153680875897408\n",
      "5599 1.1456942558288574 m= 5.674681186676025 s2= 0.008990657515823841\n",
      "5699 1.1618859767913818 m= 5.674591064453125 s2= 0.008833826519548893\n",
      "5799 1.1805377006530762 m= 5.674029350280762 s2= 0.008681586012244225\n",
      "5899 1.1962077617645264 m= 5.673961639404297 s2= 0.008534186519682407\n",
      "5999 1.2110960483551025 m= 5.674552917480469 s2= 0.00839174259454012\n",
      "6099 1.22822904586792 m= 5.674295425415039 s2= 0.008255367167294025\n",
      "6199 1.244277000427246 m= 5.674284934997559 s2= 0.008122137747704983\n",
      "6299 1.2599201202392578 m= 5.6741766929626465 s2= 0.00799352116882801\n",
      "6399 1.2750937938690186 m= 5.673959732055664 s2= 0.007869033142924309\n",
      "6499 1.2901809215545654 m= 5.674045562744141 s2= 0.007748908828943968\n",
      "6599 1.3061769008636475 m= 5.673946857452393 s2= 0.007631512824445963\n",
      "6699 1.321058750152588 m= 5.674509048461914 s2= 0.007518185768276453\n",
      "6799 1.334181547164917 m= 5.6740851402282715 s2= 0.007408254314213991\n",
      "6899 1.3496079444885254 m= 5.674706935882568 s2= 0.007300937082618475\n",
      "6999 1.3645589351654053 m= 5.674670696258545 s2= 0.007197404280304909\n",
      "7099 1.3768103122711182 m= 5.674614906311035 s2= 0.007095383945852518\n",
      "7199 1.3914029598236084 m= 5.673645496368408 s2= 0.006996450945734978\n",
      "7299 1.4049561023712158 m= 5.673940181732178 s2= 0.0069006639532744884\n",
      "7399 1.417593002319336 m= 5.674383640289307 s2= 0.0068071396090090275\n",
      "7499 1.4306247234344482 m= 5.674770832061768 s2= 0.0067166793160140514\n",
      "7599 1.444603443145752 m= 5.6742448806762695 s2= 0.006628896575421095\n",
      "7699 1.4581398963928223 m= 5.6738080978393555 s2= 0.006543179042637348\n",
      "7799 1.4701294898986816 m= 5.673752307891846 s2= 0.0064590866677463055\n",
      "7899 1.4828801155090332 m= 5.674422740936279 s2= 0.006376957520842552\n",
      "7999 1.49485182762146 m= 5.6740522384643555 s2= 0.0062976377084851265\n",
      "8099 1.5069305896759033 m= 5.674931049346924 s2= 0.006220241542905569\n",
      "8199 1.5185749530792236 m= 5.674183368682861 s2= 0.006144681945443153\n",
      "8299 1.5310604572296143 m= 5.674179553985596 s2= 0.006071110721677542\n",
      "8399 1.5421221256256104 m= 5.673624038696289 s2= 0.005999467335641384\n",
      "8499 1.5532681941986084 m= 5.673816680908203 s2= 0.005929441656917334\n",
      "8599 1.566253662109375 m= 5.674032211303711 s2= 0.005861093755811453\n",
      "8699 1.5772037506103516 m= 5.67477560043335 s2= 0.005793646909296513\n",
      "8799 1.5883846282958984 m= 5.674049377441406 s2= 0.005728051997721195\n",
      "8899 1.599374532699585 m= 5.674591064453125 s2= 0.005663637537509203\n",
      "8999 1.6110203266143799 m= 5.674103260040283 s2= 0.005600777920335531\n",
      "9099 1.621983289718628 m= 5.674189567565918 s2= 0.005539223551750183\n",
      "9199 1.6329879760742188 m= 5.674323081970215 s2= 0.005478771403431892\n",
      "9299 1.642674207687378 m= 5.674333095550537 s2= 0.005419865716248751\n",
      "9399 1.65433931350708 m= 5.674217224121094 s2= 0.005362541414797306\n",
      "9499 1.664900541305542 m= 5.674067497253418 s2= 0.005306205712258816\n",
      "9599 1.6744847297668457 m= 5.674257755279541 s2= 0.005250719841569662\n",
      "9699 1.6840078830718994 m= 5.674520969390869 s2= 0.005196636542677879\n",
      "9799 1.6946907043457031 m= 5.674491882324219 s2= 0.005144238471984863\n",
      "9899 1.7049860954284668 m= 5.674004554748535 s2= 0.005092371255159378\n",
      "9999 1.7145500183105469 m= 5.673823356628418 s2= 0.005041493568569422\n",
      "Result: p(x|y) = N(5.6740031242370605, 0.00504101300612092) \n"
     ]
    }
   ],
   "source": [
    "### initialize the variational parameters\n",
    "m = torch.full((), 1.,dtype=dtype, requires_grad=True, device=device)\n",
    "log_s2 = torch.full((),torch.log(torch.tensor(3.)), requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "# Samples fixed here - but try adding them into the loop\n",
    "nsamples = 1000\n",
    "#Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for t in range(10000):\n",
    "    Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "    Z=Z[None,:]\n",
    "    #negELBO = -Eloglike(m,log_s2)+KL(log_s2)\n",
    "    \n",
    "    param = [m,log_s2]\n",
    "    \n",
    "    negELBO = ELBO(m,log_s2,Z,emulator,y_cal[:,None],prior_mean,prior_cov,obs_error)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, negELBO.item(), 'm=', m.item(), 's2=', log_s2.exp().item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    negELBO.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        m -= learning_rate * m.grad\n",
    "        log_s2 -= learning_rate * log_s2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        m.grad = None\n",
    "        log_s2.grad = None\n",
    "        \n",
    "print(f'Result: p(x|y) = N({m.item()}, {log_s2.exp().item()}) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf985d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8928]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emulator.predict(torch.tensor(1.859))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9484bd82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L=torch.zeros((Z.shape[0],Z.shape[0]))\n",
    "mu = torch.tensor((param[0:Z.shape[0]]))\n",
    "L=L.diagonal_scatter(torch.exp(torch.tensor(param[Z.shape[0]:2*Z.shape[0]])),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e57dfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.0446e-03, -2.9898e-03,  4.2113e-03,  4.7034e-03, -7.8821e-04,\n",
       "          5.1196e-03,  3.7328e-04, -7.1257e-03,  1.2351e-03, -2.2377e-04,\n",
       "          4.7631e-03, -2.9201e-03, -8.7479e-03, -6.6511e-03, -6.8102e-03,\n",
       "          8.6172e-03,  4.7889e-04, -4.4548e-03,  1.1133e-03,  5.2984e-03,\n",
       "          3.5603e-03, -5.4476e-04, -4.9855e-03, -1.0785e-03, -2.6549e-03,\n",
       "          6.7033e-03, -1.3012e-03,  1.7599e-03,  2.7987e-03, -4.3052e-04,\n",
       "          4.6917e-03, -3.0877e-06, -3.8599e-03,  1.0564e-03,  7.2714e-03,\n",
       "         -2.1282e-03,  3.1218e-03,  7.5667e-04,  1.1193e-03, -8.8875e-03,\n",
       "         -5.7541e-03, -5.0122e-03,  7.2914e-03,  5.9982e-03,  2.3850e-03,\n",
       "          3.0103e-03,  1.1223e-03,  1.9955e-03, -4.0282e-03,  5.5483e-03,\n",
       "         -3.3005e-03, -2.9895e-03, -2.5728e-03, -1.1657e-03, -5.3076e-03,\n",
       "          7.9753e-04, -5.4171e-03, -1.2304e-03,  7.9765e-04, -3.1902e-03,\n",
       "          6.5850e-04, -4.4116e-03,  3.3613e-03, -3.1184e-03, -9.9037e-03,\n",
       "         -5.9352e-03, -1.2734e-03,  3.4754e-03,  5.0763e-03, -3.2995e-03,\n",
       "         -3.9484e-03,  1.0831e-04, -7.8180e-03, -6.2975e-03, -5.3081e-03,\n",
       "          6.5779e-03, -2.6647e-04, -2.9723e-04,  7.6185e-03, -7.1738e-03,\n",
       "          1.9655e-03,  8.4458e-03, -4.9105e-03, -4.3064e-03, -8.5396e-04,\n",
       "         -1.5693e-03,  1.4551e-03, -4.3338e-03,  9.1460e-04,  3.4503e-03,\n",
       "         -4.1721e-03, -4.9058e-05, -2.9244e-03,  1.3992e-03, -1.6011e-03,\n",
       "         -1.4692e-03,  3.0916e-03, -4.0976e-03,  1.1852e-03, -6.2808e-03,\n",
       "          1.2030e-03, -9.5031e-04, -6.0508e-03,  1.6817e-03, -1.8236e-03,\n",
       "         -1.4149e-03,  2.1403e-03, -1.3805e-03, -5.0377e-03, -1.7133e-04,\n",
       "          1.2076e-03, -6.1903e-03,  1.2260e-03,  3.5704e-03, -4.4439e-03,\n",
       "         -4.5134e-03,  2.1998e-03, -1.4706e-03, -4.8284e-03, -7.3396e-03,\n",
       "          1.5212e-03, -1.4074e-03, -3.5900e-03, -8.1791e-04, -2.2203e-03,\n",
       "         -2.9553e-03, -4.5199e-03, -1.4242e-02,  5.5657e-03,  9.0491e-03,\n",
       "         -1.6020e-03,  6.1682e-03,  1.3789e-03, -8.8904e-03, -3.9711e-03,\n",
       "         -4.9896e-03,  7.9968e-03, -2.8521e-04, -3.0062e-03, -5.7575e-04,\n",
       "          1.2582e-02,  6.2508e-05, -2.9760e-03,  5.8420e-03,  3.3533e-03,\n",
       "          2.0270e-03,  3.1626e-03,  4.2871e-03, -2.5253e-03,  6.3794e-03,\n",
       "         -1.2664e-02, -6.3320e-03, -2.9134e-03, -3.5146e-03, -1.0215e-02,\n",
       "         -1.8369e-03,  4.7528e-03, -4.7457e-03, -2.1536e-03,  3.4327e-03,\n",
       "          8.4294e-03, -6.3972e-03, -2.7330e-03,  3.2284e-03, -6.9144e-04,\n",
       "         -6.1885e-03,  4.3289e-03,  1.0329e-02,  6.6729e-03, -6.0767e-03,\n",
       "         -3.7393e-03,  5.6215e-03,  4.6332e-03, -2.7089e-03,  8.6832e-04,\n",
       "          5.4847e-03, -2.2459e-03, -5.1525e-03,  2.0850e-03,  1.1281e-03,\n",
       "         -3.1446e-03,  1.3742e-04,  2.1541e-03,  3.7367e-03, -1.9617e-03,\n",
       "          1.2946e-03, -4.7019e-03,  1.5050e-03,  2.1747e-04, -1.3587e-02,\n",
       "          2.3074e-05, -8.6070e-03,  3.3917e-03,  3.5420e-03,  1.0606e-03,\n",
       "         -1.0103e-03,  2.7708e-04, -9.5472e-04,  3.7794e-03, -7.8327e-03,\n",
       "          5.9131e-03, -7.7267e-03, -1.1445e-02, -2.6910e-04, -3.5074e-03,\n",
       "          7.2935e-03, -1.3040e-02, -1.5632e-04,  4.6049e-03,  3.9069e-03,\n",
       "          4.3090e-03,  5.2669e-03,  9.7184e-04,  4.3505e-03,  5.1108e-03,\n",
       "         -6.2308e-04,  3.8452e-03, -3.4060e-03,  5.1406e-03,  4.0686e-03,\n",
       "         -2.3596e-03, -6.5432e-03, -2.9698e-04,  1.2800e-03,  3.3317e-03,\n",
       "         -7.2106e-03, -1.1890e-02,  5.7685e-03,  1.6439e-03,  3.7964e-03,\n",
       "          3.2894e-03,  2.1541e-04,  1.3535e-03,  3.2633e-03, -7.3388e-03,\n",
       "         -3.0876e-03,  1.6218e-03, -9.2256e-03, -8.0579e-04, -7.9208e-04,\n",
       "         -6.7534e-03, -6.7611e-03, -7.1567e-04, -8.5184e-03, -2.2563e-03,\n",
       "         -3.6292e-03,  1.9810e-03,  3.9070e-03, -1.0720e-02,  3.5537e-04,\n",
       "          2.7696e-04,  4.9355e-04, -3.7647e-04,  5.9579e-03, -2.5815e-03,\n",
       "         -2.1316e-03,  1.8966e-03, -1.9977e-03,  5.6019e-03,  3.4820e-03,\n",
       "         -6.1450e-03,  3.3907e-03, -1.2385e-02, -7.8660e-04, -1.2733e-03,\n",
       "          3.3331e-04, -5.2324e-04, -4.3573e-03, -2.9806e-04,  2.3391e-03,\n",
       "         -1.9463e-03,  1.0023e-02,  2.2508e-03,  5.0479e-03,  4.1259e-03,\n",
       "         -9.3985e-04,  1.8391e-03, -3.1664e-04, -3.0562e-03,  1.2213e-03,\n",
       "         -4.7749e-03,  7.6321e-03, -1.3151e-03, -1.1273e-02,  4.1574e-04,\n",
       "          5.8484e-03, -1.9599e-03,  9.7143e-03, -4.1956e-03, -2.6987e-03,\n",
       "          1.4326e-03, -3.8788e-03,  2.1353e-03, -9.4719e-05,  3.3285e-03,\n",
       "          2.0923e-03,  8.2967e-03,  3.7816e-03, -1.7130e-03, -1.4889e-02,\n",
       "         -4.7622e-03, -2.7690e-03,  7.5297e-03, -2.3482e-04,  1.6829e-02,\n",
       "         -2.9565e-03,  4.1445e-03, -2.0602e-04, -2.1248e-03,  6.9895e-03,\n",
       "         -2.3609e-03, -3.2806e-03, -6.6532e-03, -5.2597e-03,  2.5584e-03,\n",
       "         -8.6271e-04,  2.4236e-03, -3.1075e-04,  3.7278e-03, -1.0645e-03,\n",
       "          2.7320e-03, -2.1645e-03, -8.9120e-03,  6.3258e-03, -2.1527e-03,\n",
       "          9.4346e-03, -5.9394e-03, -1.1092e-02, -1.5747e-03, -2.4336e-03,\n",
       "          4.4033e-03, -8.6841e-03,  1.7361e-03, -1.0128e-02,  1.6274e-03,\n",
       "          9.7858e-03, -9.4610e-03, -5.2858e-03,  6.4496e-03, -6.6669e-03,\n",
       "          7.4320e-03, -1.2437e-03,  2.7337e-03,  7.1959e-04,  1.8441e-03,\n",
       "          1.0516e-03, -1.0805e-03, -6.3105e-03, -5.0264e-03, -3.7485e-03,\n",
       "         -5.7896e-03, -3.7839e-03,  3.0509e-04,  3.3473e-03, -2.4452e-03,\n",
       "         -1.9177e-03, -2.5043e-03,  1.2853e-03,  7.1603e-03,  1.5332e-03,\n",
       "         -2.6467e-04, -3.0120e-03,  1.1289e-03, -7.3740e-03, -8.0041e-03,\n",
       "         -5.0767e-03, -5.1625e-04,  2.6086e-03, -1.5025e-04, -1.2753e-02,\n",
       "         -4.3154e-03,  1.2761e-03,  6.8785e-03, -6.5134e-03, -6.6305e-03,\n",
       "          3.0367e-03, -6.1028e-05, -9.9713e-04, -3.4479e-04,  2.4504e-03,\n",
       "          2.7826e-03,  8.4563e-04, -5.6894e-03,  1.0076e-03, -4.5337e-03,\n",
       "         -2.1992e-03, -4.6702e-03,  2.0546e-03, -7.1110e-03, -3.9832e-04,\n",
       "         -3.0379e-03, -1.9998e-03, -4.7794e-04,  7.1102e-03, -6.5109e-03,\n",
       "          9.2773e-04, -7.2391e-03,  2.1292e-03,  2.5650e-03, -6.5159e-03,\n",
       "         -1.4805e-03,  7.0442e-03,  1.3936e-03,  1.6501e-03,  6.1746e-03,\n",
       "          1.3236e-02,  3.1139e-03, -5.8924e-03, -6.9754e-03, -7.7443e-03,\n",
       "          1.9796e-03,  1.2060e-04, -5.9611e-03, -1.0246e-03, -4.7284e-03,\n",
       "          2.4554e-03,  4.3049e-03,  1.2993e-02,  5.9479e-03, -2.2812e-03,\n",
       "         -6.9483e-03, -1.6905e-03, -2.2090e-03,  3.6193e-03,  2.4223e-04,\n",
       "          6.9494e-03, -3.6111e-03, -6.9664e-03, -2.2808e-03, -7.2062e-03,\n",
       "         -3.5454e-03, -3.5418e-03,  1.4573e-03, -1.8333e-03,  6.9216e-03,\n",
       "          2.1004e-03, -4.4710e-03, -1.5081e-03,  4.3708e-03, -2.2297e-03,\n",
       "         -3.9342e-03, -2.4609e-03,  1.7980e-03,  1.7200e-03,  1.7969e-03,\n",
       "         -9.6120e-03, -6.7409e-03, -8.2959e-03,  7.9708e-04,  1.3219e-03,\n",
       "         -4.5760e-03, -3.5298e-03, -8.6172e-03, -5.4157e-04,  2.4005e-03,\n",
       "          5.2133e-03,  1.9123e-03, -1.0164e-03,  3.3426e-03, -2.5114e-03,\n",
       "          7.8061e-03, -2.6092e-03, -2.3161e-03, -2.7534e-03,  6.5000e-03,\n",
       "          9.0516e-04,  4.1237e-03,  2.0351e-03,  6.2608e-03, -6.1935e-03,\n",
       "         -4.4563e-03, -1.0288e-03, -7.9076e-04, -7.6604e-03,  4.4451e-03,\n",
       "          8.1223e-03, -6.4005e-03, -5.4146e-03, -9.5553e-04,  2.3124e-03,\n",
       "         -2.3555e-03,  5.1063e-03,  5.0339e-03,  6.6746e-03, -4.0978e-04,\n",
       "         -4.5681e-03, -5.8407e-03,  2.4296e-03,  7.2804e-03, -1.0818e-02,\n",
       "         -1.1005e-02, -7.0182e-03,  7.7930e-04,  1.3448e-03,  4.5230e-04,\n",
       "          7.6601e-04, -6.1884e-03,  5.0630e-03, -3.5821e-03, -4.3966e-03,\n",
       "         -7.3082e-03,  7.9172e-04,  3.3120e-03,  6.5120e-03,  1.2561e-05,\n",
       "         -2.4823e-03, -2.3506e-03, -6.5799e-03, -6.7955e-03, -3.8370e-03,\n",
       "          1.4059e-03, -6.2281e-03,  5.6611e-03, -2.3446e-03, -4.3889e-03,\n",
       "         -6.0112e-04, -3.6448e-05,  5.3204e-03, -2.2946e-03,  2.1605e-03,\n",
       "          3.8826e-03,  5.8783e-03, -1.6570e-03, -3.3767e-03,  1.2003e-02,\n",
       "         -2.1910e-03,  3.0358e-03, -1.5165e-02,  4.3937e-03,  6.1827e-03,\n",
       "          5.6435e-03,  2.4757e-03, -3.0645e-03, -7.5917e-03,  1.7363e-03,\n",
       "          2.1945e-03,  5.2611e-03,  1.5867e-03,  4.5302e-03,  4.2454e-03,\n",
       "          1.8708e-03,  4.8012e-03, -3.7293e-04, -4.2062e-03,  9.0466e-03,\n",
       "         -3.8195e-03, -5.0527e-04,  3.9046e-03, -2.1748e-03,  7.8652e-03,\n",
       "          7.7041e-03, -1.9373e-04,  5.7919e-05, -8.4274e-03,  2.2975e-03,\n",
       "         -5.8245e-03,  4.3282e-03,  2.3350e-03, -3.9445e-03,  1.9540e-03,\n",
       "          6.9077e-04,  5.9757e-03, -4.0540e-04, -3.0236e-03, -1.5475e-03,\n",
       "         -2.7695e-03, -3.0595e-03, -2.4373e-04, -6.7632e-03,  2.8316e-04,\n",
       "          6.4236e-03, -4.9660e-03, -6.1116e-03, -1.8123e-03, -7.4877e-03,\n",
       "          5.8094e-03,  2.1747e-03,  4.8806e-03,  2.5683e-03, -7.3308e-05,\n",
       "         -1.6379e-03,  6.3578e-03, -1.1866e-03, -2.6521e-03, -2.5674e-03,\n",
       "          7.1191e-03,  3.4432e-04,  1.5926e-03, -8.2787e-03, -3.0588e-03,\n",
       "         -2.3943e-03, -1.4387e-03,  1.2510e-04, -2.3492e-03,  1.7059e-03,\n",
       "          5.6438e-04, -6.0735e-03,  6.0466e-03, -4.0612e-03, -2.7960e-03,\n",
       "          5.5011e-03,  7.9602e-03,  4.0243e-03,  1.8179e-03,  7.1085e-03,\n",
       "          3.5557e-04, -5.2095e-03,  2.1831e-04, -4.9136e-03, -2.0404e-03,\n",
       "          3.9384e-03, -3.3824e-03,  2.3868e-03, -7.9050e-03, -6.0597e-03,\n",
       "         -2.3170e-03, -1.0679e-03,  5.2601e-03, -9.7449e-03,  2.0152e-03,\n",
       "         -5.8943e-03,  9.3550e-03, -1.0425e-03, -4.7431e-04, -8.1700e-06,\n",
       "          6.4994e-03,  8.8614e-03,  1.0501e-03,  2.0434e-03,  1.4740e-02,\n",
       "         -4.5555e-03,  7.5352e-03,  4.2501e-03, -6.3841e-03, -2.1179e-03,\n",
       "         -1.2936e-03,  5.4680e-03,  7.0527e-03, -2.8911e-04, -1.8719e-03,\n",
       "          3.4384e-03, -2.0814e-03,  3.2511e-03,  3.2834e-03, -1.9098e-03,\n",
       "         -3.2547e-03, -1.1663e-03, -3.3768e-03,  2.3122e-03,  3.4898e-03,\n",
       "         -1.7198e-03,  5.2816e-03, -6.6770e-03,  8.2724e-04,  3.9362e-03,\n",
       "         -7.5617e-04, -1.0811e-04,  8.8790e-03, -3.5047e-03,  2.0335e-03,\n",
       "         -8.4988e-03, -1.8058e-02,  3.3299e-03, -2.0555e-03,  7.8672e-04,\n",
       "          3.8371e-03, -3.7400e-03, -3.7643e-03, -9.6235e-04, -1.3404e-04,\n",
       "          2.5471e-03,  3.1279e-03, -4.4319e-03, -3.6032e-03, -2.8655e-03,\n",
       "          6.6319e-03,  1.1046e-02, -3.2283e-03, -7.6200e-03,  2.3684e-03,\n",
       "         -1.3674e-03, -1.7243e-03,  6.3696e-04, -8.9723e-04, -9.5746e-03,\n",
       "         -4.0466e-04,  8.6211e-04, -3.0879e-03,  6.3104e-03, -3.4838e-03,\n",
       "         -6.9738e-03,  3.5044e-03, -4.0229e-03, -7.1848e-03, -3.4242e-03,\n",
       "          7.0536e-04, -1.9593e-03, -1.0416e-02,  2.9103e-03,  6.7666e-03,\n",
       "          2.1761e-03, -6.0565e-03, -9.0105e-03, -3.1205e-03, -5.6109e-03,\n",
       "          6.2748e-03, -6.1463e-04,  1.3243e-02,  4.7079e-03,  4.1759e-03,\n",
       "          1.3111e-02, -5.2415e-03,  1.6257e-03, -4.0695e-04,  4.5595e-03,\n",
       "          4.0064e-03,  7.3322e-04,  5.7690e-03,  7.9699e-03,  6.0946e-03,\n",
       "          3.6767e-03, -5.5488e-03, -8.4707e-04, -9.4519e-03,  6.6163e-03,\n",
       "          2.3106e-03,  4.5673e-03, -1.9989e-03, -6.6252e-03,  1.4671e-03,\n",
       "          7.4172e-03,  6.9347e-03,  2.6553e-04, -1.7686e-03, -3.4964e-03,\n",
       "         -6.5374e-03, -3.9887e-03, -1.8732e-03, -2.5839e-03,  7.5409e-04,\n",
       "         -1.8136e-03,  5.7397e-03,  1.0722e-02, -2.7336e-03, -3.0986e-03,\n",
       "         -4.0344e-03, -5.7488e-03, -1.5790e-04,  5.1949e-04, -3.5126e-03,\n",
       "         -3.7791e-03,  9.4235e-03,  1.3493e-03, -4.9701e-03, -5.1138e-03,\n",
       "         -6.4496e-03,  5.5008e-03,  3.1361e-03,  3.9770e-03, -1.4795e-03,\n",
       "         -1.9075e-03, -5.1124e-03, -1.0058e-03,  2.2833e-03,  7.8691e-05,\n",
       "          6.4406e-03,  7.6424e-03, -7.4979e-05,  2.7566e-03,  7.7146e-03,\n",
       "         -1.2001e-03, -6.2808e-03,  4.7072e-04,  4.5727e-03, -8.0712e-03,\n",
       "          3.0900e-03,  4.2761e-03,  6.8464e-03, -2.1248e-03,  1.0748e-03,\n",
       "          1.4050e-03, -5.9564e-04,  3.8311e-03,  6.5598e-03,  6.9151e-03,\n",
       "         -1.7738e-03,  6.8720e-03, -2.2305e-03,  1.0998e-03,  1.1960e-03,\n",
       "         -1.6493e-03,  3.6081e-03, -4.6665e-03, -3.5951e-03,  1.5024e-03,\n",
       "         -3.5651e-03,  8.0520e-03, -5.1752e-03,  1.4698e-04, -2.7738e-03,\n",
       "         -1.0125e-02,  6.1926e-03,  2.4791e-03,  6.6480e-03,  3.5298e-03,\n",
       "         -2.4270e-03, -3.7623e-03, -8.0454e-04, -2.1163e-03,  1.0213e-02,\n",
       "          7.1327e-04,  1.9890e-03, -9.7906e-04, -1.9238e-04,  2.8842e-04,\n",
       "         -3.5158e-03, -7.7827e-04, -9.4097e-04,  1.9326e-03,  2.4790e-03,\n",
       "         -1.8839e-03, -3.1002e-03, -6.9396e-03,  4.6877e-03,  5.2826e-04,\n",
       "         -3.7532e-03,  1.5894e-03, -8.1183e-03,  2.7062e-03, -2.1044e-03,\n",
       "         -2.0845e-03, -9.9393e-03,  1.4250e-03,  3.2837e-03,  2.0675e-03,\n",
       "         -1.7389e-04,  4.4996e-03,  4.1797e-03, -5.2094e-03, -4.0153e-03,\n",
       "         -7.2447e-03,  3.6667e-03,  3.9069e-03,  1.1625e-03,  1.4735e-03,\n",
       "          1.0284e-04,  6.3718e-03,  3.0793e-03, -3.1995e-03, -2.1678e-03,\n",
       "         -6.1430e-04,  4.5433e-03,  3.7135e-04, -5.2060e-03, -7.5880e-03,\n",
       "         -5.8260e-03,  3.5881e-03,  5.4627e-03,  9.4861e-04,  4.1027e-03,\n",
       "          2.3694e-03,  1.2212e-03,  6.9145e-03,  1.9287e-03,  4.6019e-03,\n",
       "          2.8207e-03,  6.1436e-03, -1.0308e-02, -4.6485e-03, -5.0863e-03,\n",
       "         -1.6752e-03,  2.5465e-04,  8.8436e-04,  5.8766e-03, -2.1391e-03,\n",
       "          1.5954e-03, -1.0026e-03, -1.2684e-03, -5.3667e-03,  1.5595e-03,\n",
       "          5.5990e-03,  7.3034e-04,  1.0363e-02, -1.7174e-03,  6.1474e-03,\n",
       "          5.3417e-03,  2.4609e-04, -2.5644e-03, -5.0887e-05, -9.6820e-03,\n",
       "          4.0784e-03,  3.2736e-03, -8.4632e-03, -4.9757e-03,  1.9546e-03,\n",
       "          8.3941e-03,  6.3130e-03, -5.1429e-03,  1.2218e-03,  5.4260e-03,\n",
       "          4.7073e-03, -5.9909e-03, -4.0031e-03, -7.7779e-05, -5.0964e-03,\n",
       "          3.6365e-03,  7.3877e-03, -8.0651e-03,  2.3219e-03,  3.6971e-03,\n",
       "          4.9785e-03, -6.8856e-03, -5.2652e-03,  3.5529e-03, -9.5806e-04,\n",
       "         -2.9148e-03, -3.3828e-03, -1.1902e-03, -6.6379e-03, -2.3249e-03,\n",
       "          8.8756e-03, -2.8956e-03,  2.3026e-03, -2.4100e-03, -4.9896e-05,\n",
       "         -2.5772e-03,  6.3784e-03, -7.2747e-03, -3.8134e-03, -2.1043e-03,\n",
       "         -4.7661e-03, -3.6524e-03,  1.2526e-02, -5.7586e-03, -9.1870e-03,\n",
       "         -5.7846e-05, -3.1423e-03,  1.2113e-02,  5.1600e-03, -1.6076e-03,\n",
       "         -4.3527e-03,  2.3676e-03,  1.0119e-02, -6.1466e-04,  7.1686e-03,\n",
       "          6.9125e-03,  1.0257e-03, -6.2402e-03,  1.4466e-03,  4.8075e-03,\n",
       "         -3.6259e-03,  5.3886e-03, -1.0054e-02, -7.2191e-04,  2.2924e-03,\n",
       "          2.8500e-03, -3.2549e-03, -8.2979e-03,  2.7271e-03, -1.7686e-03,\n",
       "         -5.6102e-04, -5.5973e-03,  1.4993e-03,  2.0174e-03,  1.5329e-03,\n",
       "         -7.0860e-03, -4.1320e-03, -1.7435e-03, -3.1088e-03, -1.3371e-03,\n",
       "         -2.0906e-03, -2.9203e-04, -2.3386e-03, -1.5356e-03,  3.4481e-04,\n",
       "         -4.6383e-03, -1.8737e-03, -1.7104e-03, -9.5467e-03, -6.0542e-03,\n",
       "         -9.5632e-03,  4.9333e-03,  5.5318e-04,  4.4047e-03, -2.6127e-03,\n",
       "         -1.5195e-02, -2.8038e-03,  2.7363e-03,  5.9559e-03,  1.5856e-03,\n",
       "          3.6069e-03, -1.7584e-03,  3.4472e-03,  1.8082e-03, -4.4447e-03,\n",
       "         -1.0850e-02, -1.0360e-02,  7.9642e-04,  1.1518e-02, -1.3949e-03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(L,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac761cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_s2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec3f63-f945-48ee-b751-760320f70606",
   "metadata": {},
   "source": [
    "Next steps\n",
    "\n",
    "- amortized inference\n",
    "- Can we use GPtorch as function?\n",
    "- write as a class with a fwd and backward method\n",
    "- use torch.nn class to define params?\n",
    "- use random Z at each stage? Should the number of samples increase as we converge?\n",
    "- add prior for x\n",
    "- change f to avoid bimodal posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb1605-f104-45fa-8a63-4971a7356851",
   "metadata": {},
   "source": [
    "## Amortized Variational Inference\n",
    "\n",
    "Let's assume $q(x|y)$ can be modelled as $N(m(y), s2(y))$ where $m(y)$ and $s2(y)$ are both modelled as neural networks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ddc48-71b5-4673-ab5e-35d2328c59d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
