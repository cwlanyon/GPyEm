{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2d8f72-376b-4f7b-b0e0-d1b70bafe8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "from matplotlib import pyplot as plt\n",
    "import GPE_ensemble as GPE\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from  torch.distributions import normal\n",
    "\n",
    "yobs = 8.0\n",
    "sigma2 = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd90672-e817-4cd8-8c99-611507c7f5b4",
   "metadata": {},
   "source": [
    "First we'll learn how to get the variational posterior for $x | y$ for a single value of y. We have\n",
    "$$y= 4x-x^2/2+N(0,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350d8cd-93ad-464d-aceb-d676326f8bd2",
   "metadata": {},
   "source": [
    "## Variational inference\n",
    "\n",
    "Maximize the ELBO\n",
    "$$ELBO(q) = E[\\log p(y | x)] − KL (q(x)|| p(x)).$$\n",
    "\n",
    "We'll use $q(x) = N(m, s^2)$ as the variational family\n",
    "In this case, the ELBO can be computed exactly, but its a pain to do. So I've approximated the E[log p(y | x)] with a Monte Carlo sum.\n",
    "$\\Phi=(m, \\log s^2)$ are the variational parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e7d49-6e46-4465-8d7f-297c6fd1edae",
   "metadata": {},
   "source": [
    "KL (q(x) || p(x)) = E[log q(x)] − E[log p(x)]\n",
    "where expectations are with respect to q.\n",
    "\n",
    "Note, if $p(x) \\propto 1$, then $E\\log p(x)$ does not depend on $\\phi$ so can be ignored\n",
    "If $q(x) =N(m, \\sigma^2)$, then $E[\\log q(x)] = -0.5 \\log(2 \\pi \\sigma^2) - 1/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525b9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p=3\n",
    "\n",
    "rl = -10\n",
    "ru=20\n",
    "\n",
    "obs_error = 0.0001\n",
    "\n",
    "x=torch.linspace(rl,ru,p)\n",
    "\n",
    "b=0.5\n",
    "\n",
    "def lin(x):\n",
    "    y = b*x\n",
    "    return y\n",
    "\n",
    "y = lin(x) \n",
    "\n",
    "plt.plot(x[:,None],y[:,None],'o')\n",
    "\n",
    "emulator = GPE.ensemble(x[:,None],y[:,None],mean_func=\"constant\",training_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcf85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4739272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(m,log_s2,x,emulator,y,prior_mean,prior_cov,obs_error):\n",
    "    param=[m,log_s2]\n",
    "    L=torch.zeros((x.shape[0],x.shape[0]))\n",
    "    mu = torch.tensor((param[0:x.shape[0]]))\n",
    "    L=L.diagonal_scatter(torch.exp(torch.tensor(param[x.shape[0]:2*x.shape[0]])),0)\n",
    "    #L[1,0]=param[6]\n",
    "   # L[2,0]=param[7]\n",
    "   # L[2,1] =param[8]\n",
    "    covar = torch.matmul(L,L.T)\n",
    "    z=x*torch.exp(log_s2/2.)+m\n",
    "    \n",
    "    z=z.T\n",
    "    \n",
    "    mc_int = torch.sum(emulator.ensemble_log_likelihood_obs_error(z,y,obs_error)+torch.log(x_prior(z,prior_mean,prior_cov)).squeeze())\n",
    "        #mc_int +=-np.log(np.sum(((emulator.predict(z.iloc[[i]]).detach().numpy()-y.values)**2)))+np.log(x_prior(z.iloc[[i]],prior_mean,prior_cov))\n",
    "        #mc_int += (np.sum(np.log(emulator.ensemble_likelihood(z.iloc[[i]],y)))+np.log(x_prior(z.iloc[[i]],prior_mean,prior_cov)))\n",
    "    \n",
    "    lb = mc_int/x.shape[1] - q_prior(covar)\n",
    "    #print(mc_int/x.shape[1])\n",
    "    #print(-q_prior(covar))\n",
    "    #print(np.mean(z,axis=0))\n",
    "    #print(-lb)\n",
    "    return -lb\n",
    "\n",
    "def x_prior(x,mean,cov):\n",
    "\n",
    "    #var = scipy.stats.multivariate_normal(mean=mean, cov=cov)\n",
    "    #val1 = var.pdf(x)\n",
    "    dist = normal.Normal(loc=mean, scale=torch.sqrt(cov))\n",
    "    val = torch.exp(dist.log_prob(x))\n",
    "    return val\n",
    "\n",
    "def q_prior(covar):\n",
    "    qp = -(covar.shape[0]/2)*(1+torch.log(torch.tensor(2*torch.pi)))-0.5*torch.log(torch.linalg.det(covar))\n",
    "    return qp\n",
    "\n",
    "def f_likelihood(x,y,f,sigma2):\n",
    "    \n",
    "    likelihood_manual=-0.5*((f(x) - y)**2)/(sigma2)- 0.5*torch.log(2*np.pi)-0.5*torch.log(sigma2)\n",
    "    return likelihood_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa68f655-7ebf-4b2f-b77a-cb88572bcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(log_var):   \n",
    "    return(-0.5*np.log(2.0*math.pi)-0.5*log_var-0.5)\n",
    "\n",
    "def f(x):\n",
    "    return(4.*x-0.5*torch.pow(x,2.))\n",
    "# How do we define functions? x needs declaring?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ZtoX(m, log_s2):\n",
    "    #reparameterization trick\n",
    "    return(Z*torch.exp(log_s2/2.)+m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c5ab3e-19a1-4afb-bd58-5a6c8ed37629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_dnorm(x, mean, var):\n",
    "    return(-(x-mean).pow(2)/(2.*var)-0.5*np.log(2*math.pi*var))\n",
    "    # checked - could used built-in torch dnorm\n",
    "\n",
    "\n",
    "\n",
    "def Eloglike(m,log_s2):\n",
    "    X= ZtoX(m, log_s2)\n",
    "    \n",
    "    #print(X)\n",
    "    #Rewrite with f\n",
    "    loglikes = log_dnorm(f(X), yobs, sigma2) # observation likelihood\n",
    "    #-(yobs- 4.*X+0.5*X.pow(2)).pow(2)    #f(ZtoX(phi)),2.)\n",
    "    \n",
    "    return(loglikes.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3cdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean=torch.tensor([0])\n",
    "prior_cov = torch.tensor([5])\n",
    "obs_error = [0.001]\n",
    "y_cal = torch.tensor([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120e0a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.8766777515411377 m= 0.9844983220100403 s2= 2.2797038555145264\n",
      "199 3.0669758319854736 m= 0.9722641110420227 s2= 1.8312894105911255\n",
      "299 3.1957321166992188 m= 0.9620619416236877 s2= 1.5320378541946411\n",
      "399 3.3158960342407227 m= 0.9558230042457581 s2= 1.3158869743347168\n",
      "499 3.4415552616119385 m= 0.9504829049110413 s2= 1.152732253074646\n",
      "599 3.535385847091675 m= 0.9469984769821167 s2= 1.0259919166564941\n",
      "699 3.6239750385284424 m= 0.9438668489456177 s2= 0.9247155785560608\n",
      "799 3.712885618209839 m= 0.9410999417304993 s2= 0.8409972190856934\n",
      "899 3.784090042114258 m= 0.938062846660614 s2= 0.7718473076820374\n",
      "999 3.8597540855407715 m= 0.9367544054985046 s2= 0.7134897112846375\n",
      "1099 3.9320387840270996 m= 0.9363154172897339 s2= 0.6628971695899963\n",
      "1199 3.9943065643310547 m= 0.9352058172225952 s2= 0.618865430355072\n",
      "1299 4.053172588348389 m= 0.9343660473823547 s2= 0.5801464915275574\n",
      "1399 4.113320350646973 m= 0.9333866834640503 s2= 0.5460652112960815\n",
      "1499 4.163685321807861 m= 0.9335227012634277 s2= 0.5156790018081665\n",
      "1599 4.219707012176514 m= 0.9337219595909119 s2= 0.4887557923793793\n",
      "1699 4.263176918029785 m= 0.9332578182220459 s2= 0.4644215703010559\n",
      "1799 4.3095903396606445 m= 0.9337368607521057 s2= 0.44234830141067505\n",
      "1899 4.355932712554932 m= 0.9341621398925781 s2= 0.42234116792678833\n",
      "1999 4.398226737976074 m= 0.9344580173492432 s2= 0.4040749669075012\n",
      "2099 4.4413299560546875 m= 0.9342527389526367 s2= 0.3872031569480896\n",
      "2199 4.4800543785095215 m= 0.9346413016319275 s2= 0.3718278408050537\n",
      "2299 4.51759147644043 m= 0.9346507787704468 s2= 0.3575367033481598\n",
      "2399 4.552928447723389 m= 0.9344356060028076 s2= 0.3444913625717163\n",
      "2499 4.587890625 m= 0.9335741996765137 s2= 0.3322756290435791\n",
      "2599 4.617825031280518 m= 0.9339131116867065 s2= 0.3209443986415863\n",
      "2699 4.650761127471924 m= 0.9338325262069702 s2= 0.31022125482559204\n",
      "2799 4.682668685913086 m= 0.934041440486908 s2= 0.300237774848938\n",
      "2899 4.7136311531066895 m= 0.9342288374900818 s2= 0.29085573554039\n",
      "2999 4.747066497802734 m= 0.9341768026351929 s2= 0.2820272743701935\n",
      "3099 4.772611618041992 m= 0.9340534210205078 s2= 0.27376988530158997\n",
      "3199 4.801779747009277 m= 0.934171199798584 s2= 0.2659761309623718\n",
      "3299 4.830605983734131 m= 0.9344841837882996 s2= 0.2585982084274292\n",
      "3399 4.85548734664917 m= 0.9341080188751221 s2= 0.25165075063705444\n",
      "3499 4.8816633224487305 m= 0.9341654181480408 s2= 0.2450573891401291\n",
      "3599 4.9044671058654785 m= 0.9344577193260193 s2= 0.2388228327035904\n",
      "3699 4.930901527404785 m= 0.9344112873077393 s2= 0.23285214602947235\n",
      "3799 4.953927516937256 m= 0.9345662593841553 s2= 0.22716526687145233\n",
      "3899 4.978124141693115 m= 0.9344548583030701 s2= 0.221765398979187\n",
      "3999 5.000409126281738 m= 0.9346050024032593 s2= 0.21658717095851898\n",
      "4099 5.023804187774658 m= 0.9346155524253845 s2= 0.21166570484638214\n",
      "4199 5.047028064727783 m= 0.9345686435699463 s2= 0.20697610080242157\n",
      "4299 5.067770957946777 m= 0.9348968863487244 s2= 0.20245809853076935\n",
      "4399 5.089114189147949 m= 0.9351913928985596 s2= 0.1981809139251709\n",
      "4499 5.110411167144775 m= 0.934686541557312 s2= 0.1940600425004959\n",
      "4599 5.128807067871094 m= 0.9347087144851685 s2= 0.19012095034122467\n",
      "4699 5.148960590362549 m= 0.9351257085800171 s2= 0.18630625307559967\n",
      "4799 5.168968677520752 m= 0.9353438019752502 s2= 0.18263621628284454\n",
      "4899 5.187131404876709 m= 0.9354317784309387 s2= 0.1791127324104309\n",
      "4999 5.206772804260254 m= 0.935326874256134 s2= 0.17573480308055878\n",
      "5099 5.225602149963379 m= 0.9353764653205872 s2= 0.17247609794139862\n",
      "5199 5.242754936218262 m= 0.9356441497802734 s2= 0.16934600472450256\n",
      "5299 5.258862495422363 m= 0.9356309175491333 s2= 0.16633126139640808\n",
      "5399 5.276869773864746 m= 0.9358020424842834 s2= 0.1634148806333542\n",
      "5499 5.294443607330322 m= 0.9356799721717834 s2= 0.16057945787906647\n",
      "5599 5.311906337738037 m= 0.9355869889259338 s2= 0.15787069499492645\n",
      "5699 5.327625274658203 m= 0.9356246590614319 s2= 0.15524733066558838\n",
      "5799 5.343692779541016 m= 0.9355365633964539 s2= 0.1527039110660553\n",
      "5899 5.361323356628418 m= 0.9355676174163818 s2= 0.15024003386497498\n",
      "5999 5.376429557800293 m= 0.9355855584144592 s2= 0.14785397052764893\n",
      "6099 5.391487121582031 m= 0.9357170462608337 s2= 0.14555448293685913\n",
      "6199 5.406334400177002 m= 0.935795783996582 s2= 0.14333634078502655\n",
      "6299 5.42117977142334 m= 0.9355751276016235 s2= 0.1411883383989334\n",
      "6399 5.436023712158203 m= 0.9352899193763733 s2= 0.13909929990768433\n",
      "6499 5.450239658355713 m= 0.9353166818618774 s2= 0.1370585560798645\n",
      "6599 5.466516494750977 m= 0.9351378083229065 s2= 0.13506850600242615\n",
      "6699 5.479612350463867 m= 0.935102105140686 s2= 0.13315078616142273\n",
      "6799 5.492741584777832 m= 0.9353722929954529 s2= 0.13128747045993805\n",
      "6899 5.507949352264404 m= 0.9358251094818115 s2= 0.12946626543998718\n",
      "6999 5.521269798278809 m= 0.9359366297721863 s2= 0.1276945024728775\n",
      "7099 5.534494876861572 m= 0.9360907673835754 s2= 0.1259659379720688\n",
      "7199 5.547665596008301 m= 0.9362682104110718 s2= 0.12428171932697296\n",
      "7299 5.560100078582764 m= 0.9362695217132568 s2= 0.12265434116125107\n",
      "7399 5.571862697601318 m= 0.9363265633583069 s2= 0.121054507791996\n",
      "7499 5.585659503936768 m= 0.9362766146659851 s2= 0.11950859427452087\n",
      "7599 5.597630977630615 m= 0.9364315867424011 s2= 0.11799158900976181\n",
      "7699 5.611357688903809 m= 0.9362752437591553 s2= 0.11651822924613953\n",
      "7799 5.623013973236084 m= 0.9359337687492371 s2= 0.11507637798786163\n",
      "7899 5.634918689727783 m= 0.9356260299682617 s2= 0.11368738859891891\n",
      "7999 5.646492958068848 m= 0.9356298446655273 s2= 0.11231826990842819\n",
      "8099 5.657985687255859 m= 0.9359118342399597 s2= 0.11098013073205948\n",
      "8199 5.670086860656738 m= 0.936163067817688 s2= 0.10969141125679016\n",
      "8299 5.681842803955078 m= 0.936104953289032 s2= 0.10841993242502213\n",
      "8399 5.693206787109375 m= 0.936373233795166 s2= 0.10717292129993439\n",
      "8499 5.703951358795166 m= 0.9363592863082886 s2= 0.1059555858373642\n",
      "8599 5.7168450355529785 m= 0.9362267851829529 s2= 0.10476147383451462\n",
      "8699 5.726259231567383 m= 0.9367145895957947 s2= 0.1035885214805603\n",
      "8799 5.73720121383667 m= 0.9368904829025269 s2= 0.10244724154472351\n",
      "8899 5.748510837554932 m= 0.9364773035049438 s2= 0.10132935643196106\n",
      "8999 5.75894832611084 m= 0.9363124966621399 s2= 0.10023239254951477\n",
      "9099 5.7704010009765625 m= 0.936331033706665 s2= 0.09915465116500854\n",
      "9199 5.780693054199219 m= 0.935991108417511 s2= 0.0981249213218689\n",
      "9299 5.791174411773682 m= 0.9357538819313049 s2= 0.0971107929944992\n",
      "9399 5.800827980041504 m= 0.9359424114227295 s2= 0.09612110257148743\n",
      "9499 5.810752868652344 m= 0.9361375570297241 s2= 0.09514085948467255\n",
      "9599 5.820084095001221 m= 0.9359048008918762 s2= 0.09418730437755585\n",
      "9699 5.8312788009643555 m= 0.9355241060256958 s2= 0.0932500809431076\n",
      "9799 5.840447902679443 m= 0.935624361038208 s2= 0.0923277735710144\n",
      "9899 5.850825786590576 m= 0.9357736110687256 s2= 0.0914229154586792\n",
      "9999 5.86027193069458 m= 0.9357747435569763 s2= 0.0905442163348198\n",
      "Result: p(x|y) = N(0.9357829689979553, 0.09053532034158707) \n"
     ]
    }
   ],
   "source": [
    "### initialize the variational parameters\n",
    "m = torch.full((), 1.,dtype=dtype, requires_grad=True, device=device)\n",
    "log_s2 = torch.full((),torch.log(torch.tensor(3.)), requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "# Samples fixed here - but try adding them into the loop\n",
    "nsamples = 1000\n",
    "#Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for t in range(10000):\n",
    "    Z = torch.randn((nsamples), dtype=dtype, requires_grad=False, device=device)\n",
    "    Z=Z[None,:]\n",
    "    #negELBO = -Eloglike(m,log_s2)+KL(log_s2)\n",
    "    \n",
    "    param = [m,log_s2]\n",
    "    \n",
    "    negELBO = ELBO(m,log_s2,Z,emulator,y_cal[:,None],prior_mean,prior_cov,obs_error)\n",
    "    \n",
    "    if t % 100 == 99:\n",
    "        print(t, negELBO.item(), 'm=', m.item(), 's2=', log_s2.exp().item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    negELBO.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        m -= learning_rate * m.grad\n",
    "        log_s2 -= learning_rate * log_s2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        m.grad = None\n",
    "        log_s2.grad = None\n",
    "        \n",
    "print(f'Result: p(x|y) = N({m.item()}, {log_s2.exp().item()}) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbd630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3021]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emulator.predict(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9484bd82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L=torch.zeros((Z.shape[0],Z.shape[0]))\n",
    "mu = torch.tensor((param[0:Z.shape[0]]))\n",
    "L=L.diagonal_scatter(torch.exp(torch.tensor(param[Z.shape[0]:2*Z.shape[0]])),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e57dfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6902e-01, -7.8390e-02,  1.9252e-02,  8.4543e-02, -2.9563e-02,\n",
       "         -1.2193e-01, -1.2735e-02, -8.1446e-02,  1.4524e-01, -1.1641e-03,\n",
       "          7.2323e-04,  2.0618e-01, -2.5473e-02, -6.3508e-02, -2.5142e-01,\n",
       "         -9.1510e-02, -7.6487e-02,  1.6342e-02, -5.8277e-02, -1.0450e-01,\n",
       "         -7.6469e-03,  5.1650e-02,  9.6047e-02,  9.2795e-02, -7.0829e-02,\n",
       "         -9.3541e-02, -6.5685e-02, -1.1591e-01,  2.3420e-01, -1.1230e-01,\n",
       "          9.6154e-02, -2.2965e-01,  2.2371e-02, -1.1755e-01,  3.1745e-02,\n",
       "         -1.1460e-02, -2.5707e-02,  3.2531e-02,  4.7599e-02,  1.1635e-01,\n",
       "          6.7554e-02, -6.2333e-02,  3.2078e-02, -2.1503e-02, -1.6439e-02,\n",
       "          5.0331e-02,  6.9731e-02,  3.4559e-02, -4.5909e-02,  1.1800e-01,\n",
       "         -3.9206e-02, -6.2267e-03,  8.7649e-02,  1.6487e-01, -2.8042e-02,\n",
       "          4.9866e-03, -5.1931e-02, -1.3192e-01, -5.3519e-02, -6.8285e-02,\n",
       "         -8.6206e-02,  7.5825e-02,  6.0224e-02, -2.6993e-02, -9.0382e-02,\n",
       "         -6.1726e-02,  1.2962e-01,  3.3069e-02,  2.6499e-02,  1.8357e-01,\n",
       "         -2.1063e-01,  8.2926e-02, -2.1551e-02,  1.5708e-03,  6.4073e-02,\n",
       "         -1.2769e-02, -1.3785e-01,  2.8794e-03,  2.2073e-01, -1.0545e-01,\n",
       "         -3.3850e-02, -3.6183e-02,  5.5880e-02,  1.0323e-01,  2.3953e-02,\n",
       "         -1.7003e-01,  7.7098e-02,  1.1139e-01,  1.1279e-02, -3.0674e-02,\n",
       "         -1.5736e-01, -8.8720e-02,  1.8116e-02, -1.2600e-01, -7.9412e-02,\n",
       "         -5.2446e-02, -4.5812e-02, -1.0059e-01,  1.4599e-01,  1.1419e-01,\n",
       "          1.7117e-01,  3.8258e-03,  7.1278e-02, -4.1453e-02, -9.0387e-02,\n",
       "         -1.6116e-01, -5.9210e-03, -1.8128e-02,  7.5523e-02, -1.4349e-01,\n",
       "         -1.2388e-02,  7.5849e-03,  7.8756e-02, -9.2988e-02, -9.6481e-02,\n",
       "         -4.3621e-02, -1.0392e-01,  3.5731e-02,  4.6106e-02,  2.2806e-02,\n",
       "          6.9969e-02, -9.8459e-02, -4.1356e-02, -7.1118e-02, -5.7650e-02,\n",
       "          1.9485e-01, -1.0316e-01, -7.3987e-02, -1.1326e-01,  8.2469e-02,\n",
       "          1.3038e-02,  1.9257e-02,  8.6271e-02,  1.7470e-01, -1.4466e-02,\n",
       "         -1.2595e-01,  4.7990e-02,  3.1751e-02,  1.9234e-02,  3.4335e-02,\n",
       "         -1.5902e-01,  2.3003e-02, -1.1603e-01,  1.1206e-02,  6.8984e-02,\n",
       "          1.2773e-01, -3.4984e-02, -7.4642e-02,  2.5857e-02, -4.3782e-02,\n",
       "         -7.8615e-02,  8.3079e-02,  4.8152e-02, -1.3945e-01, -5.4445e-02,\n",
       "          5.1687e-02,  2.8462e-02,  3.6205e-02,  1.1651e-01,  1.0705e-02,\n",
       "          7.5320e-02,  1.5027e-01,  2.8555e-02, -4.3779e-02,  8.2498e-03,\n",
       "         -3.7112e-02, -1.5707e-01,  1.0919e-01, -2.1666e-02, -1.0506e-02,\n",
       "         -1.3257e-02, -6.2933e-02, -1.0690e-01,  5.0076e-02,  8.4085e-02,\n",
       "         -9.0976e-02,  1.3140e-01,  5.9506e-04,  9.9093e-02,  1.0185e-01,\n",
       "          1.8162e-01,  1.1653e-01, -2.1307e-02, -1.1195e-01,  1.3622e-01,\n",
       "         -5.2012e-02,  1.2764e-01,  6.0531e-02, -3.2062e-02, -5.6699e-02,\n",
       "          1.8735e-01,  2.0289e-01, -4.3768e-02, -1.7867e-02, -3.5857e-02,\n",
       "          2.6514e-02,  5.5250e-02,  1.9848e-02, -5.9319e-03, -6.2109e-02,\n",
       "          3.9025e-02,  8.8074e-02,  2.8643e-02, -1.8208e-02, -9.4379e-02,\n",
       "         -7.2168e-03,  6.5535e-02, -1.2462e-01, -2.0631e-02, -2.6343e-01,\n",
       "          7.2547e-02,  7.8296e-02, -7.9661e-02,  5.4680e-02,  1.0810e-01,\n",
       "         -6.2132e-03, -1.3118e-02,  2.9645e-01, -8.2147e-02, -5.0658e-02,\n",
       "         -1.2931e-01,  9.1491e-02, -6.7951e-03,  7.6474e-02, -2.3655e-02,\n",
       "          6.3739e-03, -5.6353e-02, -9.6874e-02,  6.2627e-03,  6.3187e-02,\n",
       "         -3.0402e-02, -4.3653e-02, -4.8515e-02,  4.2831e-02,  3.4578e-02,\n",
       "         -1.6388e-01, -3.2335e-02, -1.0804e-01,  1.4212e-01, -2.5742e-03,\n",
       "         -1.7545e-01, -3.3760e-02, -1.5296e-01, -9.7000e-03,  3.1904e-02,\n",
       "         -1.0156e-01, -1.1811e-01, -2.9800e-02, -1.3821e-01, -5.0746e-02,\n",
       "         -7.2451e-02, -8.9884e-02, -3.9291e-03,  7.3212e-02,  1.2100e-02,\n",
       "         -1.5740e-02, -1.4644e-01,  1.0626e-01,  8.6801e-02, -3.4699e-02,\n",
       "          1.0577e-01, -7.6343e-02, -1.1475e-02, -3.4798e-02, -9.5099e-02,\n",
       "         -1.0474e-01,  1.8101e-02,  1.1669e-01, -5.0395e-03,  6.4818e-02,\n",
       "         -2.9962e-02, -1.7360e-02, -2.7102e-02,  3.1594e-02,  5.7526e-02,\n",
       "         -2.1572e-02, -8.6722e-02, -6.6487e-02,  3.9576e-02,  6.4276e-02,\n",
       "          1.1992e-02,  1.7290e-01, -5.1103e-02, -6.1539e-02, -3.6141e-02,\n",
       "          1.2797e-01, -8.4045e-02,  1.0506e-01, -1.0410e-01,  9.8637e-02,\n",
       "         -4.3983e-02,  2.0402e-01,  1.2794e-02,  5.3394e-02, -1.2045e-01,\n",
       "         -1.4513e-01, -4.0670e-02,  1.3777e-01, -6.9561e-02, -1.3343e-01,\n",
       "         -4.3487e-02,  2.0842e-02,  7.7877e-02,  9.5339e-02, -4.5906e-02,\n",
       "          1.0543e-03,  5.5618e-02, -6.5190e-02,  1.0957e-01,  6.5284e-02,\n",
       "         -7.3476e-02,  1.2708e-01, -3.8936e-03,  4.2661e-02, -4.6038e-02,\n",
       "         -1.0402e-01,  1.7372e-01, -1.9669e-02,  1.3911e-01, -4.5745e-02,\n",
       "         -2.4945e-02,  1.9282e-02, -1.6199e-02, -8.1404e-02,  1.1595e-01,\n",
       "          2.6722e-02, -3.1366e-02, -2.0269e-01, -1.2441e-01, -5.6169e-02,\n",
       "         -7.8913e-02,  1.2227e-02,  1.7533e-02, -1.8759e-02,  1.0529e-01,\n",
       "         -3.7265e-02, -1.0818e-01, -6.6495e-02,  2.6649e-02,  3.2385e-02,\n",
       "          1.1407e-01, -1.0176e-01, -3.4513e-02, -1.9633e-01, -3.8979e-02,\n",
       "          6.3588e-02, -1.5992e-02,  2.0500e-01,  3.7257e-03, -9.7839e-02,\n",
       "          5.9240e-02,  1.5738e-01,  8.6869e-03, -5.8918e-02,  9.8633e-02,\n",
       "         -6.8275e-02,  9.4416e-02,  1.2861e-02,  1.3394e-01, -1.5224e-01,\n",
       "         -6.1318e-02, -7.4448e-02, -1.1810e-01,  6.3824e-02, -2.8376e-02,\n",
       "         -7.0449e-03,  7.3752e-02,  2.1484e-02, -1.0548e-01,  1.0106e-01,\n",
       "          6.5060e-02, -1.8871e-02, -2.7909e-01,  1.3355e-01, -4.5409e-02,\n",
       "         -1.8590e-02, -3.9717e-02, -4.5119e-02,  4.1239e-02,  1.2607e-01,\n",
       "         -5.6275e-02, -7.9293e-03, -1.1223e-01, -1.1519e-02,  2.4225e-01,\n",
       "          1.4111e-01,  4.2180e-02,  5.4294e-02, -1.1957e-01,  1.9860e-02,\n",
       "         -1.2813e-01, -5.8522e-02, -2.1664e-03, -4.2035e-02, -3.0501e-02,\n",
       "         -1.0283e-01,  6.2761e-02, -1.5176e-01, -1.6716e-02,  7.7384e-03,\n",
       "          6.6648e-02,  8.7069e-03,  5.6599e-02, -1.9686e-02, -3.4448e-02,\n",
       "         -1.6214e-01,  2.2440e-02,  9.6215e-02, -1.6610e-01,  6.5504e-02,\n",
       "         -1.7284e-01,  6.4488e-02, -4.2367e-02,  6.4724e-02, -1.2168e-01,\n",
       "         -6.1499e-02,  1.5894e-01,  1.0772e-01,  8.8952e-02,  1.8263e-01,\n",
       "          1.9807e-01,  1.4836e-01,  3.1093e-02, -8.5159e-03,  1.6665e-01,\n",
       "          8.8044e-02,  1.7396e-01,  1.1883e-01,  9.5236e-03,  4.8408e-02,\n",
       "         -1.0690e-01,  1.0005e-01,  7.0193e-03,  2.9108e-02, -1.4126e-01,\n",
       "         -3.7331e-02,  8.2398e-02,  7.8724e-02,  8.1543e-02,  3.3697e-02,\n",
       "          1.3865e-01, -5.6053e-02,  1.0616e-01,  2.9322e-02,  5.3550e-02,\n",
       "          2.8744e-03,  3.1971e-03,  1.9978e-02,  4.9761e-02, -3.6887e-02,\n",
       "         -5.2846e-02,  7.2950e-02,  8.8343e-02,  3.6251e-02,  1.2071e-02,\n",
       "         -4.0465e-02, -2.1274e-01, -5.0021e-02, -7.0790e-02, -8.0731e-02,\n",
       "         -5.1991e-02, -7.9007e-02, -7.7369e-02,  1.0736e-01,  1.3011e-01,\n",
       "         -1.4203e-01, -5.2473e-04, -5.8835e-02, -1.7386e-01, -8.7653e-02,\n",
       "         -1.3186e-01,  9.7583e-03,  1.3187e-01,  4.2175e-02,  1.1624e-01,\n",
       "         -6.5795e-02,  6.4167e-02, -1.3828e-02,  1.4260e-01,  1.6967e-02,\n",
       "         -9.2924e-03,  1.0076e-01, -1.1205e-01,  6.6120e-02, -1.6679e-01,\n",
       "         -4.8167e-02,  1.6304e-02, -7.6428e-03, -4.1453e-03, -1.2889e-01,\n",
       "         -4.6392e-03, -2.1079e-01, -1.5123e-01,  2.7414e-02, -9.8683e-02,\n",
       "          5.9599e-02, -2.8861e-03,  1.5679e-01, -2.4829e-02,  7.2144e-02,\n",
       "         -3.5161e-02, -1.1851e-02, -1.9710e-01,  6.0963e-02,  3.5559e-02,\n",
       "         -2.7052e-02, -1.3289e-01, -2.2173e-01,  2.1014e-02, -5.2440e-03,\n",
       "          9.3832e-02, -2.0981e-01,  1.3885e-01, -1.8062e-01, -1.5060e-01,\n",
       "         -6.8331e-02,  8.3474e-02,  9.7669e-03,  5.9040e-02,  8.2602e-03,\n",
       "         -4.6985e-02,  7.2350e-02, -8.1866e-02,  1.0344e-01,  1.5999e-01,\n",
       "         -2.1453e-01,  6.5659e-02,  6.1861e-02, -9.7648e-02,  6.3864e-02,\n",
       "         -5.4715e-02,  7.8121e-03,  8.1349e-02, -5.2507e-02, -3.4052e-02,\n",
       "         -9.6633e-02,  1.5964e-01,  5.4787e-02,  2.9323e-02, -2.6980e-02,\n",
       "         -1.6733e-01,  2.6457e-02,  2.0918e-02, -1.0821e-01,  6.7319e-02,\n",
       "          2.3467e-02, -8.7241e-02,  9.0704e-02,  9.0543e-02,  2.0452e-01,\n",
       "         -4.5631e-02, -2.9415e-02, -1.3402e-01, -3.7311e-02,  5.3215e-02,\n",
       "          8.4336e-02, -1.0861e-01, -8.7208e-02, -2.5313e-02, -6.9962e-02,\n",
       "         -7.7930e-02,  8.6933e-02,  5.6723e-02,  7.9825e-02, -4.4000e-02,\n",
       "         -2.8983e-02, -4.7715e-02,  1.3648e-01,  1.0259e-01, -4.2875e-03,\n",
       "         -1.7977e-02, -4.0557e-02, -1.2673e-01,  1.6204e-02,  6.0619e-02,\n",
       "          2.7214e-02, -1.3024e-01,  3.2511e-02, -1.3034e-01, -4.1518e-02,\n",
       "          1.3524e-01,  3.4828e-02,  1.7773e-01, -1.1179e-01, -4.5329e-02,\n",
       "          1.2196e-01, -7.8342e-02, -4.0843e-02,  1.1701e-01, -2.1715e-04,\n",
       "         -1.7430e-01,  1.2215e-01,  4.4345e-02,  3.0589e-02, -1.2919e-01,\n",
       "         -1.6160e-02,  1.1576e-01, -6.5856e-02, -6.4674e-02, -1.5079e-01,\n",
       "         -1.4376e-02,  8.4599e-02,  7.0712e-02, -5.2439e-02,  6.8321e-02,\n",
       "         -2.5315e-02,  9.5156e-02,  7.1830e-02, -6.2778e-02,  4.2160e-02,\n",
       "         -1.1581e-01,  3.8451e-02,  4.7221e-02, -1.3225e-01, -6.2504e-02,\n",
       "         -5.4902e-02, -2.2306e-02,  1.2628e-01,  1.4106e-01, -4.5431e-02,\n",
       "         -5.7502e-02, -1.2195e-01, -9.1564e-02,  2.0888e-02,  3.4431e-02,\n",
       "         -5.4744e-02,  1.2684e-02,  3.7368e-02, -1.3989e-01,  1.2250e-01,\n",
       "          1.8766e-01, -1.5418e-01,  1.2277e-02, -1.1661e-01,  4.3187e-02,\n",
       "         -6.6660e-03,  6.2112e-02, -6.7618e-02,  7.7190e-02, -2.2647e-02,\n",
       "          7.2941e-02,  1.1896e-01,  9.5484e-02, -9.1025e-02, -2.3234e-02,\n",
       "          5.9971e-02, -1.0200e-02, -3.6854e-02,  3.5384e-04,  2.3886e-02,\n",
       "          2.0689e-01, -8.1186e-02,  6.6449e-02, -2.6644e-02, -8.6123e-02,\n",
       "          9.2471e-02, -5.9312e-02,  9.0961e-03, -1.0215e-02,  1.0654e-01,\n",
       "          3.9874e-03,  8.7134e-02,  5.4501e-02,  1.6543e-02, -7.3440e-02,\n",
       "          2.2449e-01, -1.8282e-02,  1.6947e-01,  2.9338e-02,  6.0701e-02,\n",
       "         -3.0128e-02, -4.8152e-02, -8.1696e-02, -1.1130e-01,  1.2950e-01,\n",
       "         -1.2995e-01,  9.3441e-02,  8.5213e-02,  1.0425e-01,  1.4355e-02,\n",
       "         -6.4907e-02,  1.9955e-03,  1.2753e-01, -6.8293e-02, -2.1049e-03,\n",
       "         -1.8793e-02, -7.9545e-02,  4.4067e-02, -9.8991e-02,  6.3371e-02,\n",
       "         -1.8817e-01,  6.9384e-02, -3.5940e-02,  1.0804e-01, -1.7809e-01,\n",
       "          3.9388e-02,  1.0505e-01,  4.0083e-02,  4.8328e-02, -2.1833e-02,\n",
       "          8.4655e-02,  6.7753e-02,  1.2977e-02,  1.6648e-01, -6.2318e-02,\n",
       "          4.2053e-02, -7.4428e-02,  1.5481e-01, -1.0349e-01, -7.6989e-02,\n",
       "          1.0887e-01,  4.7024e-02, -2.3077e-02, -9.0262e-02,  1.1535e-01,\n",
       "         -2.5728e-03,  2.3079e-02,  3.7410e-02, -2.4808e-02, -1.1555e-01,\n",
       "          4.8974e-02,  5.8006e-02, -1.2746e-02,  1.0732e-02, -9.1579e-02,\n",
       "         -2.9590e-02,  4.0264e-02,  1.5752e-02, -1.5246e-01, -9.1792e-03,\n",
       "         -6.3099e-02,  7.7066e-02, -6.1013e-02,  1.5127e-01,  6.2521e-02,\n",
       "         -5.9744e-02, -1.0341e-01, -6.7827e-03,  4.0105e-02, -4.2367e-02,\n",
       "          9.1637e-02,  5.7300e-02, -8.7788e-02, -2.3358e-01,  1.8153e-02,\n",
       "         -8.8278e-02, -3.9474e-02,  2.3147e-02, -8.5703e-02,  1.0350e-01,\n",
       "          1.5181e-01, -1.8774e-02,  4.9230e-02,  2.6277e-04, -1.2693e-02,\n",
       "         -1.1120e-01,  2.0083e-01, -6.4715e-02,  4.2742e-02, -5.1699e-02,\n",
       "          9.3199e-03, -1.7200e-01,  5.9739e-02, -8.9699e-02,  1.0866e-01,\n",
       "         -1.3658e-02, -3.7971e-02,  1.9002e-03,  3.4231e-02, -1.4076e-01,\n",
       "         -2.2492e-02,  1.4500e-02, -4.9651e-02, -7.1090e-03,  1.1713e-01,\n",
       "          3.3702e-03,  1.9965e-02,  1.5395e-01, -1.5404e-02,  4.0021e-02,\n",
       "         -8.4877e-02, -4.8187e-02, -1.7303e-01, -9.8186e-02, -4.3405e-02,\n",
       "         -3.8341e-03, -8.7717e-03,  1.3490e-01, -9.7276e-02, -1.0350e-01,\n",
       "         -2.6059e-02,  1.9189e-01,  4.0725e-02,  2.6230e-02,  8.5008e-02,\n",
       "         -1.0994e-01,  2.0358e-02, -2.0644e-02, -1.9539e-02, -1.1408e-01,\n",
       "          4.4051e-02,  1.2306e-01, -1.4149e-01,  3.2487e-02, -1.3805e-01,\n",
       "          2.8058e-02, -3.3431e-02, -6.9663e-02,  2.9589e-04,  2.6946e-02,\n",
       "         -1.6346e-02, -1.0655e-01, -9.4562e-02,  9.7009e-02, -1.1302e-01,\n",
       "          7.9241e-02,  1.8413e-02,  5.6794e-02,  8.3612e-02,  6.7784e-02,\n",
       "         -2.3620e-02,  3.1081e-03,  2.5349e-01,  1.2314e-01,  7.6730e-02,\n",
       "          8.5709e-02,  5.7571e-02,  2.8320e-02, -7.3024e-02, -3.3276e-02,\n",
       "         -3.7623e-02, -5.8321e-02,  3.4937e-02,  3.6149e-02, -1.0711e-02,\n",
       "          5.5110e-02,  7.3165e-02,  8.8558e-02,  4.0765e-02, -1.4142e-01,\n",
       "          2.5829e-02, -1.2589e-01,  1.7289e-01, -2.7931e-02,  1.0740e-01,\n",
       "          6.0367e-03, -1.0222e-01, -3.4849e-02, -2.1303e-01,  4.3063e-02,\n",
       "          1.2007e-01,  2.0125e-01,  1.1089e-02, -1.2648e-02, -1.0632e-02,\n",
       "          3.0476e-02,  2.0466e-01,  1.3337e-01,  3.7642e-02, -5.9931e-02,\n",
       "          4.4229e-02,  3.7967e-02,  7.7659e-02, -1.2836e-01, -6.6828e-02,\n",
       "         -8.2896e-02,  2.4709e-02, -6.8604e-02, -8.3565e-03, -6.6201e-02,\n",
       "         -2.3537e-01,  1.2326e-01, -2.2072e-01, -8.4108e-02,  1.4683e-01,\n",
       "         -3.6645e-02,  7.1181e-02,  5.1705e-02, -4.0057e-02, -3.2404e-02,\n",
       "          2.2697e-02,  2.4011e-02, -1.3809e-01,  1.5374e-01, -1.2482e-02,\n",
       "         -1.3617e-02,  2.7367e-01,  1.0343e-01, -9.0300e-02, -6.0731e-02,\n",
       "         -5.7237e-02, -1.9101e-01,  4.6635e-02, -5.0949e-02,  4.2226e-02,\n",
       "         -4.9198e-04,  7.9396e-02,  1.0967e-01, -9.9542e-03,  8.7993e-03,\n",
       "         -5.9686e-02,  2.7826e-02, -1.6437e-01, -1.3449e-02, -1.0355e-01,\n",
       "         -1.4686e-01, -8.9652e-02,  2.2317e-02,  2.1833e-02, -4.9373e-02,\n",
       "          4.5400e-02,  2.1238e-02, -3.6424e-02,  1.0169e-01,  1.1157e-01,\n",
       "         -1.3916e-01,  9.3930e-02, -1.0472e-01, -8.4136e-02, -6.6610e-02,\n",
       "         -1.0753e-02, -1.2897e-01, -3.1000e-02, -7.3956e-02, -7.4698e-02,\n",
       "         -7.6012e-02,  6.9398e-02, -1.4083e-01, -7.3025e-02, -1.3093e-02,\n",
       "          6.2692e-02, -1.4529e-01, -1.2566e-01, -2.9564e-02,  1.6595e-02,\n",
       "          5.1099e-02,  1.0210e-02,  4.2159e-03,  6.6372e-02,  8.0391e-02,\n",
       "         -1.2933e-01,  1.3950e-01,  9.1644e-02, -7.1528e-02, -3.5102e-02,\n",
       "          3.7309e-02,  1.1425e-01, -3.0681e-02,  5.8390e-03, -1.3738e-01,\n",
       "          9.1063e-03,  1.3217e-02,  3.6222e-02,  3.3715e-02, -3.0845e-02,\n",
       "         -9.5375e-02, -1.7153e-01, -4.1789e-02, -2.0311e-01,  4.6523e-03,\n",
       "          6.5492e-02,  4.9726e-02, -2.9423e-03,  6.2260e-02,  2.3750e-02,\n",
       "          2.3708e-01,  9.1373e-03,  4.6090e-02, -4.4824e-02, -2.2726e-02,\n",
       "         -5.3627e-02,  1.9033e-01,  4.4171e-02, -5.9393e-02, -1.1425e-02,\n",
       "          1.7691e-01, -3.2722e-02,  1.3193e-02,  9.7881e-02,  1.4427e-03,\n",
       "         -9.1377e-02,  5.9734e-02,  1.4059e-01,  6.7756e-02,  9.1045e-03,\n",
       "          7.2388e-02, -1.7027e-01, -3.8295e-02,  7.4817e-02,  6.7753e-03,\n",
       "         -1.3422e-01,  3.2456e-02,  4.4987e-02,  6.3429e-03,  8.1683e-02,\n",
       "          6.2062e-03, -1.0602e-01, -8.0531e-02,  9.3299e-02,  1.2116e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(L,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac761cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_s2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec3f63-f945-48ee-b751-760320f70606",
   "metadata": {},
   "source": [
    "Next steps\n",
    "\n",
    "- amortized inference\n",
    "- Can we use GPtorch as function?\n",
    "- write as a class with a fwd and backward method\n",
    "- use torch.nn class to define params?\n",
    "- use random Z at each stage? Should the number of samples increase as we converge?\n",
    "- add prior for x\n",
    "- change f to avoid bimodal posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb1605-f104-45fa-8a63-4971a7356851",
   "metadata": {},
   "source": [
    "## Amortized Variational Inference\n",
    "\n",
    "Let's assume $q(x|y)$ can be modelled as $N(m(y), s2(y))$ where $m(y)$ and $s2(y)$ are both modelled as neural networks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ddc48-71b5-4673-ab5e-35d2328c59d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
