{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b1e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from GPyEm import GPE_ensemble as GPE\n",
    "\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from GPErks.gp.data.dataset import Dataset\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from torchmetrics import MeanSquaredError, R2Score\n",
    "#from GPErks.gp.experiment import GPExperiment\n",
    "#from GPErks.train.emulator import GPEmulator\n",
    "#from GPErks.perks.inference import Inference\n",
    "#from GPErks.train.early_stop import NoEarlyStoppingCriterion\n",
    "#from GPErks.train.early_stop import (\n",
    "#    GLEarlyStoppingCriterion,\n",
    "#    PQEarlyStoppingCriterion,\n",
    "#    UPEarlyStoppingCriterion,\n",
    "#)\n",
    "#from GPErks.train.early_stop import PkEarlyStoppingCriterion\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set logger and enforce reproducibility\n",
    "#from GPErks.log.logger import get_logger\n",
    "#from GPErks.utils.random import set_seed\n",
    "#log = get_logger()\n",
    "seed = 7\n",
    "#set_seed(seed)\n",
    "from time import process_time \n",
    "import scipy\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf45825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "406.703\n",
      "02\n",
      "317.407\n",
      "03\n",
      "332.914\n",
      "04\n",
      "309.14\n",
      "05\n",
      "277.849\n",
      "06\n",
      "296.377\n",
      "07\n",
      "355.546\n",
      "08\n",
      "283.103\n",
      "09\n",
      "391.145\n",
      "10\n",
      "439.316\n",
      "11\n",
      "348.01\n",
      "12\n",
      "292.465\n",
      "13\n",
      "301.222\n",
      "14\n",
      "325.678\n",
      "15\n",
      "320.459\n",
      "16\n",
      "297.968\n",
      "17\n",
      "317.709\n",
      "18\n",
      "297.346\n",
      "19\n",
      "312.492\n"
     ]
    }
   ],
   "source": [
    "mode_weights = pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/modes_weights.csv',index_col=0,delim_whitespace=False,header=0)\n",
    "\n",
    "mode_weights\n",
    "\n",
    "#mode_weights=mode_weights.drop(15,axis=0)\n",
    "#mode_weights=mode_weights.drop(14,axis=0)\n",
    "\n",
    "meshes=['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19']\n",
    "\n",
    "x_labels1=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/input/xlabels_EP.txt',delim_whitespace=True,header=None)\n",
    "x_labels=x_labels1.values.flatten().tolist()+mode_weights.columns.tolist()\n",
    "\n",
    "y_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/output/ylabels.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "\n",
    "\n",
    "all_input = []\n",
    "all_output=[]\n",
    "all_x=[]\n",
    "for i in range(len(meshes)):\n",
    "    val=meshes[i]\n",
    "    \n",
    "    inputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/\"+val+\"/X_EP.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "    outputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/\"+val+\"/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "    modeweights = np.tile(mode_weights.iloc[i,:].values, (inputData.shape[0],1))\n",
    "    input_modes = np.concatenate((inputData,modeweights),axis=1)\n",
    "    all_x.append(torch.tensor(inputData))\n",
    "    all_input.append(torch.tensor(input_modes))\n",
    "    all_output.append(torch.tensor(outputData))\n",
    "    print(val)\n",
    "    print(np.max(outputData))\n",
    "#all_input=pd.concat(all_input)\n",
    "#all_output=pd.concat(all_output\n",
    "#all_input.columns=x_labels\n",
    "#all_output.columns=y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1493530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=[]\n",
    "test_input = []\n",
    "train_output=[]\n",
    "test_output = []\n",
    "\n",
    "train_input_modes=[]\n",
    "test_input_modes = []\n",
    "train_output_modes=[]\n",
    "test_output_modes = []\n",
    "\n",
    "for i in range(len(meshes)):\n",
    "\n",
    "    X=all_x[i]\n",
    "    y=all_output[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=seed+i\n",
    "    )\n",
    "    train_input.append(X_train)\n",
    "    test_input.append(X_test)\n",
    "    train_output.append(y_train)\n",
    "    test_output.append(y_test)\n",
    "    \n",
    "for i in range(len(meshes)):\n",
    "\n",
    "    X=all_input[i]\n",
    "    y=all_output[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=seed+i\n",
    "    )\n",
    "    train_input_modes.append(X_train)\n",
    "    test_input_modes.append(X_test)\n",
    "    train_output_modes.append(y_train)\n",
    "    test_output_modes.append(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a8dda",
   "metadata": {},
   "source": [
    "# Emulator per mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff58820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "emulators=[]\n",
    "for i in range(len(meshes)):\n",
    "    emulators.append(GPE.ensemble(train_input[i],train_output[i],mean_func=\"linear\",training_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda116d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R2 = torch.zeros(len(meshes),2)\n",
    "R2_std = torch.zeros(len(meshes),2)\n",
    "for i in range(len(meshes)):\n",
    "    meanR, stdR = emulators[i].R2_sample(test_input[i],test_output[i],n=1000)\n",
    "    R2[i,:]=meanR\n",
    "    R2_std[i,:] = stdR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aec3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3826e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6.85,3.3)\n",
    "fontS=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax.scatter(meshes,R2[:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2[:,0].detach().numpy(),fmt='o',yerr=R2_std[:,0].detach().numpy())\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "ax.scatter(meshes,R2[:,1].detach().numpy(),marker='^')\n",
    "plt.errorbar(meshes,R2[:,1].detach().numpy(),fmt='^',yerr=R2_std[:,1].detach().numpy())\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5e99a-77cb-4c7c-b255-34999fdab064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_Si_first = []\n",
    "all_Si_total = []\n",
    "\n",
    "for m in range(len(meshes)):\n",
    "    input_masked=pd.DataFrame(train_input[m])\n",
    "    # Generate max and min bounds\n",
    "    nDim =input_masked.shape[1]\n",
    "    boundsMaxMin = []\n",
    "    for i in range(nDim):\n",
    "        boundsMaxMin.append([np.min(input_masked.iloc[:,i]),np.max(input_masked.iloc[:,i])])\n",
    "        print(boundsMaxMin[i])\n",
    "    \n",
    "    ##### from SALib.sample import saltelli\n",
    "    from SALib.sample import saltelli\n",
    "    from SALib.analyze import sobol\n",
    "    from SALib.test_functions import Ishigami\n",
    "    import numpy as np\n",
    "    \n",
    "    # Define the model inputs\n",
    "    problem = {\n",
    "        'num_vars': nDim,\n",
    "        'names': x_labels1,\n",
    "        'bounds': boundsMaxMin\n",
    "        #'bounds': [[-1, 1],\n",
    "         #          [-1, 1],\n",
    "          #         [-1, 1],\n",
    "           #        [-1, 1],\n",
    "            #       [-1, 1],\n",
    "             #      [-1, 1],\n",
    "              #     [-1, 1],\n",
    "               #    [-1, 1],\n",
    "                #   [-1,1]]\n",
    "    }\n",
    "    \n",
    "    # Generate samples\n",
    "    param_values = saltelli.sample(problem, 512)\n",
    "    \n",
    "    data1 = torch.tensor(param_values)\n",
    "    #data1.columns = X_train.columns\n",
    "    \n",
    "    Ymean=emulators[m].predict_sample(data1,n=5)\n",
    "    \n",
    "    Si = [[],[]]\n",
    "    nMod = train_output[m].shape[1]\n",
    "    nDim = input_masked.shape[1]\n",
    "    for j in range(Ymean.shape[1]):\n",
    "        for i in range(nMod):\n",
    "            Si[i].append(sobol.analyze(problem, Ymean[:,j,i].detach().numpy(), print_to_console=False,calc_second_order=True))\n",
    "    \n",
    "    \n",
    "    Si_first=np.zeros((nDim,nMod))\n",
    "    Si_total=np.zeros((nDim,nMod))\n",
    "    \n",
    "    for j in range(Ymean.shape[1]):\n",
    "        for i in range(nMod):\n",
    "            total_Si, first_Si, second_Si= Si[i][j].to_df()\n",
    "            Si_first[:,i] +=  first_Si.iloc[:,0]/Ymean.shape[1]\n",
    "            Si_total[:,i] +=  total_Si.iloc[:,0]/Ymean.shape[1]\n",
    "\n",
    "    all_Si_first.append(Si_first)\n",
    "    all_Si_total.append(Si_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ab5a6-dace-4f30-bbf1-6aa47d8ed4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = x_labels1.iloc[:,0]\n",
    "outputs = y_labels.iloc[:,0]\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(all_Si_first[0],cmap='magma_r',vmin=0,vmax=1)\n",
    "\n",
    "ax.set_xticks(np.arange(len(outputs)),labels=outputs)\n",
    "ax.set_yticks(np.arange(len(inputs)), labels=inputs)\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "plt.colorbar(im,fraction=0.015, pad=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c9821",
   "metadata": {},
   "source": [
    "# Emulator trained with 17/18 meshes and evaluated on the left out mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f6432b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_p=40\n",
    "reps=1\n",
    "R2_test = torch.zeros(1,reps,2)\n",
    "R2_leftout= torch.zeros(1,reps,2)\n",
    "for i in range(1):\n",
    "    for j in range(reps):\n",
    "        X=torch.cat(train_input_modes)[:,0:15]\n",
    "        y=torch.cat(train_output_modes)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            train_size=760,\n",
    "            random_state=j\n",
    "        )\n",
    "        emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "        meanR, stdR = emulator.R2_sample(torch.cat(test_input_modes)[:,0:15],torch.cat(test_output_modes),1000)\n",
    "        \n",
    "        R2_test[i,j,:]+=meanR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d867d0b-2ee1-4aa3-b101-a3ec9b3ddf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f9553-22c8-4532-97e1-dc04a6c2cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_masked=pd.DataFrame(X_train)\n",
    "# Generate max and min bounds\n",
    "nDim =input_masked.shape[1]\n",
    "boundsMaxMin = []\n",
    "for i in range(nDim):\n",
    "    boundsMaxMin.append([np.min(input_masked.iloc[:,i]),np.max(input_masked.iloc[:,i])])\n",
    "    print(boundsMaxMin[i])\n",
    "\n",
    "##### from SALib.sample import saltelli\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "from SALib.test_functions import Ishigami\n",
    "import numpy as np\n",
    "\n",
    "# Define the model inputs\n",
    "problem = {\n",
    "    'num_vars': nDim,\n",
    "    'names': x_labels[0:15],\n",
    "    'bounds': boundsMaxMin\n",
    "    #'bounds': [[-1, 1],\n",
    "     #          [-1, 1],\n",
    "      #         [-1, 1],\n",
    "       #        [-1, 1],\n",
    "        #       [-1, 1],\n",
    "         #      [-1, 1],\n",
    "          #     [-1, 1],\n",
    "           #    [-1, 1],\n",
    "            #   [-1,1]]\n",
    "}\n",
    "\n",
    "# Generate samples\n",
    "param_values = saltelli.sample(problem, 512)\n",
    "\n",
    "data1 = torch.tensor(param_values)\n",
    "#data1.columns = X_train.columns\n",
    "\n",
    "Ymean=emulator.predict_sample(data1,n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffbd10-608b-4162-b851-d4c92f0caa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Si = [[],[]]\n",
    "nMod = y_train.shape[1]\n",
    "nDim = X_train.shape[1]\n",
    "for j in range(Ymean.shape[1]):\n",
    "    for i in range(nMod):\n",
    "        Si[i].append(sobol.analyze(problem, Ymean[:,j,i].detach().numpy(), print_to_console=False,calc_second_order=True))\n",
    "\n",
    "\n",
    "Si_first=np.zeros((nDim,nMod))\n",
    "Si_total=np.zeros((nDim,nMod))\n",
    "\n",
    "for j in range(Ymean.shape[1]):\n",
    "    for i in range(nMod):\n",
    "        total_Si, first_Si, second_Si= Si[i][j].to_df()\n",
    "        Si_first[:,i] +=  first_Si.iloc[:,0]/Ymean.shape[1]\n",
    "        Si_total[:,i] +=  total_Si.iloc[:,0]/Ymean.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320da0f9-2574-4554-a97a-5eb8024af236",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = x_labels[0:15]\n",
    "outputs = y_labels.iloc[:,0]\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(Si_first,cmap='magma_r',vmin=0,vmax=1)\n",
    "\n",
    "ax.set_xticks(np.arange(len(outputs)),labels=outputs)\n",
    "ax.set_yticks(np.arange(len(inputs)), labels=inputs)\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "plt.colorbar(im,fraction=0.015, pad=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fb7a5",
   "metadata": {},
   "source": [
    "# Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    #('scaler', StandardScaler()),\n",
    "    ('preprocessor', PolynomialFeatures(degree=1, include_bias=False,interaction_only=False)),\n",
    "    ('lasso', LassoCV(n_alphas=1000,max_iter=10000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b84a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m0_mat(y_test,emulators,x_test,output):\n",
    "\n",
    "    m0=torch.zeros((y_test.shape[0],len(emulators)))\n",
    "    for i in range(len(emulators)):\n",
    "        m0[:,i]=(emulators[i].predict(x_test)[:,output]-y_train.mean(axis=0)[output])/y_train.std(axis=0)[output]\n",
    "\n",
    "\n",
    "    return m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f336bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy(a,y_train,m0,output):\n",
    "    m_t = (m0-y_train.mean(axis=0))/y_train.std(axis=0)\n",
    "    y_t = (y_train-y_train.mean(axis=0))/y_train.std(axis=0)\n",
    "    a=torch.tensor(a)\n",
    "    res = ((a*m_t-y_t)**2).mean(axis=0).detach().numpy()\n",
    "    return res[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fcbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrep_emulators = []\n",
    "\n",
    "reps=1\n",
    "nn=[60]\n",
    "R2=torch.zeros(7,len(nn),2,reps)\n",
    "ISE=torch.zeros(7,len(nn),2,reps)\n",
    "Ti=torch.zeros(7,len(nn),reps)\n",
    "\n",
    "for num, n in enumerate(nn):\n",
    "    for k in range(len(emulators)):\n",
    "        emulators2=emulators.copy()\n",
    "        emulators2.pop(k)\n",
    "        print(len(emulators2))\n",
    "\n",
    "        X_train = train_input[k]\n",
    "        y_train = train_output[k]\n",
    "        X_test = test_input[k]\n",
    "        y_test = test_output[k]\n",
    "        \n",
    "        for i in range(reps):\n",
    "\n",
    "            #b=np.random.choice(range(X_train.shape[0]),n,replace=False)\n",
    "            \n",
    "            X=X_train\n",
    "            y=y_train \n",
    "            X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                train_size=n,\n",
    "                random_state=i\n",
    "            )\n",
    "\n",
    "    \n",
    "            a_d=torch.zeros((y_train1.shape[1],len(emulators2)))\n",
    "            for j in range(y_train1.shape[1]):\n",
    "                m0=m0_mat(y_train1,emulators2,X_train1,j)\n",
    "                # fit to an order-3 polynomial data\n",
    "                y_t=(y_train1[:,j]-y_train1.mean(axis=0)[j])/y_train1.std(axis=0)[j]\n",
    "                model = model.fit(m0.detach().numpy(), y_t.detach().numpy())\n",
    "                a_d[j]=torch.tensor(model.named_steps['lasso'].coef_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_lasso_learned=GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d,a_indicator=True)\n",
    "            discrep_emulators.append(model_dc_lasso_learned)\n",
    "            R2temp,R2std=model_dc_lasso_learned.R2_sample(X_test,y_test,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrep_all_Si_first = []\n",
    "discrep_all_Si_total = []\n",
    "\n",
    "for m in range(len(meshes)):\n",
    "    input_masked=pd.DataFrame(train_input[m])\n",
    "    # Generate max and min bounds\n",
    "    nDim =input_masked.shape[1]\n",
    "    boundsMaxMin = []\n",
    "    for i in range(nDim):\n",
    "        boundsMaxMin.append([np.min(input_masked.iloc[:,i]),np.max(input_masked.iloc[:,i])])\n",
    "        print(boundsMaxMin[i])\n",
    "    \n",
    "    ##### from SALib.sample import saltelli\n",
    "    from SALib.sample import saltelli\n",
    "    from SALib.analyze import sobol\n",
    "    from SALib.test_functions import Ishigami\n",
    "    import numpy as np\n",
    "    \n",
    "    # Define the model inputs\n",
    "    problem = {\n",
    "        'num_vars': nDim,\n",
    "        'names': x_labels1,\n",
    "        'bounds': boundsMaxMin\n",
    "        #'bounds': [[-1, 1],\n",
    "         #          [-1, 1],\n",
    "          #         [-1, 1],\n",
    "           #        [-1, 1],\n",
    "            #       [-1, 1],\n",
    "             #      [-1, 1],\n",
    "              #     [-1, 1],\n",
    "               #    [-1, 1],\n",
    "                #   [-1,1]]\n",
    "    }\n",
    "    \n",
    "    # Generate samples\n",
    "    param_values = saltelli.sample(problem, 512)\n",
    "    \n",
    "    data1 = torch.tensor(param_values)\n",
    "    #data1.columns = X_train.columns\n",
    "    \n",
    "    Ymean=discrep_emulators[m].predict_sample(data1,n=5)\n",
    "    \n",
    "    Si = [[],[]]\n",
    "    nMod = train_output[m].shape[1]\n",
    "    nDim = input_masked.shape[1]\n",
    "    for j in range(Ymean.shape[1]):\n",
    "        for i in range(nMod):\n",
    "            Si[i].append(sobol.analyze(problem, Ymean[:,j,i].detach().numpy(), print_to_console=False,calc_second_order=True))\n",
    "    \n",
    "    \n",
    "    Si_first=np.zeros((nDim,nMod))\n",
    "    Si_total=np.zeros((nDim,nMod))\n",
    "    \n",
    "    for j in range(Ymean.shape[1]):\n",
    "        for i in range(nMod):\n",
    "            total_Si, first_Si, second_Si= Si[i][j].to_df()\n",
    "            Si_first[:,i] +=  first_Si.iloc[:,0]/Ymean.shape[1]\n",
    "            Si_total[:,i] +=  total_Si.iloc[:,0]/Ymean.shape[1]\n",
    "\n",
    "    discrep_all_Si_first.append(Si_first)\n",
    "    discrep_all_Si_total.append(Si_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9641771-b350-4544-8e73-7b591bce9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = x_labels1.iloc[:,0]\n",
    "outputs = y_labels.iloc[:,0]\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(discrep_all_Si_first[0],cmap='magma_r',vmin=0,vmax=1)\n",
    "\n",
    "ax.set_xticks(np.arange(len(outputs)),labels=outputs)\n",
    "ax.set_yticks(np.arange(len(inputs)), labels=inputs)\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "plt.colorbar(im,fraction=0.015, pad=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4a596-bb41-413c-b270-0688fff05003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
