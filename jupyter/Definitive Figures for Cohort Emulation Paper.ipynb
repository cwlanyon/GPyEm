{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b1e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from GPyEm import GPE_ensemble as GPE\n",
    "\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from GPErks.gp.data.dataset import Dataset\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from torchmetrics import MeanSquaredError, R2Score\n",
    "#from GPErks.gp.experiment import GPExperiment\n",
    "#from GPErks.train.emulator import GPEmulator\n",
    "#from GPErks.perks.inference import Inference\n",
    "#from GPErks.train.early_stop import NoEarlyStoppingCriterion\n",
    "#from GPErks.train.early_stop import (\n",
    "#    GLEarlyStoppingCriterion,\n",
    "#    PQEarlyStoppingCriterion,\n",
    "#    UPEarlyStoppingCriterion,\n",
    "#)\n",
    "#from GPErks.train.early_stop import PkEarlyStoppingCriterion\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set logger and enforce reproducibility\n",
    "#from GPErks.log.logger import get_logger\n",
    "#from GPErks.utils.random import set_seed\n",
    "#log = get_logger()\n",
    "seed = 7\n",
    "#set_seed(seed)\n",
    "from time import process_time \n",
    "import scipy\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf45825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "406.703\n",
      "02\n",
      "317.407\n",
      "03\n",
      "332.914\n",
      "04\n",
      "309.14\n",
      "05\n",
      "277.849\n",
      "06\n",
      "296.377\n",
      "07\n",
      "355.546\n",
      "08\n",
      "283.103\n",
      "09\n",
      "391.145\n",
      "10\n",
      "439.316\n",
      "11\n",
      "348.01\n",
      "12\n",
      "292.465\n",
      "13\n",
      "301.222\n",
      "14\n",
      "325.678\n",
      "15\n",
      "320.459\n",
      "16\n",
      "297.968\n",
      "17\n",
      "317.709\n",
      "18\n",
      "297.346\n",
      "19\n",
      "312.492\n"
     ]
    }
   ],
   "source": [
    "mode_weights = pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/modes_weights.csv',index_col=0,delim_whitespace=False,header=0)\n",
    "\n",
    "mode_weights\n",
    "\n",
    "#mode_weights=mode_weights.drop(15,axis=0)\n",
    "#mode_weights=mode_weights.drop(14,axis=0)\n",
    "\n",
    "meshes=['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19']\n",
    "\n",
    "x_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/input/xlabels_EP.txt',delim_whitespace=True,header=None)\n",
    "x_labels=x_labels.values.flatten().tolist()+mode_weights.columns.tolist()\n",
    "\n",
    "y_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/output/ylabels.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "\n",
    "\n",
    "all_input = []\n",
    "all_output=[]\n",
    "all_x=[]\n",
    "for i in range(len(meshes)):\n",
    "    val=meshes[i]\n",
    "    \n",
    "    inputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/\"+val+\"/X_EP.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "    outputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/\"+val+\"/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "    modeweights = np.tile(mode_weights.iloc[i,:].values, (inputData.shape[0],1))\n",
    "    input_modes = np.concatenate((inputData,modeweights),axis=1)\n",
    "    all_x.append(torch.tensor(inputData))\n",
    "    all_input.append(torch.tensor(input_modes))\n",
    "    all_output.append(torch.tensor(outputData))\n",
    "    print(val)\n",
    "    print(np.max(outputData))\n",
    "#all_input=pd.concat(all_input)\n",
    "#all_output=pd.concat(all_output\n",
    "#all_input.columns=x_labels\n",
    "#all_output.columns=y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1493530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=[]\n",
    "test_input = []\n",
    "train_output=[]\n",
    "test_output = []\n",
    "\n",
    "train_input_modes=[]\n",
    "test_input_modes = []\n",
    "train_output_modes=[]\n",
    "test_output_modes = []\n",
    "\n",
    "for i in range(len(meshes)):\n",
    "\n",
    "    X=all_x[i]\n",
    "    y=all_output[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=seed+i\n",
    "    )\n",
    "    train_input.append(X_train)\n",
    "    test_input.append(X_test)\n",
    "    train_output.append(y_train)\n",
    "    test_output.append(y_test)\n",
    "    \n",
    "for i in range(len(meshes)):\n",
    "\n",
    "    X=all_input[i]\n",
    "    y=all_output[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=seed+i\n",
    "    )\n",
    "    train_input_modes.append(X_train)\n",
    "    test_input_modes.append(X_test)\n",
    "    train_output_modes.append(y_train)\n",
    "    test_output_modes.append(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a8dda",
   "metadata": {},
   "source": [
    "# Emulator per mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff58820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "emulators=[]\n",
    "for i in range(len(meshes)):\n",
    "    emulators.append(GPE.ensemble(train_input[i],train_output[i],mean_func=\"linear\",training_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3056f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dda116d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPyEm/GPE_ensemble.py:208: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3618.)\n",
      "  prediction=torch.stack(prediction).T\n"
     ]
    }
   ],
   "source": [
    "R2 = torch.zeros(len(meshes),2)\n",
    "R2_std = torch.zeros(len(meshes),2)\n",
    "for i in range(len(meshes)):\n",
    "    meanR, stdR = emulators[i].R2_sample(test_input[i],test_output[i],n=1000)\n",
    "    R2[i,:]=meanR\n",
    "    R2_std[i,:] = stdR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2beb20e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9983, 0.9980, 0.9984, 0.9985, 0.9992, 0.9982, 0.9988, 0.9977, 0.9981,\n",
       "        0.9946, 0.9978, 0.9972, 0.9986, 0.9984, 0.9972, 0.9959, 0.9980, 0.9974,\n",
       "        0.9987])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3aec3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0003, 0.0006],\n",
       "        [0.0002, 0.0009],\n",
       "        [0.0003, 0.0005],\n",
       "        [0.0002, 0.0005],\n",
       "        [0.0002, 0.0002],\n",
       "        [0.0002, 0.0005],\n",
       "        [0.0002, 0.0004],\n",
       "        [0.0004, 0.0007],\n",
       "        [0.0005, 0.0005],\n",
       "        [0.0003, 0.0019],\n",
       "        [0.0003, 0.0012],\n",
       "        [0.0006, 0.0009],\n",
       "        [0.0003, 0.0004],\n",
       "        [0.0001, 0.0009],\n",
       "        [0.0003, 0.0018],\n",
       "        [0.0009, 0.0019],\n",
       "        [0.0004, 0.0008],\n",
       "        [0.0009, 0.0004],\n",
       "        [0.0004, 0.0006]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae3826e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "fontS=16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b4da4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax.scatter(meshes,R2[:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2[:,0].detach().numpy(),fmt='o',yerr=R2_std[:,0].detach().numpy())\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "ax.scatter(meshes,R2[:,1].detach().numpy())\n",
    "plt.errorbar(meshes,R2[:,1].detach().numpy(),fmt='o',yerr=R2_std[:,1].detach().numpy())\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTIndivEm.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d48880",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=[20,40,60,80,100,120,140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = torch.zeros(len(meshes),len(nn),2)\n",
    "R2_std = torch.zeros(len(meshes),len(nn),2)\n",
    "reps=5\n",
    "for i in range(len(meshes)):\n",
    "    for j in range(len(nn)):\n",
    "        for k in range(reps):\n",
    "            X=train_input[i]\n",
    "            y=train_output[i]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                train_size=nn[j],\n",
    "                random_state=k\n",
    "            )\n",
    "            emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "            meanR, stdR = emulator.R2_sample(test_input[i],test_output[i],n=1000)\n",
    "            R2[i,j,:]+=meanR/reps\n",
    "            R2_std[i,j,:] += stdR/reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697aa666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 1.01)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(nn,R2.mean(axis=0)[:,0].detach().numpy(),fmt='o',yerr=R2_std.mean(axis=0)[:,0].detach().numpy())\n",
    "plt.errorbar(nn,R2.mean(axis=0)[:,1].detach().numpy(),fmt='o',yerr=R2_std.mean(axis=0)[:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.ylim(0.7,1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90e67376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 40, 60, 80, 100, 120, 140]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed729f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8927, 0.9081],\n",
       "        [0.9901, 0.9763],\n",
       "        [0.9954, 0.9880],\n",
       "        [0.9969, 0.9925],\n",
       "        [0.9979, 0.9945],\n",
       "        [0.9983, 0.9958],\n",
       "        [0.9986, 0.9968]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2.mean(axis=0)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c9821",
   "metadata": {},
   "source": [
    "# Emulator trained with 17/18 meshes and evaluated on the left out mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed54d894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 200., 300., 400., 500., 600., 700., 800.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(100,800,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "40f6432b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.9974, 0.9941]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.9974, 0.9941]],\n",
      "\n",
      "        [[0.9975, 0.9947],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.9974, 0.9941]],\n",
      "\n",
      "        [[0.9975, 0.9947],\n",
      "         [0.9976, 0.9951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.9974, 0.9941]],\n",
      "\n",
      "        [[0.9975, 0.9947],\n",
      "         [0.9976, 0.9951],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.9974, 0.9941]],\n",
      "\n",
      "        [[0.9975, 0.9947],\n",
      "         [0.9976, 0.9951],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.9974, 0.9941]],\n",
      "\n",
      "        [[0.9975, 0.9947],\n",
      "         [0.9976, 0.9951],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9980, 0.9953]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.9974, 0.9941]],\n",
      "\n",
      "        [[0.9975, 0.9947],\n",
      "         [0.9976, 0.9951],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9980, 0.9953]],\n",
      "\n",
      "        [[0.9819, 0.9860],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9829, 0.9767],\n",
      "         [0.9858, 0.9708],\n",
      "         [0.9853, 0.9747],\n",
      "         [0.9873, 0.9784],\n",
      "         [0.9881, 0.9656]],\n",
      "\n",
      "        [[0.9909, 0.9842],\n",
      "         [0.9901, 0.9821],\n",
      "         [0.9915, 0.9831],\n",
      "         [0.9933, 0.9830],\n",
      "         [0.9925, 0.9830]],\n",
      "\n",
      "        [[0.9928, 0.9876],\n",
      "         [0.9943, 0.9869],\n",
      "         [0.9950, 0.9873],\n",
      "         [0.9956, 0.9880],\n",
      "         [0.9949, 0.9890]],\n",
      "\n",
      "        [[0.9952, 0.9918],\n",
      "         [0.9959, 0.9894],\n",
      "         [0.9961, 0.9906],\n",
      "         [0.9957, 0.9916],\n",
      "         [0.9961, 0.9908]],\n",
      "\n",
      "        [[0.9963, 0.9922],\n",
      "         [0.9966, 0.9923],\n",
      "         [0.9966, 0.9930],\n",
      "         [0.9969, 0.9928],\n",
      "         [0.9971, 0.9923]],\n",
      "\n",
      "        [[0.9968, 0.9932],\n",
      "         [0.9965, 0.9922],\n",
      "         [0.9972, 0.9938],\n",
      "         [0.9973, 0.9943],\n",
      "         [0.9972, 0.9942]],\n",
      "\n",
      "        [[0.9970, 0.9935],\n",
      "         [0.9971, 0.9943],\n",
      "         [0.9974, 0.9949],\n",
      "         [0.9975, 0.9943],\n",
      "         [0.9974, 0.9941]],\n",
      "\n",
      "        [[0.9975, 0.9947],\n",
      "         [0.9976, 0.9951],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9980, 0.9953]],\n",
      "\n",
      "        [[0.9819, 0.9860],\n",
      "         [0.9748, 0.9845],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat(train_output_modes[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m17\u001b[39m])\n\u001b[1;32m     10\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     11\u001b[0m     X,\n\u001b[1;32m     12\u001b[0m     y,\n\u001b[1;32m     13\u001b[0m     train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(train_p[i]),\n\u001b[1;32m     14\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mj\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m emulator\u001b[38;5;241m=\u001b[39mGPE\u001b[38;5;241m.\u001b[39mensemble(X_train,y_train,mean_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,training_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     17\u001b[0m meanR, stdR \u001b[38;5;241m=\u001b[39m emulator\u001b[38;5;241m.\u001b[39mR2_sample(torch\u001b[38;5;241m.\u001b[39mcat(test_input_modes[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m17\u001b[39m])[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m15\u001b[39m],torch\u001b[38;5;241m.\u001b[39mcat(test_output_modes[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m17\u001b[39m]),\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     19\u001b[0m R2_test[i,j,:]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mmeanR\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:22\u001b[0m, in \u001b[0;36mensemble.__init__\u001b[0;34m(self, X_train, y_train, mean_func, training_iter, kernel, kernel_params, ref_emulator, a, a_indicator)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m=\u001b[39mkernel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_params\u001b[38;5;241m=\u001b[39mkernel_params\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_ensemble()\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:108\u001b[0m, in \u001b[0;36mensemble.create_ensemble\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, Y)\n\u001b[0;32m--> 108\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# print('Iter %d/%d - Loss: %.3f' % (\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#     j + 1, training_iter, loss.item()\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# ))\u001b[39;00m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:288\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/functions/_inv_quad_logdet.py:209\u001b[0m, in \u001b[0;36mInvQuadLogdet.backward\u001b[0;34m(ctx, inv_quad_grad_output, logdet_grad_output)\u001b[0m\n\u001b[1;32m    207\u001b[0m left_factors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(left_factors_list, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    208\u001b[0m right_factors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(right_factors_list, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 209\u001b[0m matrix_arg_grads \u001b[38;5;241m=\u001b[39m linear_op\u001b[38;5;241m.\u001b[39m_bilinear_derivative(left_factors, right_factors)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# precond gradient\u001b[39;00m\n\u001b[1;32m    212\u001b[0m precond_arg_grads \u001b[38;5;241m=\u001b[39m precond_lt\u001b[38;5;241m.\u001b[39m_bilinear_derivative(\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;241m-\u001b[39mprecond_probe_vectors \u001b[38;5;241m*\u001b[39m coef, precond_probe_vectors \u001b[38;5;241m*\u001b[39m logdet_grad_output\n\u001b[1;32m    214\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/sum_linear_operator.py:58\u001b[0m, in \u001b[0;36mSumLinearOperator._bilinear_derivative\u001b[0;34m(self, left_vecs, right_vecs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bilinear_derivative\u001b[39m(\u001b[38;5;28mself\u001b[39m, left_vecs: Tensor, right_vecs: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Tensor], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m     59\u001b[0m         var \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39m_bilinear_derivative(left_vecs, right_vecs)\n\u001b[1;32m     60\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/sum_linear_operator.py:59\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bilinear_derivative\u001b[39m(\u001b[38;5;28mself\u001b[39m, left_vecs: Tensor, right_vecs: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Tensor], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m---> 59\u001b[0m         var \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39m_bilinear_derivative(left_vecs, right_vecs)\n\u001b[1;32m     60\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/dense_linear_operator.py:71\u001b[0m, in \u001b[0;36mDenseLinearOperator._bilinear_derivative\u001b[0;34m(self, left_vecs, right_vecs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bilinear_derivative\u001b[39m(\u001b[38;5;28mself\u001b[39m, left_vecs: Tensor, right_vecs: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Tensor], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m---> 71\u001b[0m     res \u001b[38;5;241m=\u001b[39m left_vecs\u001b[38;5;241m.\u001b[39mmatmul(right_vecs\u001b[38;5;241m.\u001b[39mmT)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (res,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reps = 5\n",
    "train_p = np.linspace(100,800,8)\n",
    "train_p=np.array([10,15,20,25,30,35,40,45,50])*17\n",
    "R2_test = torch.zeros(len(train_p),reps,2)\n",
    "R2_leftout= torch.zeros(len(train_p),reps,2)\n",
    "for i in range(len(train_p)):\n",
    "    for j in range(reps):\n",
    "        X=torch.cat(train_input_modes[0:17])[:,0:15]\n",
    "        y=torch.cat(train_output_modes[0:17])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            train_size=int(train_p[i]),\n",
    "            random_state=j\n",
    "        )\n",
    "        emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "        meanR, stdR = emulator.R2_sample(torch.cat(test_input_modes[0:17])[:,0:15],torch.cat(test_output_modes[0:17]),1000)\n",
    "        \n",
    "        R2_test[i,j,:]+=meanR\n",
    "        \n",
    "        meanR, stdR=emulator.R2_sample(test_input_modes[17][:,0:15],test_output_modes[17],1000) \n",
    "        R2_leftout[i,j,:] = meanR\n",
    "        print(R2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5e617da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884999999999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((0.993, 0.984))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caa15b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "tensor([0.9843, 0.9734])\n",
      "tensor([0.4551, 0.7718])\n",
      "0\n",
      "1\n",
      "tensor([0.9736, 0.9744])\n",
      "tensor([0.7614, 0.6320])\n",
      "0\n",
      "1\n",
      "tensor([0.9764, 0.9752])\n",
      "tensor([0.7546, 0.7965])\n",
      "0\n",
      "1\n",
      "tensor([0.9808, 0.9808])\n",
      "tensor([0.5848, 0.8122])\n",
      "0\n",
      "1\n",
      "tensor([0.9832, 0.9605])\n",
      "tensor([0.3620, 0.7870])\n",
      "0\n",
      "1\n",
      "tensor([0.9852, 0.9724])\n",
      "tensor([0.2058, 0.7683])\n",
      "0\n",
      "1\n",
      "tensor([0.9807, 0.9743])\n",
      "tensor([-0.3757,  0.9441])\n",
      "0\n",
      "1\n",
      "tensor([0.9749, 0.9760])\n",
      "tensor([-1.7247,  0.7607])\n",
      "0\n",
      "1\n",
      "tensor([0.9825, 0.9828])\n",
      "tensor([-0.3048,  0.8542])\n",
      "0\n",
      "1\n",
      "tensor([0.9823, 0.9615])\n",
      "tensor([-0.8988,  0.6209])\n",
      "0\n",
      "1\n",
      "tensor([0.9869, 0.9761])\n",
      "tensor([0.5882, 0.7345])\n",
      "0\n",
      "1\n",
      "tensor([0.9839, 0.9805])\n",
      "tensor([0.6790, 0.6875])\n",
      "0\n",
      "1\n",
      "tensor([0.9792, 0.9761])\n",
      "tensor([0.5872, 0.7453])\n",
      "0\n",
      "1\n",
      "tensor([0.9820, 0.9814])\n",
      "tensor([0.5745, 0.6982])\n",
      "0\n",
      "1\n",
      "tensor([0.9828, 0.9694])\n",
      "tensor([0.7014, 0.7182])\n",
      "0\n",
      "1\n",
      "tensor([0.9872, 0.9734])\n",
      "tensor([-3.0011,  0.2906])\n",
      "0\n",
      "1\n",
      "tensor([0.9847, 0.9784])\n",
      "tensor([-2.6177,  0.2700])\n",
      "0\n",
      "1\n",
      "tensor([0.9841, 0.9750])\n",
      "tensor([-1.3373,  0.2085])\n",
      "0\n",
      "1\n",
      "tensor([0.9851, 0.9788])\n",
      "tensor([-3.4752, -1.2280])\n",
      "0\n",
      "1\n",
      "tensor([0.9858, 0.9664])\n",
      "tensor([-2.6129,  0.3928])\n",
      "0\n",
      "1\n",
      "tensor([0.9871, 0.9730])\n",
      "tensor([0.6107, 0.9047])\n",
      "0\n",
      "1\n",
      "tensor([0.9839, 0.9802])\n",
      "tensor([0.5636, 0.9376])\n",
      "0\n",
      "1\n",
      "tensor([0.9782, 0.9778])\n",
      "tensor([0.5336, 0.9606])\n",
      "0\n",
      "1\n",
      "tensor([0.9715, 0.9811])\n",
      "tensor([-0.0156,  0.9551])\n",
      "0\n",
      "1\n",
      "tensor([0.9856, 0.9654])\n",
      "tensor([0.6110, 0.8884])\n",
      "0\n",
      "1\n",
      "tensor([0.9871, 0.9740])\n",
      "tensor([0.8555, 0.9475])\n",
      "0\n",
      "1\n",
      "tensor([0.9847, 0.9736])\n",
      "tensor([0.8795, 0.9438])\n",
      "0\n",
      "1\n",
      "tensor([0.9838, 0.9704])\n",
      "tensor([0.8318, 0.8662])\n",
      "0\n",
      "1\n",
      "tensor([0.9748, 0.9797])\n",
      "tensor([0.6032, 0.9619])\n",
      "0\n",
      "1\n",
      "tensor([0.9848, 0.9630])\n",
      "tensor([0.8597, 0.9380])\n",
      "0\n",
      "1\n",
      "tensor([0.9843, 0.9738])\n",
      "tensor([0.8254, 0.9568])\n",
      "0\n",
      "1\n",
      "tensor([0.9829, 0.9757])\n",
      "tensor([0.9702, 0.8658])\n",
      "0\n",
      "1\n",
      "tensor([0.9850, 0.9776])\n",
      "tensor([0.9370, 0.9588])\n",
      "0\n",
      "1\n",
      "tensor([0.9754, 0.9818])\n",
      "tensor([0.9129, 0.9454])\n",
      "0\n",
      "1\n",
      "tensor([0.9828, 0.9653])\n",
      "tensor([0.9771, 0.9301])\n",
      "0\n",
      "1\n",
      "tensor([0.9835, 0.9757])\n",
      "tensor([0.8790, 0.7505])\n",
      "0\n",
      "1\n",
      "tensor([0.9824, 0.9796])\n",
      "tensor([0.8443, 0.7298])\n",
      "0\n",
      "1\n",
      "tensor([0.9826, 0.9770])\n",
      "tensor([0.8180, 0.7341])\n",
      "0\n",
      "1\n",
      "tensor([0.9849, 0.9814])\n",
      "tensor([0.8782, 0.7841])\n",
      "0\n",
      "1\n",
      "tensor([0.9791, 0.9658])\n",
      "tensor([0.9313, 0.8394])\n",
      "0\n",
      "1\n",
      "tensor([0.9831, 0.9752])\n",
      "tensor([0.9148, 0.9423])\n",
      "0\n",
      "1\n",
      "tensor([0.9834, 0.9793])\n",
      "tensor([0.9137, 0.6589])\n",
      "0\n",
      "1\n",
      "tensor([0.9770, 0.9761])\n",
      "tensor([0.7029, 0.9085])\n",
      "0\n",
      "1\n",
      "tensor([0.9841, 0.9773])\n",
      "tensor([0.9454, 0.6226])\n",
      "0\n",
      "1\n",
      "tensor([0.9788, 0.9660])\n",
      "tensor([0.9117, 0.8990])\n",
      "0\n",
      "1\n",
      "tensor([0.9830, 0.9773])\n",
      "tensor([0.5499, 0.8087])\n",
      "0\n",
      "1\n",
      "tensor([0.9830, 0.9788])\n",
      "tensor([0.1154, 0.5330])\n",
      "0\n",
      "1\n",
      "tensor([0.9785, 0.9776])\n",
      "tensor([-0.0482,  0.7987])\n",
      "0\n",
      "1\n",
      "tensor([0.9877, 0.9802])\n",
      "tensor([0.1543, 0.8486])\n",
      "0\n",
      "1\n",
      "tensor([0.9783, 0.9718])\n",
      "tensor([0.9226, 0.6757])\n",
      "0\n",
      "1\n",
      "tensor([0.9825, 0.9751])\n",
      "tensor([0.5672, 0.8618])\n",
      "0\n",
      "1\n",
      "tensor([0.9843, 0.9771])\n",
      "tensor([0.8706, 0.3292])\n",
      "0\n",
      "1\n",
      "tensor([0.9818, 0.9760])\n",
      "tensor([0.7969, 0.6949])\n",
      "0\n",
      "1\n",
      "tensor([0.9870, 0.9769])\n",
      "tensor([ 0.8261, -0.0666])\n",
      "0\n",
      "1\n",
      "tensor([0.9800, 0.9692])\n",
      "tensor([0.9246, 0.8521])\n",
      "0\n",
      "1\n",
      "tensor([0.9817, 0.9730])\n",
      "tensor([0.9772, 0.9832])\n",
      "0\n",
      "1\n",
      "tensor([0.9843, 0.9780])\n",
      "tensor([0.9680, 0.9301])\n",
      "0\n",
      "1\n",
      "tensor([0.9820, 0.9704])\n",
      "tensor([0.9756, 0.9690])\n",
      "0\n",
      "1\n",
      "tensor([0.9860, 0.9805])\n",
      "tensor([0.9579, 0.9501])\n",
      "0\n",
      "1\n",
      "tensor([0.9768, 0.9638])\n",
      "tensor([0.9735, 0.9638])\n",
      "0\n",
      "1\n",
      "tensor([0.9818, 0.9759])\n",
      "tensor([0.8183, 0.6302])\n",
      "0\n",
      "1\n",
      "tensor([0.9842, 0.9768])\n",
      "tensor([0.7713, 0.8511])\n",
      "0\n",
      "1\n",
      "tensor([0.9842, 0.9707])\n",
      "tensor([ 0.7722, -0.4332])\n",
      "0\n",
      "1\n",
      "tensor([0.9874, 0.9811])\n",
      "tensor([0.6877, 0.4137])\n",
      "0\n",
      "1\n",
      "tensor([0.9853, 0.9647])\n",
      "tensor([0.7655, 0.7192])\n",
      "0\n",
      "1\n",
      "tensor([0.9804, 0.9752])\n",
      "tensor([0.7831, 0.8722])\n",
      "0\n",
      "1\n",
      "tensor([0.9833, 0.9768])\n",
      "tensor([0.8314, 0.9155])\n",
      "0\n",
      "1\n",
      "tensor([0.9825, 0.9712])\n",
      "tensor([0.7834, 0.6889])\n",
      "0\n",
      "1\n",
      "tensor([0.9853, 0.9820])\n",
      "tensor([-0.2044,  0.7082])\n",
      "0\n",
      "1\n",
      "tensor([0.9816, 0.9715])\n",
      "tensor([0.5755, 0.7110])\n",
      "0\n",
      "1\n",
      "tensor([0.9876, 0.9782])\n",
      "tensor([0.5820, 0.8036])\n",
      "0\n",
      "1\n",
      "tensor([0.9856, 0.9748])\n",
      "tensor([0.6699, 0.6715])\n",
      "0\n",
      "1\n",
      "tensor([0.9861, 0.9798])\n",
      "tensor([0.6085, 0.6937])\n",
      "0\n",
      "1\n",
      "tensor([0.9894, 0.9812])\n",
      "tensor([0.6167, 0.6728])\n",
      "0\n",
      "1\n",
      "tensor([0.9874, 0.9773])\n",
      "tensor([0.6441, 0.7468])\n",
      "0\n",
      "1\n",
      "tensor([0.9820, 0.9823])\n",
      "tensor([0.8904, 0.8948])\n",
      "0\n",
      "1\n",
      "tensor([0.9838, 0.9754])\n",
      "tensor([0.9789, 0.7628])\n",
      "0\n",
      "1\n",
      "tensor([0.9824, 0.9746])\n",
      "tensor([0.9598, 0.8137])\n",
      "0\n",
      "1\n",
      "tensor([0.9871, 0.9840])\n",
      "tensor([0.8055, 0.8851])\n",
      "0\n",
      "1\n",
      "tensor([0.9827, 0.9729])\n",
      "tensor([0.9656, 0.8246])\n",
      "0\n",
      "1\n",
      "tensor([0.9811, 0.9779])\n",
      "tensor([0.6580, 0.8399])\n",
      "0\n",
      "1\n",
      "tensor([0.9838, 0.9726])\n",
      "tensor([-1.4697,  0.8273])\n",
      "0\n",
      "1\n",
      "tensor([0.9768, 0.9743])\n",
      "tensor([0.9028, 0.7508])\n",
      "0\n",
      "1\n",
      "tensor([0.9860, 0.9837])\n",
      "tensor([0.8803, 0.6943])\n",
      "0\n",
      "1\n",
      "tensor([0.9846, 0.9698])\n",
      "tensor([-0.0952,  0.9394])\n",
      "0\n",
      "1\n",
      "tensor([0.9821, 0.9791])\n",
      "tensor([0.3165, 0.9481])\n",
      "0\n",
      "1\n",
      "tensor([0.9799, 0.9707])\n",
      "tensor([0.3021, 0.9132])\n",
      "0\n",
      "1\n",
      "tensor([0.9762, 0.9732])\n",
      "tensor([0.9055, 0.9111])\n",
      "0\n",
      "1\n",
      "tensor([0.9861, 0.9818])\n",
      "tensor([0.2785, 0.9547])\n",
      "0\n",
      "1\n",
      "tensor([0.9841, 0.9658])\n",
      "tensor([0.8728, 0.9413])\n",
      "0\n",
      "1\n",
      "tensor([0.9815, 0.9784])\n",
      "tensor([0.3957, 0.8779])\n",
      "0\n",
      "1\n",
      "tensor([0.9851, 0.9704])\n",
      "tensor([0.4341, 0.9653])\n",
      "0\n",
      "1\n",
      "tensor([0.9845, 0.9771])\n",
      "tensor([0.1016, 0.8995])\n",
      "0\n",
      "1\n",
      "tensor([0.9874, 0.9813])\n",
      "tensor([0.5620, 0.8965])\n",
      "0\n",
      "1\n",
      "tensor([0.9870, 0.9679])\n",
      "tensor([0.5140, 0.9571])\n",
      "0\n",
      "1\n",
      "tensor([0.9926, 0.9818])\n",
      "tensor([0.5489, 0.8474])\n",
      "0\n",
      "1\n",
      "tensor([0.9885, 0.9853])\n",
      "tensor([0.8866, 0.5839])\n",
      "0\n",
      "1\n",
      "tensor([0.9899, 0.9850])\n",
      "tensor([0.6251, 0.8702])\n",
      "0\n",
      "1\n",
      "tensor([0.9902, 0.9845])\n",
      "tensor([0.7783, 0.8695])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9822])\n",
      "tensor([0.3970, 0.8707])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9806])\n",
      "tensor([-1.9339,  0.9113])\n",
      "0\n",
      "1\n",
      "tensor([0.9898, 0.9862])\n",
      "tensor([-0.7965,  0.6881])\n",
      "0\n",
      "1\n",
      "tensor([0.9892, 0.9855])\n",
      "tensor([-0.9664,  0.8486])\n",
      "0\n",
      "1\n",
      "tensor([0.9900, 0.9863])\n",
      "tensor([-0.6242,  0.8682])\n",
      "0\n",
      "1\n",
      "tensor([0.9921, 0.9831])\n",
      "tensor([-1.0972,  0.6407])\n",
      "0\n",
      "1\n",
      "tensor([0.9917, 0.9839])\n",
      "tensor([0.6399, 0.7352])\n",
      "0\n",
      "1\n",
      "tensor([0.9882, 0.9859])\n",
      "tensor([0.5456, 0.8156])\n",
      "0\n",
      "1\n",
      "tensor([0.9887, 0.9874])\n",
      "tensor([0.5641, 0.6371])\n",
      "0\n",
      "1\n",
      "tensor([0.9907, 0.9855])\n",
      "tensor([0.5964, 0.7743])\n",
      "0\n",
      "1\n",
      "tensor([0.9919, 0.9834])\n",
      "tensor([0.8677, 0.7763])\n",
      "0\n",
      "1\n",
      "tensor([0.9928, 0.9822])\n",
      "tensor([-3.2460,  0.4676])\n",
      "0\n",
      "1\n",
      "tensor([0.9893, 0.9849])\n",
      "tensor([-2.9262,  0.6223])\n",
      "0\n",
      "1\n",
      "tensor([0.9921, 0.9869])\n",
      "tensor([-3.0168,  0.4100])\n",
      "0\n",
      "1\n",
      "tensor([0.9906, 0.9840])\n",
      "tensor([-1.6173, -2.2164])\n",
      "0\n",
      "1\n",
      "tensor([0.9929, 0.9812])\n",
      "tensor([-2.3136,  0.7495])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9834])\n",
      "tensor([0.5628, 0.9126])\n",
      "0\n",
      "1\n",
      "tensor([0.9904, 0.9859])\n",
      "tensor([0.4957, 0.9401])\n",
      "0\n",
      "1\n",
      "tensor([0.9913, 0.9867])\n",
      "tensor([0.5478, 0.8903])\n",
      "0\n",
      "1\n",
      "tensor([0.9909, 0.9837])\n",
      "tensor([0.4510, 0.9579])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9837])\n",
      "tensor([0.5426, 0.9481])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9834])\n",
      "tensor([0.8179, 0.9620])\n",
      "0\n",
      "1\n",
      "tensor([0.9892, 0.9866])\n",
      "tensor([0.8753, 0.9474])\n",
      "0\n",
      "1\n",
      "tensor([0.9906, 0.9851])\n",
      "tensor([0.8340, 0.9678])\n",
      "0\n",
      "1\n",
      "tensor([0.9887, 0.9823])\n",
      "tensor([0.8081, 0.9627])\n",
      "0\n",
      "1\n",
      "tensor([0.9920, 0.9827])\n",
      "tensor([0.8500, 0.9728])\n",
      "0\n",
      "1\n",
      "tensor([0.9915, 0.9833])\n",
      "tensor([0.7689, 0.9313])\n",
      "0\n",
      "1\n",
      "tensor([0.9893, 0.9852])\n",
      "tensor([0.9781, 0.8144])\n",
      "0\n",
      "1\n",
      "tensor([0.9912, 0.9852])\n",
      "tensor([0.9515, 0.9113])\n",
      "0\n",
      "1\n",
      "tensor([0.9897, 0.9836])\n",
      "tensor([0.8668, 0.9415])\n",
      "0\n",
      "1\n",
      "tensor([0.9920, 0.9843])\n",
      "tensor([0.9820, 0.9507])\n",
      "0\n",
      "1\n",
      "tensor([0.9907, 0.9841])\n",
      "tensor([0.9013, 0.7580])\n",
      "0\n",
      "1\n",
      "tensor([0.9896, 0.9871])\n",
      "tensor([0.8555, 0.7758])\n",
      "0\n",
      "1\n",
      "tensor([0.9907, 0.9856])\n",
      "tensor([0.8946, 0.8386])\n",
      "0\n",
      "1\n",
      "tensor([0.9903, 0.9869])\n",
      "tensor([0.8706, 0.6643])\n",
      "0\n",
      "1\n",
      "tensor([0.9904, 0.9841])\n",
      "tensor([0.9271, 0.7686])\n",
      "0\n",
      "1\n",
      "tensor([0.9920, 0.9852])\n",
      "tensor([0.5545, 0.8432])\n",
      "0\n",
      "1\n",
      "tensor([0.9896, 0.9865])\n",
      "tensor([0.9240, 0.6964])\n",
      "0\n",
      "1\n",
      "tensor([0.9897, 0.9871])\n",
      "tensor([0.9203, 0.6223])\n",
      "0\n",
      "1\n",
      "tensor([0.9893, 0.9814])\n",
      "tensor([0.3742, 0.6509])\n",
      "0\n",
      "1\n",
      "tensor([0.9921, 0.9848])\n",
      "tensor([0.9388, 0.7236])\n",
      "0\n",
      "1\n",
      "tensor([0.9918, 0.9853])\n",
      "tensor([0.8828, 0.7186])\n",
      "0\n",
      "1\n",
      "tensor([0.9892, 0.9874])\n",
      "tensor([0.2674, 0.8173])\n",
      "0\n",
      "1\n",
      "tensor([0.9905, 0.9846])\n",
      "tensor([0.1002, 0.8313])\n",
      "0\n",
      "1\n",
      "tensor([0.9906, 0.9854])\n",
      "tensor([0.1562, 0.8270])\n",
      "0\n",
      "1\n",
      "tensor([0.9919, 0.9850])\n",
      "tensor([0.0959, 0.7808])\n",
      "0\n",
      "1\n",
      "tensor([0.9913, 0.9842])\n",
      "tensor([0.8117, 0.9433])\n",
      "0\n",
      "1\n",
      "tensor([0.9914, 0.9874])\n",
      "tensor([0.8924, 0.6780])\n",
      "0\n",
      "1\n",
      "tensor([0.9915, 0.9854])\n",
      "tensor([0.8817, 0.8668])\n",
      "0\n",
      "1\n",
      "tensor([0.9922, 0.9838])\n",
      "tensor([0.7872, 0.0051])\n",
      "0\n",
      "1\n",
      "tensor([0.9927, 0.9854])\n",
      "tensor([0.8766, 0.8405])\n",
      "0\n",
      "1\n",
      "tensor([0.9919, 0.9837])\n",
      "tensor([0.7313, 0.9786])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9894, 0.9875])\n",
      "tensor([0.9717, 0.9754])\n",
      "0\n",
      "1\n",
      "tensor([0.9902, 0.9848])\n",
      "tensor([0.9825, 0.9576])\n",
      "0\n",
      "1\n",
      "tensor([0.9921, 0.9858])\n",
      "tensor([0.9718, 0.9449])\n",
      "0\n",
      "1\n",
      "tensor([0.9923, 0.9838])\n",
      "tensor([0.9805, 0.9552])\n",
      "0\n",
      "1\n",
      "tensor([0.9913, 0.9844])\n",
      "tensor([0.9223, 0.5817])\n",
      "0\n",
      "1\n",
      "tensor([0.9920, 0.9876])\n",
      "tensor([0.7465, 0.8448])\n",
      "0\n",
      "1\n",
      "tensor([0.9917, 0.9872])\n",
      "tensor([0.8609, 0.3906])\n",
      "0\n",
      "1\n",
      "tensor([0.9928, 0.9877])\n",
      "tensor([0.7037, 0.3527])\n",
      "0\n",
      "1\n",
      "tensor([0.9919, 0.9825])\n",
      "tensor([0.7785, 0.5351])\n",
      "0\n",
      "1\n",
      "tensor([0.9906, 0.9821])\n",
      "tensor([0.8175, 0.6976])\n",
      "0\n",
      "1\n",
      "tensor([0.9905, 0.9881])\n",
      "tensor([0.8357, 0.8852])\n",
      "0\n",
      "1\n",
      "tensor([0.9901, 0.9862])\n",
      "tensor([0.8491, 0.7765])\n",
      "0\n",
      "1\n",
      "tensor([0.9909, 0.9862])\n",
      "tensor([0.4987, 0.6935])\n",
      "0\n",
      "1\n",
      "tensor([0.9927, 0.9830])\n",
      "tensor([0.8511, 0.7958])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9855])\n",
      "tensor([0.6442, 0.7433])\n",
      "0\n",
      "1\n",
      "tensor([0.9914, 0.9872])\n",
      "tensor([0.7142, 0.7711])\n",
      "0\n",
      "1\n",
      "tensor([0.9921, 0.9864])\n",
      "tensor([0.5929, 0.6832])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9892])\n",
      "tensor([0.5796, 0.7511])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9839])\n",
      "tensor([0.7617, 0.7810])\n",
      "0\n",
      "1\n",
      "tensor([0.9900, 0.9869])\n",
      "tensor([0.9677, 0.8880])\n",
      "0\n",
      "1\n",
      "tensor([0.9900, 0.9881])\n",
      "tensor([0.9773, 0.7918])\n",
      "0\n",
      "1\n",
      "tensor([0.9900, 0.9862])\n",
      "tensor([0.7848, 0.7963])\n",
      "0\n",
      "1\n",
      "tensor([0.9919, 0.9877])\n",
      "tensor([0.9662, 0.8682])\n",
      "0\n",
      "1\n",
      "tensor([0.9920, 0.9853])\n",
      "tensor([0.9630, 0.7913])\n",
      "0\n",
      "1\n",
      "tensor([0.9911, 0.9859])\n",
      "tensor([0.6887, 0.6641])\n",
      "0\n",
      "1\n",
      "tensor([0.9902, 0.9865])\n",
      "tensor([-1.4284,  0.6693])\n",
      "0\n",
      "1\n",
      "tensor([0.9910, 0.9856])\n",
      "tensor([0.2444, 0.7161])\n",
      "0\n",
      "1\n",
      "tensor([0.9925, 0.9861])\n",
      "tensor([-2.1549,  0.9100])\n",
      "0\n",
      "1\n",
      "tensor([0.9924, 0.9837])\n",
      "tensor([0.0964, 0.4853])\n",
      "0\n",
      "1\n",
      "tensor([0.9908, 0.9846])\n",
      "tensor([0.2539, 0.9343])\n",
      "0\n",
      "1\n",
      "tensor([0.9887, 0.9855])\n",
      "tensor([0.8875, 0.9380])\n",
      "0\n",
      "1\n",
      "tensor([0.9907, 0.9847])\n",
      "tensor([0.3402, 0.9363])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9865])\n",
      "tensor([0.4103, 0.9487])\n",
      "0\n",
      "1\n",
      "tensor([0.9911, 0.9838])\n",
      "tensor([0.9158, 0.8160])\n",
      "0\n",
      "1\n",
      "tensor([0.9912, 0.9850])\n",
      "tensor([0.3877, 0.8569])\n",
      "0\n",
      "1\n",
      "tensor([0.9892, 0.9820])\n",
      "tensor([0.4633, 0.9365])\n",
      "0\n",
      "1\n",
      "tensor([0.9920, 0.9850])\n",
      "tensor([0.2407, 0.9396])\n",
      "0\n",
      "1\n",
      "tensor([0.9937, 0.9875])\n",
      "tensor([0.5504, 0.8416])\n",
      "0\n",
      "1\n",
      "tensor([0.9923, 0.9836])\n",
      "tensor([0.4156, 0.9067])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9903])\n",
      "tensor([0.4791, 0.8312])\n",
      "0\n",
      "1\n",
      "tensor([0.9938, 0.9899])\n",
      "tensor([0.8564, 0.4753])\n",
      "0\n",
      "1\n",
      "tensor([0.9934, 0.9910])\n",
      "tensor([0.7414, 0.8121])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9915])\n",
      "tensor([0.7657, 0.8266])\n",
      "0\n",
      "1\n",
      "tensor([0.9936, 0.9879])\n",
      "tensor([0.6633, 0.7458])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9889])\n",
      "tensor([-2.1516,  0.8951])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9892])\n",
      "tensor([-1.7390,  0.8977])\n",
      "0\n",
      "1\n",
      "tensor([0.9934, 0.9904])\n",
      "tensor([-0.9300,  0.9118])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9912])\n",
      "tensor([-0.9206,  0.9003])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9882])\n",
      "tensor([-0.5783,  0.7534])\n",
      "0\n",
      "1\n",
      "tensor([0.9941, 0.9896])\n",
      "tensor([0.6388, 0.7772])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9895])\n",
      "tensor([0.6587, 0.8142])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9901])\n",
      "tensor([0.5810, 0.7370])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9901])\n",
      "tensor([0.6118, 0.7990])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9892])\n",
      "tensor([0.7794, 0.7686])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9898])\n",
      "tensor([-3.0898,  0.9482])\n",
      "0\n",
      "1\n",
      "tensor([0.9939, 0.9885])\n",
      "tensor([-2.8342,  0.7846])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9892])\n",
      "tensor([-2.8442,  0.1254])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9901])\n",
      "tensor([-3.3073,  0.4963])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9876])\n",
      "tensor([-2.9445, -1.0437])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9897])\n",
      "tensor([0.9773, 0.9240])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9913])\n",
      "tensor([0.5730, 0.9602])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9908])\n",
      "tensor([0.5909, 0.9296])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9912])\n",
      "tensor([0.5561, 0.9608])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9893])\n",
      "tensor([0.4881, 0.9547])\n",
      "0\n",
      "1\n",
      "tensor([0.9938, 0.9887])\n",
      "tensor([0.9413, 0.9584])\n",
      "0\n",
      "1\n",
      "tensor([0.9929, 0.9904])\n",
      "tensor([0.8643, 0.9384])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9896])\n",
      "tensor([0.7824, 0.9642])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9906])\n",
      "tensor([0.8374, 0.9743])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9896])\n",
      "tensor([0.8047, 0.9717])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9887])\n",
      "tensor([0.9674, 0.9466])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9897])\n",
      "tensor([0.9822, 0.9513])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9894])\n",
      "tensor([0.9587, 0.8996])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9908])\n",
      "tensor([0.9702, 0.9023])\n",
      "0\n",
      "1\n",
      "tensor([0.9940, 0.9880])\n",
      "tensor([0.9142, 0.9479])\n",
      "0\n",
      "1\n",
      "tensor([0.9940, 0.9877])\n",
      "tensor([0.9459, 0.7845])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9899])\n",
      "tensor([0.9262, 0.7351])\n",
      "0\n",
      "1\n",
      "tensor([0.9939, 0.9895])\n",
      "tensor([0.9019, 0.8619])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9910])\n",
      "tensor([0.9364, 0.6836])\n",
      "0\n",
      "1\n",
      "tensor([0.9932, 0.9902])\n",
      "tensor([0.9234, 0.7606])\n",
      "0\n",
      "1\n",
      "tensor([0.9939, 0.9891])\n",
      "tensor([0.9310, 0.8791])\n",
      "0\n",
      "1\n",
      "tensor([0.9929, 0.9899])\n",
      "tensor([0.8981, 0.7992])\n",
      "0\n",
      "1\n",
      "tensor([0.9932, 0.9900])\n",
      "tensor([0.8611, 0.6680])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9907])\n",
      "tensor([0.8964, 0.7476])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9909])\n",
      "tensor([0.5601, 0.7123])\n",
      "0\n",
      "1\n",
      "tensor([0.9935, 0.9882])\n",
      "tensor([0.8555, 0.7507])\n",
      "0\n",
      "1\n",
      "tensor([0.9931, 0.9900])\n",
      "tensor([0.3175, 0.8145])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9907])\n",
      "tensor([0.9676, 0.9421])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9906])\n",
      "tensor([0.5030, 0.7992])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9901])\n",
      "tensor([0.9285, 0.7345])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9890])\n",
      "tensor([0.9438, 0.9467])\n",
      "0\n",
      "1\n",
      "tensor([0.9941, 0.9911])\n",
      "tensor([0.9254, 0.9075])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9907])\n",
      "tensor([0.8417, 0.8533])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9904])\n",
      "tensor([0.7786, 0.1398])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9903])\n",
      "tensor([0.8732, 0.8670])\n",
      "0\n",
      "1\n",
      "tensor([0.9929, 0.9894])\n",
      "tensor([0.9862, 0.9768])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9912])\n",
      "tensor([0.9850, 0.9671])\n",
      "0\n",
      "1\n",
      "tensor([0.9949, 0.9906])\n",
      "tensor([0.9679, 0.9700])\n",
      "0\n",
      "1\n",
      "tensor([0.9934, 0.9900])\n",
      "tensor([0.9674, 0.9607])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9894])\n",
      "tensor([0.9800, 0.9645])\n",
      "0\n",
      "1\n",
      "tensor([0.9929, 0.9878])\n",
      "tensor([0.8776, 0.2920])\n",
      "0\n",
      "1\n",
      "tensor([0.9945, 0.9904])\n",
      "tensor([0.7673, 0.7301])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9904])\n",
      "tensor([0.8252, 0.4981])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9914])\n",
      "tensor([0.7497, 0.4053])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9890])\n",
      "tensor([0.8592, 0.4962])\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9894])\n",
      "tensor([0.8355, 0.8605])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9905])\n",
      "tensor([0.7716, 0.8122])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9902])\n",
      "tensor([0.8320, 0.7286])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9870])\n",
      "tensor([0.7567, 0.6487])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9892])\n",
      "tensor([0.8986, 0.8016])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9895])\n",
      "tensor([0.7122, 0.7289])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9893])\n",
      "tensor([0.7500, 0.7712])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9893])\n",
      "tensor([0.7290, 0.7179])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9910])\n",
      "tensor([0.6844, 0.7613])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9904])\n",
      "tensor([0.7259, 0.7260])\n",
      "0\n",
      "1\n",
      "tensor([0.9933, 0.9901])\n",
      "tensor([0.9722, 0.9168])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9903])\n",
      "tensor([0.9819, 0.9209])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9907])\n",
      "tensor([0.9600, 0.7743])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9901])\n",
      "tensor([0.9805, 0.8883])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9887])\n",
      "tensor([0.9746, 0.7850])\n",
      "0\n",
      "1\n",
      "tensor([0.9936, 0.9886])\n",
      "tensor([0.1127, 0.9141])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9889])\n",
      "tensor([-1.8435,  0.8045])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9892])\n",
      "tensor([0.1356, 0.7262])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9883])\n",
      "tensor([-2.4632,  0.6956])\n",
      "0\n",
      "1\n",
      "tensor([0.9943, 0.9896])\n",
      "tensor([0.3587, 0.5111])\n",
      "0\n",
      "1\n",
      "tensor([0.9931, 0.9880])\n",
      "tensor([0.3672, 0.9414])\n",
      "0\n",
      "1\n",
      "tensor([0.9937, 0.9888])\n",
      "tensor([0.7717, 0.9482])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9887])\n",
      "tensor([0.1667, 0.9439])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9878])\n",
      "tensor([0.7461, 0.9527])\n",
      "0\n",
      "1\n",
      "tensor([0.9939, 0.9890])\n",
      "tensor([0.7377, 0.9308])\n",
      "0\n",
      "1\n",
      "tensor([0.9936, 0.9886])\n",
      "tensor([0.0342, 0.8669])\n",
      "0\n",
      "1\n",
      "tensor([0.9942, 0.9863])\n",
      "tensor([0.2898, 0.8854])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9893])\n",
      "tensor([-0.0082,  0.9634])\n",
      "0\n",
      "1\n",
      "tensor([0.9938, 0.9869])\n",
      "tensor([0.1863, 0.8762])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9889])\n",
      "tensor([0.4097, 0.8701])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9928])\n",
      "tensor([0.6008, 0.7902])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9909])\n",
      "tensor([0.8483, 0.2932])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9920])\n",
      "tensor([0.8130, 0.6005])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9929])\n",
      "tensor([0.7934, 0.8725])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9915])\n",
      "tensor([0.6567, 0.8353])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9923])\n",
      "tensor([0.4052, 0.9068])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9889])\n",
      "tensor([-2.0987,  0.8303])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9917])\n",
      "tensor([-1.4918,  0.8730])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9923])\n",
      "tensor([-2.0018,  0.6448])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9917])\n",
      "tensor([-1.1774,  0.8576])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9929])\n",
      "tensor([0.6735, 0.7822])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9911])\n",
      "tensor([0.6509, 0.8192])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9910])\n",
      "tensor([0.6701, 0.7486])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9919])\n",
      "tensor([0.6028, 0.8333])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9911])\n",
      "tensor([0.8747, 0.7652])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9926])\n",
      "tensor([-2.8742,  0.7779])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9905])\n",
      "tensor([-3.3921,  0.4227])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9958, 0.9901])\n",
      "tensor([-2.7398,  0.3251])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9917])\n",
      "tensor([-3.6830,  0.6827])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9916])\n",
      "tensor([-4.3582,  0.3174])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9937])\n",
      "tensor([0.5585, 0.9439])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9920])\n",
      "tensor([0.5504, 0.9635])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9915])\n",
      "tensor([0.6517, 0.9403])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9928])\n",
      "tensor([0.5710, 0.9617])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9929])\n",
      "tensor([0.5035, 0.9435])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9920])\n",
      "tensor([0.7989, 0.9681])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9911])\n",
      "tensor([0.9356, 0.9822])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9915])\n",
      "tensor([0.8072, 0.9413])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9923])\n",
      "tensor([0.8491, 0.9634])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9915])\n",
      "tensor([0.7909, 0.9307])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9920])\n",
      "tensor([0.8740, 0.9689])\n",
      "0\n",
      "1\n",
      "tensor([0.9946, 0.9915])\n",
      "tensor([0.9820, 0.9274])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9904])\n",
      "tensor([0.8411, 0.9482])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9928])\n",
      "tensor([0.9652, 0.9088])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9912])\n",
      "tensor([0.9783, 0.9481])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9917])\n",
      "tensor([0.9617, 0.7719])\n",
      "0\n",
      "1\n",
      "tensor([0.9944, 0.9909])\n",
      "tensor([0.9784, 0.7668])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9903])\n",
      "tensor([0.9378, 0.7945])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9935])\n",
      "tensor([0.8693, 0.7318])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9919])\n",
      "tensor([0.9379, 0.8057])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9922])\n",
      "tensor([0.5279, 0.8650])\n",
      "0\n",
      "1\n",
      "tensor([0.9948, 0.9910])\n",
      "tensor([0.9408, 0.8110])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9913])\n",
      "tensor([0.9235, 0.7502])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9924])\n",
      "tensor([0.9596, 0.6923])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9928])\n",
      "tensor([0.4531, 0.6670])\n",
      "0\n",
      "1\n",
      "tensor([0.9951, 0.9918])\n",
      "tensor([0.9062, 0.8891])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9914])\n",
      "tensor([0.9274, 0.8068])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9925])\n",
      "tensor([0.3661, 0.8883])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9929])\n",
      "tensor([0.9766, 0.8355])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9922])\n",
      "tensor([0.8567, 0.7847])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9926])\n",
      "tensor([0.7618, 0.9247])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9923])\n",
      "tensor([0.9082, 0.9179])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9924])\n",
      "tensor([0.7780, 0.1496])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9928])\n",
      "tensor([0.9550, 0.5296])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9926])\n",
      "tensor([0.8295, 0.9259])\n",
      "0\n",
      "1\n",
      "tensor([0.9947, 0.9924])\n",
      "tensor([0.9784, 0.9723])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9928])\n",
      "tensor([0.9855, 0.9717])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9921])\n",
      "tensor([0.9504, 0.9224])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9923])\n",
      "tensor([0.9931, 0.9571])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9936])\n",
      "tensor([0.9782, 0.9672])\n",
      "0\n",
      "1\n",
      "tensor([0.9950, 0.9916])\n",
      "tensor([0.9473, 0.3847])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9919])\n",
      "tensor([0.8126, 0.7234])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9914])\n",
      "tensor([0.8619, 0.4633])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9937])\n",
      "tensor([0.7648, 0.5630])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9919])\n",
      "tensor([0.9507, 0.6594])\n",
      "0\n",
      "1\n",
      "tensor([0.9952, 0.9920])\n",
      "tensor([0.8506, 0.8692])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9922])\n",
      "tensor([0.8104, 0.8300])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9908])\n",
      "tensor([0.8422, 0.6126])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9929])\n",
      "tensor([0.5826, 0.8052])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9915])\n",
      "tensor([0.9067, 0.7613])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9919])\n",
      "tensor([0.7284, 0.7568])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9919])\n",
      "tensor([0.7487, 0.7734])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9914])\n",
      "tensor([0.7262, 0.6964])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9933])\n",
      "tensor([0.7129, 0.7434])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9924])\n",
      "tensor([0.7755, 0.7608])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9916])\n",
      "tensor([0.9727, 0.9266])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9935])\n",
      "tensor([0.9815, 0.8063])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9903])\n",
      "tensor([0.9178, 0.7976])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9934])\n",
      "tensor([0.7474, 0.8085])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9921])\n",
      "tensor([0.9113, 0.8937])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9911])\n",
      "tensor([-0.3889,  0.9023])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9910])\n",
      "tensor([-1.6642,  0.7434])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9910])\n",
      "tensor([-0.2605,  0.8917])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9939])\n",
      "tensor([-1.6024,  0.6466])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9927])\n",
      "tensor([0.3258, 0.5407])\n",
      "0\n",
      "1\n",
      "tensor([0.9939, 0.9900])\n",
      "tensor([0.9253, 0.9458])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9904])\n",
      "tensor([0.6391, 0.9380])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9904])\n",
      "tensor([0.2602, 0.9500])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9926])\n",
      "tensor([0.8362, 0.9499])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9918])\n",
      "tensor([0.6535, 0.9375])\n",
      "0\n",
      "1\n",
      "tensor([0.9953, 0.9918])\n",
      "tensor([0.4620, 0.8788])\n",
      "0\n",
      "1\n",
      "tensor([0.9954, 0.9886])\n",
      "tensor([0.5051, 0.8801])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9907])\n",
      "tensor([0.0507, 0.9274])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9936])\n",
      "tensor([0.5169, 0.8750])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9914])\n",
      "tensor([0.0694, 0.8792])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9941])\n",
      "tensor([0.7657, 0.6696])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9934])\n",
      "tensor([0.8287, 0.7333])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9933])\n",
      "tensor([0.7081, 0.6694])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9935])\n",
      "tensor([0.8049, 0.7240])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9931])\n",
      "tensor([0.5738, 0.8484])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9934])\n",
      "tensor([-2.2190,  0.5166])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9931])\n",
      "tensor([-2.1864,  0.8785])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9927])\n",
      "tensor([-1.4810,  0.9080])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9935])\n",
      "tensor([-0.7074,  0.8222])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9924])\n",
      "tensor([-2.0841,  0.9060])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9935])\n",
      "tensor([0.9634, 0.7894])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9927])\n",
      "tensor([0.6525, 0.7152])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9930])\n",
      "tensor([0.6425, 0.7426])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9936])\n",
      "tensor([0.6172, 0.8275])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9925])\n",
      "tensor([0.8443, 0.7740])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9937])\n",
      "tensor([-3.3474,  0.2772])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9924])\n",
      "tensor([-3.2522,  0.5531])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9926])\n",
      "tensor([-3.6722, -1.0377])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9933])\n",
      "tensor([-3.2282,  0.4601])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9925])\n",
      "tensor([-5.1345,  0.5579])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9945])\n",
      "tensor([0.9767, 0.8168])\n",
      "0\n",
      "1\n",
      "tensor([0.9958, 0.9933])\n",
      "tensor([0.5105, 0.9603])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9926])\n",
      "tensor([0.5344, 0.9530])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9946])\n",
      "tensor([0.6906, 0.9592])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9934])\n",
      "tensor([0.4866, 0.9463])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9939])\n",
      "tensor([0.8940, 0.9641])\n",
      "0\n",
      "1\n",
      "tensor([0.9957, 0.9928])\n",
      "tensor([0.8112, 0.9720])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9927])\n",
      "tensor([0.8022, 0.9538])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9940])\n",
      "tensor([0.7889, 0.9633])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9931])\n",
      "tensor([0.9375, 0.9591])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9934])\n",
      "tensor([0.8615, 0.9127])\n",
      "0\n",
      "1\n",
      "tensor([0.9955, 0.9925])\n",
      "tensor([0.8644, 0.9150])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9928])\n",
      "tensor([0.8327, 0.9547])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9942])\n",
      "tensor([0.9681, 0.9317])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9925])\n",
      "tensor([0.9721, 0.9594])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9941])\n",
      "tensor([0.9659, 0.7507])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9925])\n",
      "tensor([0.9777, 0.7577])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9927])\n",
      "tensor([0.9026, 0.8102])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9937])\n",
      "tensor([0.9705, 0.7978])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9932])\n",
      "tensor([0.9385, 0.7967])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9939])\n",
      "tensor([0.7454, 0.8363])\n",
      "0\n",
      "1\n",
      "tensor([0.9956, 0.9925])\n",
      "tensor([0.8950, 0.7329])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9926])\n",
      "tensor([0.9066, 0.7767])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9936])\n",
      "tensor([0.8435, 0.6541])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9933])\n",
      "tensor([0.8625, 0.6812])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9943])\n",
      "tensor([0.0083, 0.8851])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9926])\n",
      "tensor([0.8918, 0.8335])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9930])\n",
      "tensor([0.0531, 0.8980])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9946])\n",
      "tensor([0.2761, 0.8991])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9924])\n",
      "tensor([0.9140, 0.8003])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9943])\n",
      "tensor([0.9369, 0.9039])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9931])\n",
      "tensor([0.9127, 0.7433])\n",
      "0\n",
      "1\n",
      "tensor([0.9962, 0.9936])\n",
      "tensor([0.8193, 0.7728])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9955])\n",
      "tensor([0.7953, 0.7187])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9941])\n",
      "tensor([0.7996, 0.9330])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9944])\n",
      "tensor([0.9712, 0.9671])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9933])\n",
      "tensor([0.9810, 0.9665])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9933])\n",
      "tensor([0.9697, 0.9695])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9952])\n",
      "tensor([0.9878, 0.9620])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9949])\n",
      "tensor([0.9788, 0.9630])\n",
      "0\n",
      "1\n",
      "tensor([0.9961, 0.9940])\n",
      "tensor([0.9372, 0.7933])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9931])\n",
      "tensor([0.9658, 0.7782])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9933])\n",
      "tensor([0.8603, 0.3254])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9949])\n",
      "tensor([0.7826, 0.7365])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9938])\n",
      "tensor([0.8362, 0.5996])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9936])\n",
      "tensor([0.8463, 0.8105])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9937])\n",
      "tensor([0.7716, 0.8238])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9932])\n",
      "tensor([0.8509, 0.6689])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9947])\n",
      "tensor([0.8508, 0.7685])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9933])\n",
      "tensor([0.8632, 0.7861])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9939])\n",
      "tensor([0.7368, 0.7746])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9931])\n",
      "tensor([0.7795, 0.7467])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9932])\n",
      "tensor([0.5984, 0.6743])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9950])\n",
      "tensor([0.6168, 0.7313])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9971, 0.9939])\n",
      "tensor([0.7611, 0.7381])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9938])\n",
      "tensor([0.7813, 0.9386])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9946])\n",
      "tensor([0.9793, 0.8059])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9924])\n",
      "tensor([0.9002, 0.8040])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9948])\n",
      "tensor([0.9218, 0.8176])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9937])\n",
      "tensor([0.9088, 0.7933])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9932])\n",
      "tensor([-1.4397,  0.7937])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9918])\n",
      "tensor([-3.1634,  0.6646])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9926])\n",
      "tensor([-1.1267,  0.6964])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9946])\n",
      "tensor([0.0435, 0.9159])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9940])\n",
      "tensor([0.4188, 0.4363])\n",
      "0\n",
      "1\n",
      "tensor([0.9964, 0.9926])\n",
      "tensor([0.8723, 0.9494])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9919])\n",
      "tensor([0.8787, 0.9428])\n",
      "0\n",
      "1\n",
      "tensor([0.9959, 0.9913])\n",
      "tensor([0.8917, 0.9526])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9941])\n",
      "tensor([0.7499, 0.9479])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9940])\n",
      "tensor([0.9450, 0.9374])\n",
      "0\n",
      "1\n",
      "tensor([0.9960, 0.9937])\n",
      "tensor([-0.1124,  0.8722])\n",
      "0\n",
      "1\n",
      "tensor([0.9963, 0.9904])\n",
      "tensor([-0.0388,  0.8790])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9928])\n",
      "tensor([0.4472, 0.9141])\n",
      "0\n",
      "1\n",
      "tensor([0.9965, 0.9944])\n",
      "tensor([-0.3449,  0.8748])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9936])\n",
      "tensor([0.4022, 0.8797])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9946])\n",
      "tensor([0.9131, 0.5912])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9946])\n",
      "tensor([0.7941, 0.7385])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9950])\n",
      "tensor([0.7771, 0.6657])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9938])\n",
      "tensor([0.8287, 0.8541])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9936])\n",
      "tensor([0.6444, 0.6654])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9940])\n",
      "tensor([-2.6234,  0.8324])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9942])\n",
      "tensor([-2.2551,  0.8773])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9947])\n",
      "tensor([-1.3286,  0.8938])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9944])\n",
      "tensor([-1.1868,  0.6293])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9930])\n",
      "tensor([-2.1443,  0.9089])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9938])\n",
      "tensor([0.6548, 0.7808])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9946])\n",
      "tensor([0.6621, 0.7030])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9945])\n",
      "tensor([0.6294, 0.7396])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9936])\n",
      "tensor([0.6262, 0.8265])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9929])\n",
      "tensor([0.8710, 0.8036])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9946])\n",
      "tensor([-4.0092,  0.2651])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9938])\n",
      "tensor([-4.3928,  0.6671])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9943])\n",
      "tensor([-3.9649,  0.2165])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9939])\n",
      "tensor([-4.8205,  0.7909])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9932])\n",
      "tensor([-4.4823,  0.6663])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9950])\n",
      "tensor([0.9748, 0.7936])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9946])\n",
      "tensor([0.9667, 0.9550])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9947])\n",
      "tensor([0.4943, 0.9317])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9948])\n",
      "tensor([0.6312, 0.9586])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9946])\n",
      "tensor([0.5543, 0.9517])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9948])\n",
      "tensor([0.8418, 0.9736])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9942])\n",
      "tensor([0.9770, 0.9537])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9943])\n",
      "tensor([0.7878, 0.8911])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9943])\n",
      "tensor([0.7858, 0.9638])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9936])\n",
      "tensor([0.9383, 0.9667])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9955])\n",
      "tensor([0.8593, 0.9297])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9940])\n",
      "tensor([0.9655, 0.9376])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9940])\n",
      "tensor([0.7709, 0.9545])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9941])\n",
      "tensor([0.9644, 0.9329])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9934])\n",
      "tensor([0.9709, 0.9492])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9947])\n",
      "tensor([0.9733, 0.7734])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9932])\n",
      "tensor([0.9505, 0.7703])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9936])\n",
      "tensor([0.9108, 0.7914])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9949])\n",
      "tensor([0.9261, 0.7797])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9935])\n",
      "tensor([0.9170, 0.8060])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9948])\n",
      "tensor([0.9341, 0.7889])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9931])\n",
      "tensor([0.9183, 0.7278])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9938])\n",
      "tensor([0.4557, 0.7762])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9937])\n",
      "tensor([0.7867, 0.6974])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9934])\n",
      "tensor([0.4483, 0.6650])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9949])\n",
      "tensor([0.9743, 0.8314])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9935])\n",
      "tensor([0.9145, 0.8122])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9939])\n",
      "tensor([0.8313, 0.9035])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9951])\n",
      "tensor([0.3071, 0.7827])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9937])\n",
      "tensor([0.9081, 0.9095])\n",
      "0\n",
      "1\n",
      "tensor([0.9966, 0.9959])\n",
      "tensor([0.9504, 0.8369])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9944])\n",
      "tensor([0.8583, 0.8933])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9945])\n",
      "tensor([0.9513, 0.6480])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9947])\n",
      "tensor([0.7766, 0.6190])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9947])\n",
      "tensor([0.8111, 0.9125])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9956])\n",
      "tensor([0.9826, 0.9773])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9941])\n",
      "tensor([0.9893, 0.9672])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9946])\n",
      "tensor([0.9855, 0.9686])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9951])\n",
      "tensor([0.9825, 0.9687])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9949])\n",
      "tensor([0.9761, 0.9661])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9949])\n",
      "tensor([0.9206, 0.8086])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9940])\n",
      "tensor([0.8004, 0.7856])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9947])\n",
      "tensor([0.8396, 0.2980])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9952])\n",
      "tensor([0.8177, 0.4554])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9938])\n",
      "tensor([0.8434, 0.8398])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9945])\n",
      "tensor([0.8424, 0.8279])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9943])\n",
      "tensor([0.7494, 0.8226])\n",
      "0\n",
      "1\n",
      "tensor([0.9967, 0.9938])\n",
      "tensor([0.8087, 0.7881])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9949])\n",
      "tensor([0.8069, 0.7447])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9939])\n",
      "tensor([0.6115, 0.7660])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9948])\n",
      "tensor([0.7401, 0.7489])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9947])\n",
      "tensor([0.7809, 0.7437])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9949])\n",
      "tensor([0.6997, 0.7062])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9951])\n",
      "tensor([0.6068, 0.7472])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9938])\n",
      "tensor([0.7554, 0.7358])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9951])\n",
      "tensor([0.7813, 0.8061])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9951])\n",
      "tensor([0.9801, 0.8046])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9933])\n",
      "tensor([0.9024, 0.7996])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9953])\n",
      "tensor([0.9150, 0.8320])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9944])\n",
      "tensor([0.9744, 0.8012])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9940])\n",
      "tensor([-1.2723,  0.7260])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9940])\n",
      "tensor([-0.8580,  0.9202])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9943])\n",
      "tensor([-1.2015,  0.6640])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9939])\n",
      "tensor([-0.3857,  0.8935])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9942])\n",
      "tensor([0.4078, 0.6252])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9940])\n",
      "tensor([0.8415, 0.9267])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9939])\n",
      "tensor([0.5461, 0.9498])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9930])\n",
      "tensor([0.9092, 0.9398])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9940])\n",
      "tensor([0.5741, 0.9502])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9946])\n",
      "tensor([0.4046, 0.9395])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9943])\n",
      "tensor([0.3915, 0.8952])\n",
      "0\n",
      "1\n",
      "tensor([0.9969, 0.9930])\n",
      "tensor([0.4110, 0.8819])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9943])\n",
      "tensor([-0.0374,  0.9155])\n",
      "0\n",
      "1\n",
      "tensor([0.9968, 0.9942])\n",
      "tensor([-0.5801,  0.9072])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9942])\n",
      "tensor([0.5024, 0.9095])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9948])\n",
      "tensor([0.8659, 0.6326])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9949])\n",
      "tensor([0.7697, 0.6821])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9955])\n",
      "tensor([0.7762, 0.7383])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9942])\n",
      "tensor([0.8440, 0.8548])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9943])\n",
      "tensor([0.6567, 0.6647])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9944])\n",
      "tensor([-2.5209,  0.9424])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9946])\n",
      "tensor([-2.0646,  0.8938])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9951])\n",
      "tensor([-1.4980,  0.9024])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9945])\n",
      "tensor([-2.5774,  0.8073])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9945])\n",
      "tensor([-1.8501,  0.8916])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9943])\n",
      "tensor([0.6531, 0.7396])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9948])\n",
      "tensor([0.6512, 0.7381])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9954])\n",
      "tensor([0.6118, 0.7362])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9941])\n",
      "tensor([0.6439, 0.8243])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9943])\n",
      "tensor([0.6444, 0.7352])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9947])\n",
      "tensor([-5.5030,  0.5123])\n",
      "0\n",
      "1\n",
      "tensor([0.9980, 0.9945])\n",
      "tensor([-4.5919,  0.7006])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9949])\n",
      "tensor([-4.1141,  0.3910])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9945])\n",
      "tensor([-3.7532,  0.8697])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9947])\n",
      "tensor([-3.7499,  0.6064])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9955])\n",
      "tensor([0.5387, 0.8379])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9949])\n",
      "tensor([0.4830, 0.9588])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9956])\n",
      "tensor([0.5260, 0.8727])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9955])\n",
      "tensor([0.6378, 0.9551])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9951])\n",
      "tensor([0.4860, 0.9480])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9960])\n",
      "tensor([0.9823, 0.9735])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9946])\n",
      "tensor([0.9762, 0.9113])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9948])\n",
      "tensor([0.8065, 0.9009])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9948])\n",
      "tensor([0.7837, 0.9633])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9940])\n",
      "tensor([0.9422, 0.9473])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9954])\n",
      "tensor([0.8127, 0.8951])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9943])\n",
      "tensor([0.8410, 0.9455])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9949])\n",
      "tensor([0.7950, 0.9285])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9953])\n",
      "tensor([0.9730, 0.9491])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9945])\n",
      "tensor([0.9740, 0.9360])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9972, 0.9957])\n",
      "tensor([0.8927, 0.8008])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9934])\n",
      "tensor([0.9765, 0.7748])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9945])\n",
      "tensor([0.8994, 0.7269])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9952])\n",
      "tensor([0.9340, 0.7774])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9948])\n",
      "tensor([0.9257, 0.8116])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9956])\n",
      "tensor([0.9446, 0.7123])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9939])\n",
      "tensor([0.8969, 0.7435])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9943])\n",
      "tensor([0.9340, 0.7679])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9950])\n",
      "tensor([0.7204, 0.7302])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9940])\n",
      "tensor([0.6166, 0.7285])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9955])\n",
      "tensor([0.9067, 0.8923])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9937])\n",
      "tensor([0.9659, 0.8542])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9942])\n",
      "tensor([0.8718, 0.9281])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9955])\n",
      "tensor([0.4563, 0.8094])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9944])\n",
      "tensor([0.9115, 0.9020])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9960])\n",
      "tensor([0.9498, 0.7266])\n",
      "0\n",
      "1\n",
      "tensor([0.9972, 0.9942])\n",
      "tensor([0.7828, 0.9264])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9951])\n",
      "tensor([0.9530, 0.6357])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9954])\n",
      "tensor([0.8167, 0.6331])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9947])\n",
      "tensor([0.8173, 0.9331])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9956])\n",
      "tensor([0.9732, 0.9798])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9947])\n",
      "tensor([0.9875, 0.9712])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9952])\n",
      "tensor([0.9882, 0.9670])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9958])\n",
      "tensor([0.9814, 0.9691])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9955])\n",
      "tensor([0.9876, 0.9645])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9947])\n",
      "tensor([0.9073, 0.6106])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9945])\n",
      "tensor([0.9630, 0.8086])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9947])\n",
      "tensor([0.9536, 0.4760])\n",
      "0\n",
      "1\n",
      "tensor([0.9978, 0.9951])\n",
      "tensor([0.9686, 0.8547])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9946])\n",
      "tensor([0.9687, 0.8337])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9951])\n",
      "tensor([0.8079, 0.7850])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9945])\n",
      "tensor([0.7788, 0.8245])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9945])\n",
      "tensor([0.8089, 0.8024])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9955])\n",
      "tensor([0.7814, 0.8346])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9942])\n",
      "tensor([0.7655, 0.7395])\n",
      "0\n",
      "1\n",
      "tensor([0.9970, 0.9951])\n",
      "tensor([0.7329, 0.7778])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9948])\n",
      "tensor([0.7640, 0.7445])\n",
      "0\n",
      "1\n",
      "tensor([0.9979, 0.9953])\n",
      "tensor([0.7084, 0.7166])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9951])\n",
      "tensor([0.6824, 0.7427])\n",
      "0\n",
      "1\n",
      "tensor([0.9980, 0.9944])\n",
      "tensor([0.5953, 0.7526])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9954])\n",
      "tensor([0.9445, 0.8093])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9959])\n",
      "tensor([0.7400, 0.8033])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9946])\n",
      "tensor([0.9163, 0.7897])\n",
      "0\n",
      "1\n",
      "tensor([0.9974, 0.9958])\n",
      "tensor([0.9180, 0.8309])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9955])\n",
      "tensor([0.9730, 0.8141])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9945])\n",
      "tensor([0.8395, 0.6973])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9953])\n",
      "tensor([-2.1768,  0.9333])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9950])\n",
      "tensor([-0.5522,  0.6828])\n",
      "0\n",
      "1\n",
      "tensor([0.9971, 0.9949])\n",
      "tensor([0.0240, 0.9214])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9951])\n",
      "tensor([0.0683, 0.6715])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9941])\n",
      "tensor([0.2264, 0.9433])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9946])\n",
      "tensor([0.7718, 0.9438])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9950])\n",
      "tensor([0.9057, 0.9009])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9944])\n",
      "tensor([0.3804, 0.9572])\n",
      "0\n",
      "1\n",
      "tensor([0.9976, 0.9953])\n",
      "tensor([0.4980, 0.9437])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9940])\n",
      "tensor([0.4360, 0.9210])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9947])\n",
      "tensor([0.4334, 0.8942])\n",
      "0\n",
      "1\n",
      "tensor([0.9975, 0.9948])\n",
      "tensor([0.4466, 0.9049])\n",
      "0\n",
      "1\n",
      "tensor([0.9973, 0.9954])\n",
      "tensor([-0.1288,  0.8842])\n",
      "0\n",
      "1\n",
      "tensor([0.9977, 0.9946])\n",
      "tensor([0.4740, 0.9220])\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9759, 0.9832])\n",
      "tensor([0.8385, 0.7039])\n",
      "0\n",
      "1\n",
      "tensor([0.9785, 0.9831])\n",
      "tensor([0.8915, 0.5519])\n",
      "0\n",
      "1\n",
      "tensor([0.9665, 0.9857])\n",
      "tensor([0.8184, 0.7476])\n",
      "0\n",
      "1\n",
      "tensor([0.9725, 0.9892])\n",
      "tensor([0.8662, 0.8778])\n",
      "0\n",
      "1\n",
      "tensor([0.9714, 0.9822])\n",
      "tensor([0.8884, 0.5758])\n",
      "0\n",
      "1\n",
      "tensor([0.9805, 0.9804])\n",
      "tensor([-2.2210,  0.9243])\n",
      "0\n",
      "1\n",
      "tensor([0.9786, 0.9861])\n",
      "tensor([-2.0054,  0.5207])\n",
      "0\n",
      "1\n",
      "tensor([0.9750, 0.9822])\n",
      "tensor([-1.2008,  0.7962])\n",
      "0\n",
      "1\n",
      "tensor([0.9741, 0.9887])\n",
      "tensor([-2.7071,  0.8484])\n",
      "0\n",
      "1\n",
      "tensor([0.9701, 0.9850])\n",
      "tensor([-2.2006,  0.9052])\n",
      "0\n",
      "1\n",
      "tensor([0.9791, 0.9888])\n",
      "tensor([0.6291, 0.6886])\n",
      "0\n",
      "1\n",
      "tensor([0.9733, 0.9887])\n",
      "tensor([0.6368, 0.7100])\n",
      "0\n",
      "1\n",
      "tensor([0.9692, 0.9875])\n",
      "tensor([0.5783, 0.7144])\n",
      "0\n",
      "1\n",
      "tensor([0.9744, 0.9908])\n",
      "tensor([0.6110, 0.8405])\n",
      "0\n",
      "1\n",
      "tensor([0.9741, 0.9818])\n",
      "tensor([0.6319, 0.7162])\n",
      "0\n",
      "1\n",
      "tensor([0.9811, 0.9772])\n",
      "tensor([-3.6991,  0.4892])\n",
      "0\n",
      "1\n",
      "tensor([0.9804, 0.9877])\n",
      "tensor([-5.0789,  0.3311])\n",
      "0\n",
      "1\n",
      "tensor([0.9753, 0.9816])\n",
      "tensor([-4.9469,  0.3708])\n",
      "0\n",
      "1\n",
      "tensor([0.9700, 0.9900])\n",
      "tensor([-4.1104,  0.9139])\n",
      "0\n",
      "1\n",
      "tensor([0.9705, 0.9810])\n",
      "tensor([-4.2508,  0.5896])\n",
      "0\n",
      "1\n",
      "tensor([0.9725, 0.9784])\n",
      "tensor([0.9357, 0.8107])\n",
      "0\n",
      "1\n",
      "tensor([0.9684, 0.9841])\n",
      "tensor([0.2459, 0.9514])\n",
      "0\n",
      "1\n",
      "tensor([0.9760, 0.9835])\n",
      "tensor([0.5579, 0.8115])\n",
      "0\n",
      "1\n",
      "tensor([0.9741, 0.9883])\n",
      "tensor([0.5555, 0.9525])\n",
      "0\n",
      "1\n",
      "tensor([0.9801, 0.9778])\n",
      "tensor([0.5543, 0.9112])\n",
      "0\n",
      "1\n",
      "tensor([0.9767, 0.9848])\n",
      "tensor([0.9665, 0.9707])\n",
      "0\n",
      "1\n",
      "tensor([0.9742, 0.9878])\n",
      "tensor([0.9312, 0.9433])\n",
      "0\n",
      "1\n",
      "tensor([0.9793, 0.9866])\n",
      "tensor([0.8240, 0.9056])\n",
      "0\n",
      "1\n",
      "tensor([0.9750, 0.9882])\n",
      "tensor([0.8092, 0.9755])\n",
      "0\n",
      "1\n",
      "tensor([0.9798, 0.9866])\n",
      "tensor([0.9497, 0.9065])\n",
      "0\n",
      "1\n",
      "tensor([0.9759, 0.9838])\n",
      "tensor([0.7877, 0.9357])\n",
      "0\n",
      "1\n",
      "tensor([0.9648, 0.9865])\n",
      "tensor([0.8067, 0.9267])\n",
      "0\n",
      "1\n",
      "tensor([0.9778, 0.9874])\n",
      "tensor([0.8217, 0.9153])\n",
      "0\n",
      "1\n",
      "tensor([0.9741, 0.9883])\n",
      "tensor([0.9599, 0.8584])\n",
      "0\n",
      "1\n",
      "tensor([0.9793, 0.9864])\n",
      "tensor([0.9660, 0.9030])\n",
      "0\n",
      "1\n",
      "tensor([0.9745, 0.9846])\n",
      "tensor([0.9653, 0.7228])\n",
      "0\n",
      "1\n",
      "tensor([0.9642, 0.9857])\n",
      "tensor([0.9385, 0.7023])\n",
      "0\n",
      "1\n",
      "tensor([0.9766, 0.9829])\n",
      "tensor([0.9597, 0.6796])\n",
      "0\n",
      "1\n",
      "tensor([0.9746, 0.9854])\n",
      "tensor([0.9179, 0.6756])\n",
      "0\n",
      "1\n",
      "tensor([0.9726, 0.9842])\n",
      "tensor([0.9667, 0.8046])\n",
      "0\n",
      "1\n",
      "tensor([0.9757, 0.9844])\n",
      "tensor([0.9347, 0.7087])\n",
      "0\n",
      "1\n",
      "tensor([0.9721, 0.9862])\n",
      "tensor([0.9208, 0.6646])\n",
      "0\n",
      "1\n",
      "tensor([0.9760, 0.9816])\n",
      "tensor([0.7895, 0.7919])\n",
      "0\n",
      "1\n",
      "tensor([0.9735, 0.9862])\n",
      "tensor([0.6980, 0.7510])\n",
      "0\n",
      "1\n",
      "tensor([0.9796, 0.9890])\n",
      "tensor([0.7069, 0.7171])\n",
      "0\n",
      "1\n",
      "tensor([0.9689, 0.9841])\n",
      "tensor([0.9047, 0.9575])\n",
      "0\n",
      "1\n",
      "tensor([0.9773, 0.9872])\n",
      "tensor([0.4667, 0.8350])\n",
      "0\n",
      "1\n",
      "tensor([0.9786, 0.9833])\n",
      "tensor([0.8813, 0.9169])\n",
      "0\n",
      "1\n",
      "tensor([0.9796, 0.9866])\n",
      "tensor([0.4234, 0.8335])\n",
      "0\n",
      "1\n",
      "tensor([0.9797, 0.9847])\n",
      "tensor([0.6942, 0.9250])\n",
      "0\n",
      "1\n",
      "tensor([0.9626, 0.9796])\n",
      "tensor([0.9205, 0.6753])\n",
      "0\n",
      "1\n",
      "tensor([0.9760, 0.9853])\n",
      "tensor([0.8458, 0.8661])\n",
      "0\n",
      "1\n",
      "tensor([0.9826, 0.9769])\n",
      "tensor([0.9386, 0.6947])\n",
      "0\n",
      "1\n",
      "tensor([0.9792, 0.9823])\n",
      "tensor([0.7888, 0.6348])\n",
      "0\n",
      "1\n",
      "tensor([0.9785, 0.9867])\n",
      "tensor([0.9205, 0.9197])\n",
      "0\n",
      "1\n",
      "tensor([0.9748, 0.9774])\n",
      "tensor([0.9781, 0.9660])\n",
      "0\n",
      "1\n",
      "tensor([0.9700, 0.9835])\n",
      "tensor([0.9707, 0.9567])\n",
      "0\n",
      "1\n",
      "tensor([0.9815, 0.9744])\n",
      "tensor([0.9703, 0.9508])\n",
      "0\n",
      "1\n",
      "tensor([0.9774, 0.9846])\n",
      "tensor([0.9753, 0.9551])\n",
      "0\n",
      "1\n",
      "tensor([0.9793, 0.9856])\n",
      "tensor([0.9660, 0.9651])\n",
      "0\n",
      "1\n",
      "tensor([0.9675, 0.9836])\n",
      "tensor([0.9153, 0.5667])\n",
      "0\n",
      "1\n",
      "tensor([0.9738, 0.9817])\n",
      "tensor([0.7967, 0.4357])\n",
      "0\n",
      "1\n",
      "tensor([0.9777, 0.9789])\n",
      "tensor([0.9430, 0.4649])\n",
      "0\n",
      "1\n",
      "tensor([0.9762, 0.9813])\n",
      "tensor([0.9065, 0.4078])\n",
      "0\n",
      "1\n",
      "tensor([0.9799, 0.9807])\n",
      "tensor([0.8427, 0.6858])\n",
      "0\n",
      "1\n",
      "tensor([0.9747, 0.9854])\n",
      "tensor([0.7578, 0.7465])\n",
      "0\n",
      "1\n",
      "tensor([0.9721, 0.9843])\n",
      "tensor([0.7686, 0.7843])\n",
      "0\n",
      "1\n",
      "tensor([0.9779, 0.9795])\n",
      "tensor([0.9490, 0.9114])\n",
      "0\n",
      "1\n",
      "tensor([0.9768, 0.9860])\n",
      "tensor([0.7796, 0.7756])\n",
      "0\n",
      "1\n",
      "tensor([0.9790, 0.9864])\n",
      "tensor([0.7432, 0.7464])\n",
      "0\n",
      "1\n",
      "tensor([0.9724, 0.9870])\n",
      "tensor([0.7075, 0.7414])\n",
      "0\n",
      "1\n",
      "tensor([0.9723, 0.9866])\n",
      "tensor([0.7241, 0.7273])\n",
      "0\n",
      "1\n",
      "tensor([0.9832, 0.9750])\n",
      "tensor([0.7016, 0.6882])\n",
      "0\n",
      "1\n",
      "tensor([0.9755, 0.9839])\n",
      "tensor([0.7056, 0.7285])\n",
      "0\n",
      "1\n",
      "tensor([0.9829, 0.9878])\n",
      "tensor([0.7096, 0.7421])\n",
      "0\n",
      "1\n",
      "tensor([0.9737, 0.9843])\n",
      "tensor([0.9380, 0.7980])\n",
      "0\n",
      "1\n",
      "tensor([0.9588, 0.9848])\n",
      "tensor([0.7123, 0.7953])\n",
      "0\n",
      "1\n",
      "tensor([0.9825, 0.9656])\n",
      "tensor([0.8637, 0.7712])\n",
      "0\n",
      "1\n",
      "tensor([0.9735, 0.9897])\n",
      "tensor([0.7067, 0.8090])\n",
      "0\n",
      "1\n",
      "tensor([0.9834, 0.9879])\n",
      "tensor([0.8640, 0.8139])\n",
      "0\n",
      "1\n",
      "tensor([0.9703, 0.9743])\n",
      "tensor([-0.4505,  0.6582])\n",
      "0\n",
      "1\n",
      "tensor([0.9712, 0.9818])\n",
      "tensor([-1.1352,  0.9045])\n",
      "0\n",
      "1\n",
      "tensor([0.9802, 0.9748])\n",
      "tensor([-0.2581,  0.5984])\n",
      "0\n",
      "1\n",
      "tensor([0.9719, 0.9849])\n",
      "tensor([0.0568, 0.7195])\n",
      "0\n",
      "1\n",
      "tensor([0.9813, 0.9791])\n",
      "tensor([-0.1849,  0.8609])\n",
      "0\n",
      "1\n",
      "tensor([0.9756, 0.9814])\n",
      "tensor([0.8899, 0.9238])\n",
      "0\n",
      "1\n",
      "tensor([0.9673, 0.9851])\n",
      "tensor([0.5273, 0.9426])\n",
      "0\n",
      "1\n",
      "tensor([0.9781, 0.9790])\n",
      "tensor([0.0916, 0.8617])\n",
      "0\n",
      "1\n",
      "tensor([0.9730, 0.9883])\n",
      "tensor([0.2852, 0.9412])\n",
      "0\n",
      "1\n",
      "tensor([0.9805, 0.9828])\n",
      "tensor([0.4302, 0.9375])\n",
      "0\n",
      "1\n",
      "tensor([0.9748, 0.9868])\n",
      "tensor([0.4913, 0.9384])\n",
      "0\n",
      "1\n",
      "tensor([0.9806, 0.9820])\n",
      "tensor([0.0517, 0.8872])\n",
      "0\n",
      "1\n",
      "tensor([0.9850, 0.9775])\n",
      "tensor([0.4983, 0.8975])\n",
      "0\n",
      "1\n",
      "tensor([0.9772, 0.9886])\n",
      "tensor([-0.4332,  0.9145])\n",
      "0\n",
      "1\n",
      "tensor([0.9771, 0.9866])\n",
      "tensor([-0.0951,  0.9158])\n",
      "0\n",
      "1\n",
      "tensor([0.9818, 0.9860])\n",
      "tensor([0.8933, 0.7802])\n",
      "0\n",
      "1\n",
      "tensor([0.9758, 0.9818])\n",
      "tensor([0.9004, 0.5460])\n",
      "0\n",
      "1\n",
      "tensor([0.9743, 0.9842])\n",
      "tensor([0.7368, 0.6854])\n",
      "0\n",
      "1\n",
      "tensor([0.9735, 0.9890])\n",
      "tensor([0.7857, 0.8426])\n",
      "0\n",
      "1\n",
      "tensor([0.9763, 0.9849])\n",
      "tensor([0.6945, 0.2557])\n",
      "0\n",
      "1\n",
      "tensor([0.9782, 0.9870])\n",
      "tensor([-2.1531,  0.9206])\n",
      "0\n",
      "1\n",
      "tensor([0.9781, 0.9830])\n",
      "tensor([-1.7925,  0.4892])\n",
      "0\n",
      "1\n",
      "tensor([0.9776, 0.9838])\n",
      "tensor([-1.1798,  0.7916])\n",
      "0\n",
      "1\n",
      "tensor([0.9769, 0.9898])\n",
      "tensor([-2.4705,  0.7740])\n",
      "0\n",
      "1\n",
      "tensor([0.9787, 0.9882])\n",
      "tensor([-1.9396,  0.9205])\n",
      "0\n",
      "1\n",
      "tensor([0.9762, 0.9848])\n",
      "tensor([0.5869, 0.8000])\n",
      "0\n",
      "1\n",
      "tensor([0.9713, 0.9875])\n",
      "tensor([0.5992, 0.7138])\n",
      "0\n",
      "1\n",
      "tensor([0.9751, 0.9880])\n",
      "tensor([0.5930, 0.7078])\n",
      "0\n",
      "1\n",
      "tensor([0.9759, 0.9923])\n",
      "tensor([0.5697, 0.8399])\n",
      "0\n",
      "1\n",
      "tensor([0.9707, 0.9854])\n",
      "tensor([0.6297, 0.7162])\n",
      "0\n",
      "1\n",
      "tensor([0.9807, 0.9775])\n",
      "tensor([-3.1755,  0.5739])\n",
      "0\n",
      "1\n",
      "tensor([0.9740, 0.9872])\n",
      "tensor([-5.9896,  0.7065])\n",
      "0\n",
      "1\n",
      "tensor([0.9775, 0.9811])\n",
      "tensor([-4.3009,  0.6066])\n",
      "0\n",
      "1\n",
      "tensor([0.9765, 0.9899])\n",
      "tensor([-3.8681,  0.8459])\n",
      "0\n",
      "1\n",
      "tensor([0.9719, 0.9861])\n",
      "tensor([-4.1451,  0.5905])\n",
      "0\n",
      "1\n",
      "tensor([0.9729, 0.9806])\n",
      "tensor([0.5408, 0.8071])\n",
      "0\n",
      "1\n",
      "tensor([0.9739, 0.9850])\n",
      "tensor([0.5113, 0.9400])\n",
      "0\n",
      "1\n",
      "tensor([0.9762, 0.9816])\n",
      "tensor([0.4399, 0.8491])\n",
      "0\n",
      "1\n",
      "tensor([0.9742, 0.9890])\n",
      "tensor([0.5405, 0.9547])\n",
      "0\n",
      "1\n",
      "tensor([0.9774, 0.9851])\n",
      "tensor([0.5122, 0.9373])\n",
      "0\n",
      "1\n",
      "tensor([0.9716, 0.9831])\n",
      "tensor([0.9650, 0.9372])\n",
      "0\n",
      "1\n",
      "tensor([0.9775, 0.9832])\n",
      "tensor([0.6718, 0.9687])\n",
      "0\n",
      "1\n",
      "tensor([0.9801, 0.9893])\n",
      "tensor([0.8202, 0.9133])\n",
      "0\n",
      "1\n",
      "tensor([0.9731, 0.9873])\n",
      "tensor([0.7937, 0.9747])\n",
      "0\n",
      "1\n",
      "tensor([0.9789, 0.9866])\n",
      "tensor([0.9378, 0.9234])\n",
      "0\n",
      "1\n",
      "tensor([0.9738, 0.9837])\n",
      "tensor([0.7743, 0.9048])\n",
      "0\n",
      "1\n",
      "tensor([0.9712, 0.9858])\n",
      "tensor([0.8773, 0.9002])\n",
      "0\n",
      "1\n",
      "tensor([0.9715, 0.9867])\n",
      "tensor([0.9562, 0.8657])\n",
      "0\n",
      "1\n",
      "tensor([0.9739, 0.9861])\n",
      "tensor([0.9596, 0.8432])\n",
      "0\n",
      "1\n",
      "tensor([0.9792, 0.9862])\n",
      "tensor([0.9664, 0.9155])\n",
      "0\n",
      "1\n",
      "tensor([0.9747, 0.9837])\n",
      "tensor([0.9678, 0.6596])\n",
      "0\n",
      "1\n",
      "tensor([0.9699, 0.9865])\n",
      "tensor([0.9457, 0.8272])\n",
      "0\n",
      "1\n",
      "tensor([0.9741, 0.9868])\n",
      "tensor([0.9562, 0.6490])\n",
      "0\n",
      "1\n",
      "tensor([0.9781, 0.9800])\n",
      "tensor([0.8824, 0.6668])\n",
      "0\n",
      "1\n",
      "tensor([0.9812, 0.9872])\n",
      "tensor([0.9453, 0.7963])\n",
      "0\n",
      "1\n",
      "tensor([0.9734, 0.9867])\n",
      "tensor([0.8986, 0.6959])\n",
      "0\n",
      "1\n",
      "tensor([0.9698, 0.9859])\n",
      "tensor([0.9299, 0.7498])\n",
      "0\n",
      "1\n",
      "tensor([0.9784, 0.9860])\n",
      "tensor([0.4576, 0.7351])\n",
      "0\n",
      "1\n",
      "tensor([0.9739, 0.9870])\n",
      "tensor([0.7494, 0.7186])\n",
      "0\n",
      "1\n",
      "tensor([0.9764, 0.9895])\n",
      "tensor([0.9405, 0.6980])\n",
      "0\n",
      "1\n",
      "tensor([0.9785, 0.9893])\n",
      "tensor([0.6672, 0.9622])\n",
      "0\n",
      "1\n",
      "tensor([0.9603, 0.9861])\n",
      "tensor([0.4910, 0.8760])\n",
      "0\n",
      "1\n",
      "tensor([0.9784, 0.9818])\n",
      "tensor([0.8869, 0.5115])\n",
      "0\n",
      "1\n",
      "tensor([0.9756, 0.9859])\n",
      "tensor([0.4229, 0.7755])\n",
      "0\n",
      "1\n",
      "tensor([0.9811, 0.9887])\n",
      "tensor([0.8485, 0.9268])\n",
      "0\n",
      "1\n",
      "tensor([0.9622, 0.9807])\n",
      "tensor([0.8884, 0.7200])\n",
      "0\n",
      "1\n",
      "tensor([0.9733, 0.9812])\n",
      "tensor([0.8746, 0.7450])\n",
      "0\n",
      "1\n",
      "tensor([0.9763, 0.9860])\n",
      "tensor([0.9421, 0.5487])\n",
      "0\n",
      "1\n",
      "tensor([0.9754, 0.9858])\n",
      "tensor([0.7654, 0.1836])\n",
      "0\n",
      "1\n",
      "tensor([0.9806, 0.9862])\n",
      "tensor([0.9203, 0.8698])\n",
      "0\n",
      "1\n",
      "tensor([0.9751, 0.9798])\n",
      "tensor([0.9591, 0.9589])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0.9739, 0.9824])\n",
      "tensor([0.9756, 0.9362])\n",
      "0\n",
      "1\n",
      "tensor([0.9761, 0.9792])\n",
      "tensor([0.9726, 0.9632])\n",
      "0\n",
      "1\n",
      "tensor([0.9815, 0.9782])\n",
      "tensor([0.9697, 0.9470])\n",
      "0\n",
      "1\n",
      "tensor([0.9820, 0.9891])\n",
      "tensor([0.9728, 0.9654])\n",
      "0\n",
      "1\n",
      "tensor([0.9808, 0.9853])\n",
      "tensor([0.8553, 0.6245])\n",
      "0\n",
      "1\n",
      "tensor([0.9688, 0.9843])\n",
      "tensor([0.7634, 0.8209])\n",
      "0\n",
      "1\n",
      "tensor([0.9704, 0.9857])\n",
      "tensor([0.9154, 0.5701])\n",
      "0\n",
      "1\n",
      "tensor([0.9785, 0.9879])\n",
      "tensor([0.8669, 0.8560])\n",
      "0\n",
      "1\n",
      "tensor([0.9781, 0.9866])\n",
      "tensor([0.9463, 0.6722])\n",
      "0\n",
      "1\n",
      "tensor([0.9760, 0.9793])\n",
      "tensor([0.8455, 0.7442])\n",
      "0\n",
      "1\n",
      "tensor([0.9624, 0.9854])\n",
      "tensor([0.7329, 0.7572])\n",
      "0\n",
      "1\n",
      "tensor([0.9698, 0.9854])\n",
      "tensor([0.6942, 0.6771])\n",
      "0\n",
      "1\n",
      "tensor([0.9806, 0.9873])\n",
      "tensor([0.7683, 0.7464])\n",
      "0\n",
      "1\n",
      "tensor([0.9788, 0.9848])\n",
      "tensor([0.7549, 0.6277])\n",
      "0\n",
      "1\n",
      "tensor([0.9815, 0.9841])\n",
      "tensor([0.7149, 0.7485])\n",
      "0\n",
      "1\n",
      "tensor([0.9713, 0.9761])\n",
      "tensor([0.7428, 0.7258])\n",
      "0\n",
      "1\n",
      "tensor([0.9824, 0.9847])\n",
      "tensor([0.7162, 0.7047])\n",
      "0\n",
      "1\n",
      "tensor([0.9776, 0.9809])\n",
      "tensor([0.5825, 0.7246])\n",
      "0\n",
      "1\n",
      "tensor([0.9839, 0.9882])\n",
      "tensor([0.7133, 0.7488])\n",
      "0\n",
      "1\n",
      "tensor([0.9703, 0.9846])\n",
      "tensor([0.6916, 0.8007])\n",
      "0\n",
      "1\n",
      "tensor([0.9740, 0.9851])\n",
      "tensor([0.8906, 0.7880])\n",
      "0\n",
      "1\n",
      "tensor([0.9769, 0.9851])\n",
      "tensor([0.8612, 0.7811])\n",
      "0\n",
      "1\n",
      "tensor([0.9739, 0.9838])\n",
      "tensor([0.7195, 0.7736])\n",
      "0\n",
      "1\n",
      "tensor([0.9810, 0.9869])\n",
      "tensor([0.8929, 0.8252])\n",
      "0\n",
      "1\n",
      "tensor([0.9736, 0.9865])\n",
      "tensor([0.7805, 0.7277])\n",
      "0\n",
      "1\n",
      "tensor([0.9640, 0.9791])\n",
      "tensor([0.6876, 0.9128])\n",
      "0\n",
      "1\n",
      "tensor([0.9739, 0.9825])\n",
      "tensor([0.7702, 0.6204])\n",
      "0\n",
      "1\n",
      "tensor([0.9735, 0.9811])\n",
      "tensor([-0.3355,  0.6137])\n",
      "0\n",
      "1\n",
      "tensor([0.9797, 0.9877])\n",
      "tensor([-0.9613,  0.8440])\n",
      "0\n",
      "1\n",
      "tensor([0.9670, 0.9864])\n",
      "tensor([0.8428, 0.9226])\n",
      "0\n",
      "1\n",
      "tensor([0.9607, 0.9828])\n",
      "tensor([0.3925, 0.9390])\n",
      "0\n",
      "1\n",
      "tensor([0.9810, 0.9806])\n",
      "tensor([0.2710, 0.8912])\n",
      "0\n",
      "1\n",
      "tensor([0.9752, 0.9874])\n",
      "tensor([0.1036, 0.8828])\n",
      "0\n",
      "1\n",
      "tensor([0.9801, 0.9851])\n",
      "tensor([0.3955, 0.9475])\n",
      "0\n",
      "1\n",
      "tensor([0.9758, 0.9804])\n",
      "tensor([0.4796, 0.9292])\n",
      "0\n",
      "1\n",
      "tensor([0.9707, 0.9800])\n",
      "tensor([0.4172, 0.8833])\n",
      "0\n",
      "1\n",
      "tensor([0.9880, 0.9819])\n",
      "tensor([-0.1557,  0.9279])\n",
      "0\n",
      "1\n",
      "tensor([0.9815, 0.9834])\n",
      "tensor([-0.1543,  0.8850])\n",
      "0\n",
      "1\n",
      "tensor([0.9718, 0.9867])\n",
      "tensor([-0.0271,  0.8996])\n"
     ]
    }
   ],
   "source": [
    "reps = 5\n",
    "train_p = np.linspace(100,800,8)\n",
    "train_p=np.array([10,15,20,25,30,35,40,45,50])*18\n",
    "R2_test = torch.zeros(len(train_p),reps,len(meshes),2)\n",
    "R2_leftout= torch.zeros(len(train_p),reps,len(meshes),2)\n",
    "for k in range(len(train_p)):\n",
    "    for i in range(len(meshes)):\n",
    "        for j in range(reps):\n",
    "            X=torch.cat(train_input_modes[0:i]+train_input_modes[i+1:])[:,0:15]\n",
    "            y=torch.cat(train_output_modes[:i]+train_output_modes[i+1:])\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                train_size=int(train_p[k]),\n",
    "                random_state=j\n",
    "            )\n",
    "            X_test= torch.cat(test_input_modes[0:i]+test_input_modes[i+1:])[:,0:15]\n",
    "            y_test=torch.cat(test_output_modes[:i]+test_output_modes[i+1:])\n",
    "            emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "\n",
    "            meanR, stdR = meanR, stdR = emulator.R2_sample(X_test,y_test,1000)\n",
    "            R2_test[k,j,i,:]=meanR\n",
    "\n",
    "            meanR, stdR=emulator.R2_sample(test_input_modes[i][:,0:15],test_output_modes[i],1000) \n",
    "            R2_leftout[k,j,i,:] = meanR\n",
    "            print(R2_test[k,j,i,:])\n",
    "            print(R2_leftout[k,j,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7c76b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ad603b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1286,  0.9066])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a5e512b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.05882352941177"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "800/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3db995bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9806, 0.9739],\n",
       "        [0.9901, 0.9840],\n",
       "        [0.9939, 0.9886],\n",
       "        [0.9954, 0.9914],\n",
       "        [0.9963, 0.9931],\n",
       "        [0.9969, 0.9939],\n",
       "        [0.9973, 0.9946],\n",
       "        [0.9976, 0.9951]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_test.mean(axis=[1,2])[:8].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110fb965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.7030e-01, 7.7612e-01],\n",
       "        [4.1755e-01, 7.8686e-01],\n",
       "        [2.5943e-01, 8.0537e-01],\n",
       "        [1.2036e-04, 7.9295e-01],\n",
       "        [2.8950e-01, 7.7975e-01],\n",
       "        [3.2031e-01, 8.0222e-01],\n",
       "        [2.4608e-01, 8.0715e-01],\n",
       "        [3.0436e-01, 8.1715e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_leftout.mean(axis=[1,2])[:8].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3da43b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9806, 0.9739],\n",
       "        [0.9901, 0.9840],\n",
       "        [0.9939, 0.9886],\n",
       "        [0.9954, 0.9914],\n",
       "        [0.9963, 0.9931],\n",
       "        [0.9969, 0.9939],\n",
       "        [0.9973, 0.9946],\n",
       "        [0.9976, 0.9951]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_test.mean(axis=[1,2])[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "796ab181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(train_p[:7],R2_test.mean(axis=[1,2])[:7,0].detach().numpy(),fmt='o',yerr=R2_test.std(axis=[1,2])[:7,0].detach().numpy())\n",
    "plt.errorbar(train_p[:7],R2_test.mean(axis=[1,2])[:7,1].detach().numpy(),fmt='o',yerr=R2_test.std(axis=[1,2])[:7,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'),fontsize=fontS)\n",
    "plt.xlabel('Training Points',fontsize=fontS)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('figures/WeavingDTLatentNLeftin.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95738804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9828, 0.9750],\n",
       "        [0.9911, 0.9851],\n",
       "        [0.9943, 0.9896],\n",
       "        [0.9957, 0.9919],\n",
       "        [0.9965, 0.9934],\n",
       "        [0.9971, 0.9943],\n",
       "        [0.9975, 0.9949]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_test.mean(axis=[1,2])[:7,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "401de880",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(train_p[:7],R2_leftout.mean(axis=[1,2])[:7,0].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=[1,2])[:7,0].detach().numpy())\n",
    "plt.errorbar(train_p[:7],R2_leftout.mean(axis=[1,2])[:7,1].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=[1,2])[:7,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'),fontsize=fontS)\n",
    "plt.xlabel('Training Points',fontsize=fontS)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('figures/WeavingDTLatentNLeftout.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7eb0391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$R^2$')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "me=3\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(train_p[:7],R2_leftout[:,:,me].mean(axis=1)[:7,0].detach().numpy(),fmt='o',yerr=R2_leftout[:,:,me].std(axis=1)[:7,0].detach().numpy())\n",
    "plt.errorbar(train_p[:7],R2_leftout[:,:,me].mean(axis=1)[:7,1].detach().numpy(),fmt='o',yerr=R2_leftout[:,:,me].std(axis=1)[:7,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Training Points')\n",
    "plt.ylabel('$R^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58e4d7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([180, 270, 360, 450, 540, 630, 720, 810, 900])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5e36b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 19, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_leftout.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f64eae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8606, -2.0670,  0.6174, -4.4172,  0.5699,  0.8961,  0.8684,  0.9496,\n",
       "         0.8100,  0.6741,  0.8829,  0.9721,  0.8808,  0.7996,  0.7097,  0.8169,\n",
       "        -0.3944,  0.4448,  0.1026])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_leftout.mean(axis=1)[7,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f286261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAAHSCAYAAACD5AiJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABVG0lEQVR4nO3deXxU1f3/8fdkZ0sgbAkQZFHAyCJBqAgqCiiWRakrAlahrcV9+br3K+AGar8W5ddqqxaFAO4gVMWKgMqiQYEooCiSCJKEnYQlCSE5vz/ijAlZSCYz98xkXs/HYx5w79yZvBkyc+Zzz7nnuIwxRgAAAAAAx4XZDgAAAAAAoYqCDAAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAALCEggwAAAAALKEgAwAAAABLKMgAAAAAwJII2wHqi5KSEmVlZalJkyZyuVy24wAAAACwxBijQ4cOqU2bNgoLq74PjILMR7KyspSUlGQ7BgAAAIAAsWPHDrVr167aYyjIfKRJkyaSSl/02NhYy2kAAAAA2JKXl6ekpCRPjVAdCjIfcQ9TjI2NpSADAAAAUKNLmZjUAwAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAALCEggwAAAAALAnZgiwjI0Mvvvii/vjHP6pXr16KiIiQy+XSY489ZjsaAAAAgBARstPeP/vss3r22WdtxwAAAAAQwkK2h6xFixYaMWKEHnnkEX3wwQe6/PLLbUcCAAAAEGJCtofsL3/5S7nt1157zVISAAAAAKEqZAsyAAAQGIpLjNIy9mv3oQK1ahKjfh3jFR7msh0LABxBQQYAAKxZsjFbjy76RkmH09VKB7VbTbWjcS/976geGtY90XY8APA7CjIA9VdJsfTTaunwLqlxa+mUc6SwcNupAh69FXDKko3ZWjjvBb0ZOVttovZ79mcVxuuReddJ1/6ZogxAvUdB5qXCwkIVFhZ6tvPy8iymAVDB5kXSkvukvKxf98W2kYY9KSWPspcrwC3ZmK2pizcrO7fAsy8xLkaTRybzxRg+VVxitGLhv/WPyBkV7kvQfv0jcoYeXBilockPckIACBSc6PSLkJ1lsa6mTZumuLg4zy0pKcl2JABumxdJb1xXvhiTpLzs0v2bF9nJVVZJsZTxmfTNW6V/lhTbTqQlG7M1KXVduWJMknJyCzQpdZ2WbMy2lAz1UdqPe3Rb0UuSpBPrLff2bUUvK+3HPQ4nA1CpzYukGd2lV0dIb08s/XNG98BoU6WAbFdrih4yLz3wwAO66667PNt5eXkUZQhNgXa2rKS4tGdMppI7jSSXtOR+qdtwezkDsPeuuMRo6uLN1b1qmrp4s4YmJ9BbUZ1Aez8EsOLMVWrj2l/l/WEuqY32aVvmKum00Q4mA1CB+0Tnia2E+0TnVbPtjj4JwHa1NijIvBQdHa3o6GjbMYLPsSPSE21K//5glhTVyG4e1M3mRdIH90qHyvSc2P4A/Gl1xZ6xcoyUt7P0uI7nOhbL45dGzcioXFljuVFLy9hfoWesLCMpO7dAaRn71b9zc+eCBZMA/0IQaNcGtnId9Olx/hJorxvguEA/0RnoxWINUJDVMzQccEygfgAe3uXb43ypTKNW8V1pt1HbfajqYsyb40JOoL4ffrFkY7YmL9qkXXm/Xvts+9rAzp06SytreJwlgfi6AY4L5BOdgV4s1hDXkNUjSzZma+CTyzTmxc91+2sbNObFzzXwyWVc9wHfO+kHoEo/AC2M3y5u1Mqnx/lUbRo1h7VqEuPT4/zp6LHj6nD/e+pw/3s6euy47TgB/X6Qfr02sGxRIdm/NjC8wwDlN0hQSWUvm6QSI+U3SFB4hwHOBvtFoL5ugOMC+URnALertUFBVk9wMT4cFcAfgGnF3ZRl4qv9kpdlmiutuJuzwaSAbtT6dYxXYlxMJT13pVwq7Rno1zHeyVjBIYDfDye7NlAqvTawuKo3jD+FhavByKflcrlUcsJdJZJcLpcajHzaylntgH7d4BMBd2InkDVu7dvjfCmA29XaoCCrB2g44LgA/gDcfaRIU4uuk6QKRZl7e2rReO0+UuRwssDuvQsPc2nyyGRJqlCUubcnj0wOiCHQZT/L0jL22/9sC+D3Q22uDbQieZRcV82WK7ZNud2u2LZyWRzmGfCvG+os4D5HAtkp55ReD1vdKbvYtqXHOS2Qi8VaCNmCbNWqVWrRooXn9tprr0kqnc6+7P4dO3ZYTnpyQdVwlB2y89PqoJqSFGUE8AdgqyYx+rCknyYV3aEcle/NyVFzTSq6Qx+W9LMy9C6ge+8kDeueqOfHpSghrvxrkxAXo+fHpQTENTNLNmZryDOfeLavn7XW/tDsAH4/BMW1gcmj5Lpjo/T7/0iXvyz9/j9y3fGN1WvuguJ1k3ToaIGuefBp3fbgQ9rw6WIVH6enpyaWbMzWRf+3TGeHbdaosNV64dVXdd70jwJnNNGxI9KUuNLbsSO205T2Ug978peNKk7ZDZtu5xqtQC4WayFkJ/UoKirSvn37Kuw/evSojh496tkuLg78giFYGg7PjHxuc68IqBnIUAvuD8C8bFV+3Yyr9H4LH4DuoXf/ze2njwrPUr+w79RKB7VbTZVW0k1GYdaG3u0+UqRXiq7T85EzVGLKr71UtvfutxZ679yGdU/U0OSEgJwcyD00+8TfOPfQbGtFYwC/H4Lm2sCwcDuznlYhGF639R++qjZrpuq1qF++yyz7f9q1rLmy+k9W74t/by1XoFuyMVsL572gtyNnq03Uryeqswrj9ci866Rr/xwQJ58CTvKo0smJKp1Jdrq973HuYvGN61RalJX9DLZcLNZCyPaQDRo0SMaYk946dOhgO+pJBUPD4ZmB7NAJZ58CaaFe1FwAny0rO/TOKEyflyRrUck5+rwkWeaXjzxbQ+8CufeurPAwl/p3bq5Lz2yr/p2bB0QxFtBDs8u8HyrOn2n3/VD22sAwlXh6BM4O26wwlXBtYBUC/ZrK9R++ql6rb1NLU/7EckuzT71W36b1H75qJVegKy4xWrHw3/pH5AwlqPyooQTt1z8iZ2jFwn8zfLEqyaOkE3qzZbk325PrqtlS7AmFdGwb6zPc1pTLGMNvnQ/k5eUpLi5Oubm5io2NdfRnF5cYDXxymXJyC6o6N6uEuBitvO9CO1+sSopLV3Kv8qL3X84e3/GN/QWFWdC1dipdh6yt3bNlv1iyMVtTF28uN5zX9nTVZd+rLpVU2ntn9b0awNb8uE9jXvz8pMfN/+PZ9tZJ27xIJR/cp7BDZc8e238/uHsEHo6cXW4h5iwTr0eKrtNl9AhUyt0jK1V6zt1aj2zx8ePa+1gXtTT7VNnHRImRdruaq+Vfvld4RMgOhKrUmh9265TU3yhB+6t87XLUXD+N+1z9T7MwE69bQZ40Pan072PfkjpfyPeRmgiw73G1qQ14p9YD7h6BSanrquqstXsxfiCvX+EW4Au6BqzkUTra6WJNeOQ5tdJBPXXDUMV0PjcgGo5AHHpX9r3q7r1zC4j3qluANWpS+SHXYZUUsyW/9H7avhaqIADfD8PC1uriqGdlTjhll+Dar+ejnpUrrI8kPudO5L6m8sQTOwmWT+x898WHOkP7qrxkJswlJWifNn3xoc4YMNzZcGUF4OdIceaqciclThTmktpon7ZlrpJOG+1gsjK4vMN7ATb0uTYoyOoJT8OxaJOyy6yZYrvhkBTQM5BJCvgFXQNeWLinsJh+ykDrDW5Z7qF3gcT9Xj1xsdmAeK9KAXtywj2M8+KwNE2upKdnatF1ATHcM+DeD7+skeaqZDCl55qFIFg01ZZAPLGTf2CnT4/zi82LZJbcJ1eZzxET20Yu258jroM+Pc7n+D4SsijI6pFh3RM1tFNDfTdtoPJd0Wow5EF1O+di+0MWAngGsqBZ4f3YEemJX6aFfjBLimpkLwvqLBC/5EkK6C8D/TrG65rGG/RE0YwK9yVov56PnKEHI+9Vv46/dT5cIAuGEQoBLtBO7DRo1tanx/nc5kUyb1wnc8JJAJOXJb1xndXlDDp36iytrOFxTguW7yPwi5Cd1KNe2rxI4c//RmeEb9dZYT/ojGU3KPy5HvYnzAjkKUkDeEFX1G8BN3HGSb8MqPTLgKWlKsJVosmRsyWpwrUf7u3JkbMVXmGJYWcF3NpGgT5CAbXW7TcXa5eaV7t8Ro6aq9tvLnY2mCSVFCt/8T0yxlT4ghkmyRij/MX32Psc6TBA+Q0Sqn3t8hskKLzDAGeDSXwfCXEUZPVFIM9iGMAz8vFlBfhFoH8Z+Gm1GuTnVHohvlRalDXIz7H6ZYU10uCE8IgIZfWfLEkVCgv3dnb/yVZGxxRnrqrR+7Q4c5WzwTwBwtVg5NNyuVwVTt2USHK5XGow8mm+j1Tj6LHj6nD/e+pw/3s6eox173yFgqw+CPAz25J+nZK0SUL5/banJOXLClAq0L8MBHg+94x8Za8LlH5dI81aURbIIxTgtd4X/17p5zynPa7yQyl3u5or/ZznrK1D9uO2H316nF8kjyodNtmkTbndrti2VodT8n0ktHENWX0QLNcIJI+SOg0KrKlcA3hBV8BRgf5lIIDznWyNNJdK10gbmpzg/NDUerJoKirqffHvVTx4rDZ98aHyD+xUg2Zt1e03FyvB4nXju01TdfHhcX6TPEph3YaXmwXSZXsWSL6PhDR6yOqDAD9zXE7ZDzvbH36S58uKkSodvmAkvqwgNAR6T0oA50vL2F9uWvQTGUnZuQVKy6h6um2/CtQRCqiz8IgInTFguM4a8SedMWC49Um8wjsMUJaJr/YarSzT3M41WidyT5He44rSP22384F8eQf8joKsPgjgM8fBYElJX006drtyTHy5/TmmuSYdu11LSvpaSgY4KNC/DARwvpqufWZ7jTTdnPbr9ti3pDu+oRiDT/Xr3FLPRf5BUtXXtz0XOVH9Ord0OFmQ4ORJyGLIYn1AN7fX3EONskv66b+FZ1VYbNYoTOm2hhoFiRNnlTv3tJa8VsHK/WWg0nXIptv/MhCg+Wq69lkgrJHmEQgjFFDvhIe5NOiyCbpp3jE9HDlbbfRrr3COmuuRovG67MoJtBHVCcTLO+B3FGT1QTBdIxDVSJqSazuFR9mhRiUK8yzoWpZ7qFEgrUMTKJZszNbkRZs829fPWqvEQFngGN5JHlW6zk2ZaysC6st7AObr1zFeiXExysktqOqUmBLiSteasyrAPn9RPw3rnihd+2dduWiAkg6ne05y7mjcS/97ZQ/ahprg5EnIoSCrL9xnjj+4t/zU94FyZjtABcVQI6n8DJk/rQ6Is2XuWeVO/ALqnlXu+XEpNLzByn1tRaAKsHzhYS5NHpmsSanrqjolpskjk+kVCFbHjkhP/DIj34NZpYUtqjWse6KGJicoLaOPdh8qUKsmpSckeA/UECdPQg7XkNUnXCNQa0Ex1GjzIunv/X7dnnuFNKO71bXlTjarnFQ6q5z1RXEBhwzrnqjnx6WoVWx0uf0JcTGcnAh2J54Qs7mETBAJD3Opf+fmuvTMturfuTnFGFANCrL6hm7uWnEPNapm3jYl2hxqFKALfgf8rHKABcO6J2rpXed7tl+5oa9W3nchxVgwC8ATYgDqHwoyhDT3UCOpynnb7A01CuAFv4NmqCfgsLKfFQzRCnIBekIMQP1DQVbfuMcdT8llnHsNuYcaJcSVH5ZofahRbRb8dlhQDPUEAG8F8AkxAPUPk3oAKnsB8v7AuQA5gBf8DppZ5QDAG7U5IRZAE8wA/sZSN/5BDxnwi4C7ADmAF/wO6KGeAFBXAXxCDLBlycZsDXnmE8/29bPWauCTy7RkY3Y1j0JNUJABgcq94Hd1U47EtrW24DezygGotwL4hBhgg3upm115heX2u5e6CYSi7Oix4+pw/3vqcP97OnrsuO04tUJBBgQq94Lfkqrsh7K84DezygGolwL8hBjgJJa68T8KMiCQuRf8bpJQfn9sm9L9AbDGHLPKAah3guCEGOAUlrrxPyb1AAJd8iip0yBpelLp9ti3pM4X8kUACEANoyKUOX247RjwBfcJsQ/uLT/1fWyb0mIsAE6IAU5gqRv/oyADggELfgOA8zghBrDUjQMYsggAAFAVToghxLmXuqnmikolstRNnVCQAQAAVCWqkTQlt/QW1ch2GsBxLHXjfxRkAAAAAKrEUjf+xTVkAAAAAKo1rHuiBpzaQj2m/FdS6VI3557Wkp4xH6CHDAAAAMBJsdSNf9BDBqBOmOYbAADAe/SQAQAAAIAl9JABwcA9yxcAAADqFXrIAAAAAMASCjIAAAAAsISCDAAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAAEGtuMR4/p6Wsb/cdqCjIAMAAAAQtJZszNaQZz7xbF8/a60GPrlMSzZmW0xVcxRkAAAAAILSko3ZmpS6TrvyCsvtz8kt0KTUdUFRlLEwNAAAAICTahgVoczpw23H8CguMZq6eLMqG5xoJLkkTV28WUOTExQe5nI4Xc2FfA/Z+++/ryFDhig+Pl6NGjVSSkqKZs6cqZKSEtvRAAAAAFQhLWO/snMLqrzfSMrOLVBaxn7nQnkhpAuy6dOna/jw4fr444/VrFkznXrqqUpPT9dtt92m0aNHU5QBAAAAAWr3oaqLMW+OsyVkC7I1a9bowQcfVFhYmObNm6cff/xR6enpWrdunVq3bq1FixbpmWeesR0TAAAAQCVaNYnx6XG2hGxB9thjj8kYoz/84Q8aM2aMZ3+vXr08hdj06dNVVFRkKyIAAACAKvTrGK/EuBhVdXWYS1JiXIz6dYx3MlathWRBlpeXp6VLl0qSJk6cWOH+K6+8UrGxsdq3b5+WL1/udDwAAACEqKPHjqvD/e+pw/3v6eix47bjBLTwMJcmj0yWpApFmXt78sjkgJ7QQwrRgmz9+vU6duyYYmJilJKSUuH+yMhI9e3bV5L0xRdfOB0PAAAAQA0M656o58elqFVsdLn9CXExen5cioZ1T7SUrOZCctr7H374QZLUvn17RURU/hJ06tRJH3/8sefYExUWFqqw8Nf1DvLy8nwfFAAAAEC1hnVP1IBTW6jHlP9Kkl65oa/OPa1lwPeMuYVkD9mBAwckSc2aNavyGPd97mNPNG3aNMXFxXluSUlJvg8KAAAA4KTKFl/9OsYHTTEmhWhBVlBQOvVlVFRUlcdER5d2e+bn51d6/wMPPKDc3FzPbceOHb4PCgAAAKBeC8khizExpVNfHjt2rMpj3MMRGzRoUOn90dHRnqINAAAAALwRkj1kJxuOWPa+6oY1AgAAAEBdhGRBdtppp0mStm/fruPHK59OdNu2beWOBQAAAABfC8mCrHfv3oqMjFRBQYHWrVtX4f6ioiKtXbtWkvSb3/zG6XgAAAAAQkRIFmSxsbEaMmSIJOnll1+ucP+bb76pvLw8NW/eXIMGDXI4HQAAAIBQEZIFmSQ99NBDcrlceumllzR//nzP/vT0dN11112SpHvvvbfamRgBAAAAoC5CtiAbMGCAHn30UZWUlOjaa69V586d1atXL6WkpGjXrl0aPny47r77btsxAQAAANRjIVuQSaW9ZIsXL9aFF16offv2aevWrerRo4dmzJihd999V+Hh4bYjAgAAAKjHQnIdsrJGjBihESNG2I4BAAAAIASFdA8ZAAAAANhEQQYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYQkEGAAAAAJZQkAEAAACAJSG/DhkAAACA4NYwKkKZ04fbjuEVesgAAACAAFFcYjx/T8vYX24b9RMFGQAAABAAlmzM1pBnPvFsXz9rrQY+uUxLNmZbTAV/oyADAAAALFuyMVuTUtdpV15huf05uQWalLqOoqweoyADAAAALCouMZq6eLMqG5zo3jd18WaGL9ZTFGQAAACARWkZ+5WdW1Dl/UZSdm6B0jL2OxcKjqEgAwAAACzafajqYsyb4xBcKMgAAAAAi1o1ifHpcQguFGQAAACARf06xisxLkauKu53SUqMi1G/jvFOxoJDKMgAAAAAi8LDXJo8MlmSKhRl7u3JI5MVHlZVyYZgRkEGAAAAWDase6KeH5eiVrHR5fYnxMXo+XEpGtY90VIy+FuE7QAAAAAASouyAae2UI8p/5UkvXJDX517Wkt6xuo5esgAAACAAFG2+OrXMZ5iLARQkAEAAACAJRRkAAAAAGAJBRkAAAAAWEJBBgAAAACWUJABAAAAgCUUZAAAAABgCQUZAAAAAFhCQQYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYQkEGAAAAAJZQkAEAAACAJRRkAAAAAGAJBRkAAAAAWEJBBgAAAACWUJABAAAAgCUUZAAAAABgCQUZAAAAAFgSYTsAAAAAgFINoyKUOX247RhwUEgWZCtWrNCaNWuUlpamtLQ0ZWVlSZJ27Nihdu3aWU4HAAAAIFSEZEF22WWXKTc313YMAAAAACEuJAuyM844Q126dFG/fv3Ur18/nXXWWbYjAQAAAAhBIVmQrVq1ynYEAAAAAGCWRQAAAACwhYIMAAAAACyhIAMAAAAAS0LyGjJfKCwsVGFhoWc7Ly/PYhoAAAAAwYgeMi9NmzZNcXFxnltSUpLtSAAAAACCjMsYY2yHqKl7771XixYtqvXjZs2apf79+1d5v8vlklS7haEr6yFLSkpSbm6uYmNja50RAAAAQP2Ql5enuLi4GtUGQTVkMSsrS1u2bKn1444cOeLzLNHR0YqOjvb58wIAAAAIHUE1ZDE1NVXGmFrfhgwZYjs6AAAAAFQQVAUZAAAAANQnFGQAAAAAYAkFGQAAAABYEpIF2a233qoWLVp4bm49e/b07Lv00kstJgQAAAAQCoJqlkVfOXTokPbt21dh/4EDBzx/z83NdTISAAAAgBAUVOuQBbLarDUAAAAAoP6qTW0QkkMWAQAAACAQUJABAAAAgCUUZAAAAABgCQUZAAAAAFhCQQYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYQkEGAAAAAJZQkAEAAACAJRRkAAAAAGAJBRkAAAAAWEJBBgAAAACWUJABAAAAgCUUZAAAAABgCQUZAAAAAFhCQQYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYQkEGRx09dlwd7n9PHe5/T0ePHbcdBwAAALCKggwAAAAALKEgAwAAAABLKMgAAAAAwBIKMgAAAACwhIIMAAAAACyhIAMAAAAASyjIAAAAAMASCjIAAAAAsISCDAAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAALCEggwAAAAALKEgAwAAAABLKMgAAAAAwBIKMgAAAACwpEYF2Z49e5Senq7Dhw9Xev/evXs1e/ZsnwYDAAAAgPqu2oLs+PHjuuGGG5SQkKCUlBS1bNlSd9xxh/Lz88sd9+OPP+qGG27wa1AAAAAAqG+qLciee+45vf7663rkkUf03nvv6c4779RLL72kc845R7t27XIqIwAAAADUS9UWZP/+97/1v//7v3rooYc0bNgwPfHEE1q7dq3y8/N1zjnnaOvWrU7lBAAAAIB6p9qCLCMjQ+ecc065faeffrpWr16tFi1aaMCAAVq3bp1fA/raoUOHlJqaqnHjxqlLly5q0KCBGjZsqDPOOEP33HOPsrOzbUcEAAAAECKqLchatGhRaYESHx+vZcuWqWfPnrrgggv08ccf+y2gr910000aP3685s6dq5ycHHXr1k3t2rXTli1b9Ne//lU9evTQ2rVrbccEAAAAEAKqLcj69OmjBQsWVHpfo0aN9N5772nw4MH6y1/+4pdw/nLZZZdp6dKl2r9/v9avX6/vv/9eW7ZsUb9+/bRv3z5dfvnlFSYuAQAAAABfq7Ygu/baa7V9+3bt27ev0vujoqL01ltv6U9/+pPat2/vl4C+9uyzz2rBggUaPHiwIiIiPPs7d+6st956S1FRUdqxY4eWLFliMSUAAACAUFBtQXbFFVdozZo1at68edVPEBamF154QRkZGT4P5w/x8fFV3peUlKRu3bpJkr7//nunIgEAAAAIUTVaGDqUFBQUSJIaNGhgOQkAAACA+o6CrIz09HRPz9iAAQMsp6mfikuM5+9pGfvLbQMAAAChJuLkh4SG4uJi3XrrrZKkCy+8UH369Kn2+MLCQhUWFnq28/Ly/JqvPliyMVuTF23ybF8/a60S42I0eWSyhnVPtJgMAAAAsMOrHrJ9+/bpww8/1KJFi2q1OPSePXu8+XGOePDBB/XZZ5+pSZMm+te//nXS46dNm6a4uDjPLSkpyYGUwWvJxmxNSl2nXXmF5fbn5BZoUuo6LdnI+m8AAAAIPS5jTK3GjE2dOlXTpk1TUVGRZ9+gQYM0a9asSmda/Oabb/Sf//xHixcv1tq1a8s9rrbuvfdeLVq0qNaPmzVrlvr371/l/S+88IImTZqkiIgILVy4UMOHDz/pc1bWQ5aUlKTc3FzFxsbWOmN9VlxiNPDJZcrOLaj0fpekhLgYrbzvQoWHuZwNBwAAAPhYXl6e4uLialQb1GrI4vz58zV16lTPdlhYmEpKSrR8+XINHTpUa9asUXx8vDZs2KBXX31VCxcu1Pbt2yVJxhi5XHX7sp2VlaUtW7bU+nFHjhyp8r7XX39dN998s1wul1555ZUaFWOSFB0drejo6FpnCUVpGfurLMYkyUjKzi1QWsZ+9e9c9YyeAAAAQH1Tq4LshRdekCSNGjVKM2bMUIcOHZSZmaknnnhCL730kp599lnl5uZq5syZkkqLMEk688wzNXz48BoXO1VJTU1VampqnZ6jrPfff1/jx49XSUmJ/v73v2vs2LE+e278avehqosxb44DAAAA6otaFWTp6emKjY3V3Llz1ahRI0lShw4d9K9//UsHDhzQzJkzlZubK2OMzj33XI0bN04jRoxQYmLgTdjw6aef6oorrlBRUZGmTZumm266yXakeqtVkxifHgcAAADUF7Wa1CMvL09dunTxFGNlPfDAAzp48KAk6e9//7s++eQT/fGPfwzIYuyrr77SyJEjlZ+frwceeED333+/7Uj1Wr+O8UqMi1FVA1ZdkhLjYtSvY9WLdgMAAAD1Ua1nWYyJqbwXIzk5WZKUmJioSZMm1S2VH23ZskXDhg1TXl6ebrrpJj3xxBO2I9V74WEuTR5Z+vtxYlHm3p48MpkJPQAAABByfLYOmbtQO+WUU3z1lH5x2223ae/evXK5XNqwYYMGDhxY6XETJkzQhAkTHE5Xfw3rnqjnx6Vo8qJN5aa+T2AdMgAAAISwWhdk27Zt01//+lf16NFDPXr0UJs2bcrdHx4e7rNw/uCeqt4Yo9WrV1d53JAhQ5yKFDKGdU/UgFNbqMeU/0qSXrmhr849rSU9YwAAAAhZtS7IsrKydN9993m2mzZtqh49eqh79+6SSgue4uLigC3MVqxYYTtCSCtbfPXrGE8xBgAAgJBW63XINmzYoPT0dG3YsEE5OTk6cOCAPv30U3322WdyuVz68ssv1ahRI51++uk688wz1atXL8+fzZo189e/AwAAAACCTq0KsquvvlpXX321Z3vPnj3lCrT09HRt2bJFx44dU3p6utLT08stBp2UlKTMzEyfhQcAAACAYFanST1atmypoUOHaujQoZ59hYWF2rRpU7lC7euvv1Zubq527NhR58AAAAAAUF/4bJZFt+joaKWkpCglJaXc/szMTKWnp/v6xwEAAABA0PJ5QVaVDh06qEOHDk79OAAAAAAIeLVeGBoAAAAA4BsUZAAAAABgCQUZAAAAAFhCQQYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYQkEGAAAAAJZQkAEAAACAJRRkAAAAAGAJBRkAAAAAWEJBBgAAAACWUJABAAAAgCUUZAAAAABgCQUZAAAAAFhCQQYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYQkEGAAAAAJZQkAEAAACAJRG2AyC0NIyKUOb04bZjAAAAAAGBHjIAAAAAsISCDAAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAALCEggwAAAAALKEgAwAAAABLKMgAAAAAwBIKMgAAAACwhIIMAAAAACyhIAMAAAAASyjIAAAAAMASCjIAAAAAsCTkCrLDhw/rqaee0uWXX66uXbuqWbNmioqKUtu2bXXFFVfok08+sR0RAAAAQIhwGWOM7RBO2rp1q0477TRJUrNmzdSmTRuFh4crIyNDhw4dkiRNnTpVDz/8cK2eNy8vT3FxccrNzVVsbKzPcwMAAAAIDrWpDUKuh6xp06aaOXOmvv/+e+3fv18bN25Uenq69u7dq6eeekqSNGXKFK1du9ZyUgAAAAD1XcgVZC1atNAtt9zi6SVzi4qK0j333KPf/va3MsZo4cKFdgICAAAACBkhV5CdTLdu3SRJR48etZwEAAAAQH1HQVaGMUaff/65JCklJcVyGgAAAAD1HQWZSnvDvv76a1133XVavXq1+vfvrzFjxtiOBQAAAKCei7AdwKYzzzxT6enpnu3GjRtrypQpuvfeexURUf1LU1hYqMLCQs92Xl6e33ICAAAAqJ9Cuoesd+/eGjBggHr06KFGjRrp8OHDevPNN7V69eqTPnbatGmKi4vz3JKSkhxIDAAAAKA+Cap1yO69914tWrSo1o+bNWuW+vfvX+0xRUVFevHFF3X33Xfr+PHjWr58uQYOHFjl8ZX1kCUlJbEOGQAAABDiarMOWVANWczKytKWLVtq/bgjR46c9JjIyEjddNNNys/P1//8z/9oypQpWrp0aZXHR0dHKzo6utZZAAAAAMAtqHrInPDdd9/p9NNPV7NmzbR///4aP642VTAAAACA+qs2tUFIX0NWmePHj5f7EwAAAAD8hYLsBAsXLpRUOgMjAAAAAPhTyBVkzzzzjObPn1/hurJDhw7pySef1COPPCJJuvXWW23EAwAAABBCgmpSD1/4+uuvdffddys8PFydO3dW06ZNtW/fPu3YsUPHjh2Ty+XSww8/rCuvvNJ2VAAAAAD1XMgVZLfddpsSExP1ySef6KefflJGRoaioqLUqVMnDRw4UDfeeKPOOuss2zEBAAAAhICQK8hSUlKUkpJiOwYAAAAAhN41ZAAAAAAQKCjIAAAAAMASCjIAAAAAsISCDAAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAALCEggwAAAAALKEgAwAAAABLKMgAAAAAwBIKMgAAAACwhIIMAAAAACyhIAMAAAAASyjIAAAAAMASCjIAAAAAsISCDAAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAALCEggwAAAAALKEgAwAAAABLKMgAAAAAwBIKMgAAAACwhIIMAAAAACyhIAMAAAAASyjIAAAAAMASCjIAAAAAsISCDAAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAALCEggwAAAAALKEgAwAAAABLKMgAAAAAwBIKMgAAAACwhIIMAAAAACyhIAMAAAAASyjIAAAAAMCSCNsBAAAAAKcVFRWpuLjYdgwEgfDwcEVGRvrt+SnIgCBw9NhxJT/8oSRp8yMXq2EUb10AALyRl5envXv3qrCw0HYUBJHo6Gi1aNFCsbGxPn9uvtUBAAAgJOTl5Wnnzp1q3LixWrRoocjISLlcLtuxEMCMMSoqKlJubq527twpST4vyijIJBUUFKhHjx7aunWrJGnHjh1q166d5VQAAADwpb1796px48Zq164dhRhqrEGDBmrSpIl+/vln7d271+cFGZN6SHrsscc8xRgAAADqn6KiIhUWFiouLo5iDLXmcrkUFxenwsJCFRUV+fS5Q74g+/bbb/X0009r1KhRtqMAAADAT9wTePhzcgbUb+7fHV9PBhPSBZkxRjfeeKPCw8P17LPP2o4DAAAAP6N3DN7y1+9OSF9D9vLLL+uzzz7T448/rg4dOtiOAwAAACDEhGwP2Z49e3Tfffepa9eu+p//+R/bcQAAAACEoJDtIbvzzju1f/9+vfHGG4qKirIdBwAAAPUQa4niZELyN+Ljjz/W3Llzdc0112jw4MFePUdhYWG5BQXz8vJ8FQ8AAABAiAi5IYsFBQX685//rCZNmuiZZ57x+nmmTZumuLg4zy0pKcmHKQEAAAD7evToIZfLpQYNGtS5A2LQoEFyuVy1vp3o1ltv9dz30UcflbtvxYoVXv2MKVOm1OnfVhdB1UN27733atGiRbV+3KxZs9S/f39Jv6459re//U2JiYleZ3nggQd01113ebbz8vIoygAAAFBvbNiwQRs3bpRU2qnx1ltvacKECV4/X48ePXT8+PEK+1etWiVJ6t69u+Li4qp9jqKiIr322mue7Tlz5mjo0KGe7bi4OA0YMKDC47Zv364dO3YoNjZWPXr0qHB/+/bta/zv8LWgKsiysrK0ZcuWWj/uyJEjkqQffvhBTz/9tHr16qVbb721Tlmio6MVHR1dp+cAAAAAAtWcOXMkSU2bNtXBgwc1Z86cOhVkM2fOrHS/uxds5syZGjRoULXPsWTJEu3du9eT6Z133tHzzz+vRo0aSZJ69+6tlStXVnjclClTNHXqVPXu3VsrVqzw+t/gD0E1ZDE1NVXGmFrfhgwZIknatGmTjh07ph9//FFt27ZVQkJCuZtbSkqKEhIS9Ne//tXWPxUAAACwpri4WPPnz5ck/b//9/8UHh6uTz75RNu3b7eay10k3nzzzTrjjDN05MgRLViwwGqmugqqgsxXDh8+rF27dlW4ue3Zs0e7du3S4cOHLaYEAABAsCsuMZ6/p2XsL7cdyJYuXars7GwlJCTommuu0YUXXihjjObOnWstU25urhYvXixJuvbaa3XttddK+rVIC1YhVZBddtll1fakue3YsUPGGKsX9wEAACC4LdmYrSHPfOLZvn7WWg18cpmWbMy2mKpmZs+eLUm6+uqrFR4errFjx0qyW/y88cYbKigoUK9evZScnKxrr71WLpdLH3/8sbKzA/81rUpIFWQAAACAE5ZszNak1HXalVdYbn9OboEmpa4L6KLs8OHDWrhwoSR5CrHf/e53atCggb799lt99dVXVnK5i0F3pg4dOuicc85RcXGx5s2bZyWTL1CQAQAAAD5UXGI0dfFmVTY40b1v6uLNATt88e2339bRo0d16qmnqm/fvpKkJk2aaMSIEZLs9JJlZmZq5cqVcrlcuuaaazz768OwRQoyAAAAwIfSMvYrO7egyvuNpOzcAqVl7HcuVC24ixt3sePm7pmaP39+pdPX+5N7cr9zzz233FJTV111lSIjI5Wenq5vvvnG0Uy+QkFWhvtasnbt2tmOAgAAgCC1+1DVxZg3xzlp586dWr58uaSKBdkll1yiZs2aaffu3frvf//raK7U1NRKM7Vo0UIXXXSRpODtJaMgAwAAAHyoVZMYnx7npLlz56qkpEQpKSnq2rVrufuioqJ05ZVXSnK2+ElLS9OWLVsUGRnp+flluXvu5s2bp5KSEsdy+UpQLQwNAAAABLp+HeOVGBejnNyCSq8jc0lKiItRv47xTkc7KXehtW7dOs+CzZV59913lZeXp9jYWL9ncs/4WFRUpObNm1d53M6dO7Vs2TLPGsTBgoIMAAAA8KHwMJcmj0zWpNR1cknlijJ3iTN5ZLLCw6oueGxYv369Nm7cKJfLpVatWlV53IEDB5Sfn6+3335bN9xwg18zFRUV6fXXX5ckNW/eXBERlZcvR44c0eHDhzVnzpygK8gYsggAAAD42LDuiXp+XIpaxUaX258QF6Pnx6VoWPdES8mq5u4dO++885STk1Pl7e677y53vD998MEH2rt3rxo1aqTMzMwqM7399tuSpHfeeUdHjx71ey5foiADAAAA/GBY90Qtvet8z/YrN/TVyvsuDMhirLi4WPPnz5ckjR8/vtpjx40bJ0lasWKFduzY4ddc7qJv9OjRaty4cZXHDR48WImJiTp8+LAWLFjg10y+RkEGAAAA+EnZYYn9OsYH3DBFt48++kg5OTmKiYnRFVdcUe2xycnJ6t27t4wxmjt3rt8yHTx4UIsXL5Z08iIxPDzcsz5ZsM22SEEGAAAAhDh3ETNy5EjFxcWd9Hh3L5k/i58333xThYWFSkhI0ODBg2ucaenSpcrJyfFbLl9zGWMCc4nwIJOXl6e4uDjl5uY6MtsMQsvRY8eV/PCHkqTNj1yshlHMxwMAQG0UFBQoIyNDHTt2VEyMc9PN04bXH7X5HapNbUAPGQAAAABYQkEGBIHikl87stMy9pfbBgAAQPCizxQIcEs2Zmvyok2e7etnrVViXIwmj0wOyFmaAADArxpGRShz+nDbMfxm4MCBNT52woQJmjBhgh/TBCcKMiCALdmYrUmp63Rif1hOboEmpa4L2HVMAABAaFi1alWNjw22BZudQkEGBKjiEqOpizdXKMYkyUhySZq6eLOGJicE7BS6AACgfmN+wLrjGjIgQKVl7Fd2bkGV9xtJ2bkFSsvY71woAAAA+BQFGRCgdh+quhjz5jgAAAAEHgoyIEC1alKzNVJqehwAAAACDwUZEKD6dYxXYlyMqro6zCUpMS5G/TrGOxkLAAAAPkRBBgSo8DCXJo9MlqQKRZl7e/LIZCb0AAAACGIUZEAAG9Y9Uc+PS1Gr2Ohy+xPiYpjyHgAAoB5g2nsgwA3rnqgBp7ZQjyn/lSS9ckNfnXtaS3rGAAAA6gF6yIAgULb46tcxnmIMAACgnqAgAwAAAABLKMgAAAAAfzl2RJoSV3o7dsR2GgQgCjIAAAAAsISCDAAAAAhx1157rVwul8aOHVuj45955hm5XC6dccYZNf4ZgwYNksvlqvXtRLfeeqvnvo8++qjcfStWrPDqZ0yZMqXG/w5fY5ZFAAAAIMRdd911mj9/vhYuXKjDhw+rcePG1R6fmpoqSRo/fnyNf0aPHj10/PjxCvtXrVolSerevbvi4uKqfY6ioiK99tprnu05c+Zo6NChnu24uDgNGDCgwuO2b9+uHTt2KDY2Vj169Khwf/v27Wv87/A1CjIAAAAgxA0dOlQJCQnKycnRggULqi20vv32W61fv75WPWqSNHPmzEr3u3vBZs6cqUGDBlX7HEuWLNHevXvVtGlTHTx4UO+8846ef/55NWrUSJLUu3dvrVy5ssLjpkyZoqlTp6p3795asWJFjTM7gSGLAAAAQIgLDw/XmDFjJElz586t9tg5c+ZIKh2CmJSU5Pdslf3sm2++WWeccYaOHDmiBQsWOJrB1yjIAAAAAH8pKf717z+tLr8dYNy9YkuXLtWuXbsqPcYYo3nz5pU73im5ublavHixpNJr3q699lpJvxZpwYqCDAAAAPCHzYukv/f7dXvuFdKM7qX7A1Dv3r3VvXt3FRcXa/78+ZUe89lnn+mnn35SgwYNdPnllzua74033lBBQYF69eql5ORkz0QkH3/8sbKzsx3N4ksUZAAAAICvbV4kvXGddOiEQiEvu3R/gBZl7l6vqoYtuifzuPTSSxUbG+tYLunXnjD3dWsdOnTQOeeco+LiYk+vXTCiIAMAAAB8qaRYWnKfJFPJnb/sW3J/QA5fHDt2rMLCwvTll19qy5Yt5e47duyY3nrrLUnOD1fMzMzUypUr5XK5dM0113j214dhixRkAAAAgC/9tFrKy6rmACPl7Sw9LsC0bdtWF1xwgaRfe8Pc/vOf/+jAgQNq1aqVLrroIkdzpaamyhijc889t9xEIldddZUiIyOVnp6ub775xtFMvkJBBgAAAPjS4conxPD6OIe5e79OHAboLtDGjBmjiAhnV89y/2x3j5hbixYtPMVhsPaSUZABAAAAvtS4tW+Pc9jll1+uhg0batu2bVq9urQX7+DBg3r//fclOT9cMS0tTVu2bFFkZKSuvPLKCve7rymbN2+eSkpKHM3mCxRkAAAAgC+dco4U20aSq4oDXFJs29LjAlDjxo112WWXSfq1Z+qNN95QYWGhTj/9dPXp08fRPLNnz5YkFRUVqXnz5nK5XOVu7l6znTt3atmyZY5m8wUKMgAAAMCXwsKlYU/+snFiUfbL9rDppccFqOuuu05SaSFWVFTkKcyc7h0rKirS66+/Lklq3ry5WrduXemtcePGkoJz2CIFGQAAAOBryaOkq2ZLTRLK749tU7o/eZSdXDU0ZMgQJSQkaN++ffrnP//pmeHQPTzQKR988IH27t2rRo0aKTMzUzk5OZXe3n77bUnSO++8o6NHjzqasa4oyAAAAAB/SB4l3Zz26/bYt6Q7vgn4YkySwsPDPUMB77nnHhljdP7556t9+/aO5nD3eI0ePdrTC1aZwYMHKzExUYcPH9aCBQuciucTFGQAAACAv5QdlnjKOQE9TPFE7uGJBQUF5badcvDgQS1evLhGPzs8PNyzPlmwDVukIAMAAABQwZlnnqkePXpIkmJiYnTFFVc4+vPffPNNFRYWKiEhQYMHDz7p8ePGjZMkLV26VDk5Of6O5zMhWZCdODPLibeyq38DAAAAoerrr7+WMUb5+fmKjY31y88wxsgYo0GDBpXb/8c//lHGGGVnZys8/OQ9iykpKTLG6Pjx40pIKH/t3pQpU2SM0YoVK3yY3DecXdEtwAwYMKDS/d26dXM4CQAAAIBQFNIF2cqVK21HAAAAABDCQrogAwAAAPwqqpE0Jdd2Cr8ZOHBgjY+dMGGCJkyY4Mc0wYmCDAAAAIBXVq1aVeNjhwwZ4sckwSukC7LbbrtN3333ncLCwtSpUyeNGDFCl1xyiVyuE1dUBwAAAHAiY4ztCEEvpAuymTNnltt+/vnndd555+mtt95Sy5Ytq31sYWGhCgsLPdt5eXl+yQgAAACg/grJae+HDRumN954Qz/++KMKCgr0888/a+bMmYqNjdWnn36qkSNH6vjx49U+x7Rp0xQXF+e5JSUlOZQeoahhVIQypw9X5vThahgV0udRAAAA6hWXoZ/RY+3atRowYICKioo0e/bsalcEr6yHLCkpSbm5uX5bowEAAADeKSgoUEZGhjp27KiYmBjbcRCEavM7lJeXp7i4uBrVBkF1qv3ee+/VokWLav24WbNmqX///ic9rm/fvrriiis0f/58vfPOO9UWZNHR0YqOjq51FgAAANhDXwS85a/fnaAqyLKysrRly5ZaP+7IkSM1PrZ///6aP3++tm7dWuufAwAAgMAUFlZ6pU5xcbHlJAhW7t8d9++SrwTVNWSpqakyxtT6VpspNiMjIyXppNeQAQAAIHhERkYqPDxc+fn5tqMgSOXn5ys8PNxTL/hKUBVkTti0aZMkqV27dpaTAAAAwFdcLpcaNmyo3NxceslQa8XFxcrNzVXDhg19vkRWUA1Z9Lddu3Zp7ty5kli4DgAAoL5p1aqVMjMz9dNPPyk+Pl7R0dGsP4tqGWNUWFio/fv3q6SkRK1atfL5zwi5guyBBx5Q9+7dNXr0aDVs2NCzPz09XePHj9eBAwfUqlUr3XjjjRZTAgAAwNeioqLUrl077d27V9nZ2bbjIIg0atRICQkJioqK8vlzh1xB9u2332r69OmKiIjQqaeeqri4OO3Zs0fbtm2TJLVu3VqLFi1S06ZN7QYFAACAzzVs2FDt27fX8ePHmTMANRIREaGICP+VTSFXkE2aNEmtW7dWWlqasrKytHXrVjVs2FB9+/bV8OHDdfPNN6tFixa2YwIAAMCP/P0lG6gpFob2kdos/gYAAACg/qpNbcAsiwAAAABgCQUZAAAAAFhCQQYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYwuILPuJePSAvL89yEgAAAAA2uWuCmqwwRkHmI4cOHZIkJSUlWU4CAAAAIBAcOnRIcXFx1R7DwtA+UlJSoqysLDVp0kQul8tqlry8PCUlJWnHjh0BuUh1IOcjm3fI5h2yeS+Q85HNO2TzDtm8F8j5yOadQMpmjNGhQ4fUpk0bhYVVf5UYPWQ+EhYWpnbt2tmOUU5sbKz1X8bqBHI+snmHbN4hm/cCOR/ZvEM275DNe4Gcj2zeCZRsJ+sZc2NSDwAAAACwhIIMAAAAACyhIKuHoqOjNXnyZEVHR9uOUqlAzkc275DNO2TzXiDnI5t3yOYdsnkvkPORzTuBnK06TOoBAAAAAJbQQwYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYQkEWhN5//30NGTJE8fHxatSokVJSUjRz5kyVlJRUODYnJ0ezZ8/WLbfcon79+ik6Oloul0t/+MMfrGdbv369Hn74YZ1//vlq0aKFIiMj1apVK11yySVasGCB1WzLly/Xbbfdpv79+6tt27aKjo5WkyZN1KdPHz366KM6dOiQtWyVeemll+Ryufzyf1ubbFOmTPHkqOr23XffWctX1htvvKFhw4apdevWio6OVtu2bTVs2DD9+9//tpLtZK+b+/bqq686nk2SDh06pEceeUS9e/dW48aNFRUVpfbt22vs2LFat26dTzJ5my03N1cPP/ywunfvroYNG6pp06Y677zzNH/+fJ9lysjI0Isvvqg//vGP6tWrlyIiIuRyufTYY4+d9LFr1qzRpZdeqpYtW6pBgwZKTk7Wo48+qoKCAmvZnGobvMnmVNvgTTYn24a6/M6V5Y/2wZtsTrUPdX3d/Nk2eJPNqbbB29fNibbB22xOtA0+YxBUpk2bZiQZSaZTp06mZ8+eJiwszEgyo0aNMsXFxeWO/9vf/uY5vuxt4sSJVrNt3bq1XJ6OHTuaPn36mGbNmnn2/f73v6/w73EimzHGjB071kgyERERpn379uass84yp5xyinG5XJ68P/30k5VsJ9q9e7eJj4/3y/9tbbNNnjzZSDJJSUlmwIABld589bp5k88YYwoKCsyoUaPKPa5v374mKSnJhIWFmT59+ljJVtXrNWDAAJOcnOx5ru+++87xbLt27TJdunQxkkxYWJjp3Lmz6dWrl2ncuLGRZMLDw828efPqnMubbD///LM57bTTPDl69eplkpOTPe/VP//5zz7Jdfvtt1f6Wfroo49W+7jU1FQTHh5uJJm2bdua3r17m8jISCPJ9O3b1xw5csRKNqfahtpmc7Jt8OZ1c7Jt8PZ3rix/tQ/eZHOqffD2dXOibfAmm1NtgzfZnGobvMnmVNvgKxRkQWT16tXG5XKZsLCwcr/gGzZsMK1btzaSzNNPP13uMS+//LIZOnSoeeihh8y7775rbr31Vr80urXN9sMPP5jExETz5JNPmqysLM/+4uJiM3PmTM8bZubMmY5nM8aYt956y3zwwQfm6NGj5fZv2rTJ9OzZ00gyv/3tb61kO9HYsWNNWFiYGT58uE//b73J5m5wJ0+e7JMMvs5njDFjxowxksx5551XoQHbvXu3+fDDD61lq8pDDz1kJJl+/fpZyTZx4kQjyXTt2tV8++23nv2HDx82f/rTn4wkExsba3Jzcx3PdsEFFxhJ5owzzjAZGRnlHtOmTRsjycyePbtOuYwx5tFHHzUjRowwjzzyiPnggw/M5ZdfftIvBBkZGSY6OtpIMk899ZQpKSkxxhiTmZlpunbtaiSZm2++2Uo2p9qG2mZzsm3w5nVzqm3wNt+J/NU+eJPNqfbB29fNibbBF/+nZfmybfAmm1NtgzfZnGobfIWCLIj89re/NZLMn/70pwr3zZ0710gyzZs3N8eOHavyOdwfiL5udGubLT8/v9ozw3/+85+NJNOzZ0/Hs51MWlqa54xLfn6+1WwfffSRkWQmTZrk8/9bb7I5WZB5k++DDz4wkky3bt0qfKGyna0qJSUlpkOHDj77EupNtoSEBCPJLFq0qMJjioqKTIsWLYwk8/777zuabcOGDZ4zpWvWrKnwmNdee81zptvXfv/735/0C8FNN91kJJmLLrqown2rVq0ykkxkZKTJyclxPNuJ/NU2nOhk2ZxsG2qb7WR82TZUprb5/Nk+eJPNyfahrJpkc6pt8CZbVXzdNniTzam2obbZbLYN3uIasiCRl5enpUuXSpImTpxY4f4rr7xSsbGx2rdvn5YvXx7w2WJiYtSwYcMqn/Oiiy6SJH3//feOZzuZbt26SZKKi4tVWFhoLVtBQYEmTZqkVq1a6YknnvA6hz+y+Zu3+WbMmCFJ+stf/qIGDRoEVLaqfPbZZ8rMzFRkZKSuueYaK9ny8/MlSZ06darwmIiICJ1yyimSpOPHjzuabdWqVZKkdu3a6eyzz67wmNGjRyssLEzbtm3TV1995XU2bxhjPNc7VfbvOeecc9StWzcVFRXp3XffdTRbIHOqbfAHX7UNvuDP9qE+cqJt8DVftg3ecqJt8EYgtw1VoSALEuvXr9exY8cUExOjlJSUCvdHRkaqb9++kqQvvvgi6LO5L3av6wejP7KtWbNGUukHUFxcnLVsjz32mLZu3aqnn35aTZs29TqHP7ItX75cV155pS688EJdccUVeuqpp5STk2M1X35+vj7++GO5XC4NHz5cK1as0MSJEzV48GBdfvnlmjFjhk8uyPf171xqaqokadiwYWrRooWVbD179pQkrV69usJj9u/fr++++04RERE688wzHc124MABSVLbtm0rfc6oqCjPa/b55597nc0b27dvV3Z2tiRpwIABlR7j3u/0Z3Yw81Xb4A++aht8wZ/tQ135u32oLafaBl/zZdvgLSfaBm8EcttQFQqyIPHDDz9Iktq3b6+IiIhKj3GfoXAf6xR/ZHvjjTckVf1Fxulsxhjl5ORo7ty5uv766xUREaFnnnnGWrZvv/1WTz/9tM4991xdd911dcrh62yS9Omnn+qtt97S8uXL9fbbb+u+++5Tp06d9Morr1jLl56eruPHj6tNmzZ68skndcEFF+jf//63li1bpnfeeUd33nmnunXrpg0bNjierSqFhYV68803JUnjx4+vU666ZJsyZYoiIyN1zz33aNasWdq1a5eOHDmiVatWacSIETpy5Ijuv/9+JSUlOZrN/aV3586dlR5/7Ngx7d27V5K0ZcsWr7N5w50xOjpabdq0qfQYW5/ZwcxXbYOv+KNtqCt/tw915e/2obacaht8yddtg7ecaBu8EchtQ1UoyIKEu9pv1qxZlce473Mf6xRfZ/vvf/+rhQsXSpLuueceq9kWLlwol8ulsLAwJSYmaty4cerSpYtWrFihSy+91Eo2Y4xuvPFGlZSU6B//+EedMvg6W2Jioh588EGtXbtW+/bt09GjR7Vq1Spdcsklys/P14QJE7R48WIr+dy9Fbt379b06dM1cuRIfffddyosLFRaWppSUlKUlZWlSy+9VIcPH3Y0W1UWL16sgwcPKi4uTiNHjvQ6U12zXXjhhfroo4/Us2dPTZgwQQkJCWrcuLEGDhyo7Oxspaam6tFHH3U8m7vH7Oeff1ZaWlqF4xcuXOiZKt/W52LTpk3lcrkqPcbWZ3aw8mXbUFf+bBvqwon2wVtOtQ+15VTb4Eu+bhu85UTb4I1AbhuqQkEWJNzDNKKioqo8Jjo6WtKvY3qd4sts27dv19ixYyVJN910k8477zyr2Zo3b64BAwbo7LPPVtu2beVyuZSWlqbZs2fX+XX2NtvLL7+szz77THfccYe6d+9epwy+znbjjTfq8ccf11lnnaX4+Hg1aNBA55xzjt577z2NHj1axhjdeeedMsY4nu/IkSOSpKKiInXq1Elvv/22unbtqqioKPXt21fvvfeeGjZsqO3bt2vWrFmOZquKe0jKlVdeqZiYGK8z+SJbRkaGdu/eLZfLpVNOOUU9evRQgwYNlJmZqZdeekmZmZmOZ/vNb36jPn36SJKuv/76ctcVffHFF7rzzjs928H8uQjftw115c+2oS6caB+85VT7UFtOtQ2+5Ou2oS783TZ4I5DbhqpQkAUJ9xvu2LFjVR7jvojY6bH1vsq2f/9+XXLJJdq7d68GDRrkk2Efdc127rnnauXKlVqzZo1+/vlnbdq0SWeffbb+9a9/6Xe/+53j2fbs2aP77rtP7dq10+TJk+v0832drToul0vTp0+XJP3444/6+uuvHc9XttG66aabFBkZWe74hIQEz4XRS5YscTRbZfbt26f3339fknw27MjbbNOmTdMNN9wgl8ulDRs2KDMzU19//bV2796tiRMnasWKFRowYIByc3MdzzZ37lwlJCTo22+/1emnn66uXbuqY8eOOvvss3X06FHP2ePGjRt7nc0bgfyZHWz80TbUlT/bBm851T74mq/bh9pyqm3wFX+0Dd5yom3wVqC2DVWhIAsSNRnaUpMhP/7gi2yHDx/Wb3/7W23evFl9+vTRokWLPGePbWcr6/TTT9fixYvVunVrLVmyRCtXrnQ027333qv9+/frb3/7m18/RPzx+9alSxfFx8dLkrZu3ep4vrI53bOhnej000+XpDqd0fPVa/f666+rqKhIHTp00MCBA73OU9dsu3fv1iOPPCJJeuWVVzwXcUulDdkLL7yg5ORkZWVl1WmIlLevW9euXbV+/Xrdfvvt6tChgzIzM3XkyBGNHTtW69atU2xsrKTSL1VOcmc8ePBglWf8bX1mBxN/tQ2+5su2wVtOtQ/+4Mv2obacaht8xR9tgzecahu8FahtQ1UoyILEaaedJql02EZV04du27at3LFOqWu2wsJCXXrppfriiy+UnJysJUuWqEmTJgGRrTKNGjXSoEGDJEnr1q1zNNv69eslSbfccosSEhLK3f76179KkubNm+fZ52S2mnCfeazrFLje5Ovatavnvqq+0Ln3FxcXO5qtMu4hKePGjavyGiQnsn355ZcqKChQ48aN1a9fvwrHR0REeN4PX375paPZ3BISEjRjxgz9+OOPKiws1O7du5WamqqOHTt6MrmHrzjFnbGwsFBZWVmVHmPrMztY+LNt8AdftQ3ecqp98BdftQ+15VTb4Cv+aBu84VTbUBeB2DZUhYIsSPTu3VuRkZEqKCio9IO+qKhIa9eulVQ6djZYsh0/flxXXXWVli1bpk6dOumjjz7y6fSt/nrd3A1GXRqOumTbtWtXhZt7HHx+fr5nn41sVdm7d692794tqXRtkLrwJl+7du08Mz25vwifyL2/qqly/ZXtRD/++KNnCu1x48Z5ncUX2Woy3bO7B8h93ZRT2U5m06ZN2rJli2JiYjRkyBCvs3mjffv2ni+97jVxTuTe7/RndjDwd9vgL75oG+rK3+2DP/iyfagtp9oGX/BX2+ANp9oGf7DZNlSFgixIxMbGen5pXn755Qr3v/nmm8rLy1Pz5s09ZyQCPZsxRtdff70WLVqkNm3aaOnSpVVOD+10turk5uZ6Fqaty9oa3mTbsGGDjDGV3tzXDEycONGzz8lsJ/PMM8/IGKO4uDjPDEhO57vyyislSbNnz67wmIKCAr3++uuSSmeOcjpbWXPmzJEk9evXr9zZ27ryJpu79+bw4cOVzlZ1/PhxffLJJ5JKhx05ma06xhg98MADkqSxY8c6PizQ5XJp9OjRkir/96xevVrfffedIiMjNWrUKEezBTon2gZ/8FXb4C2n2gd/8GX74A0n2gZf8Ffb4A2n2gZfs902VMkgaKxcudK4XC4TFhZm5s2b59m/YcMG07p1ayPJPPnkk9U+x+TJk40kM3HiROvZbr31ViPJtGjRwmzevNmneeqSbefOneb22283GzdurPBca9asMWeffbaRZHr06GGOHz/uaLbq+Pr/trbZNm7caCZNmlThdcvPzzePP/64CQsLM5LME088YSWfMcZkZ2ebxo0bG0nmscceM8XFxcYYY44ePWp+//vfG0mmWbNmZvfu3Y5nK+vUU081kszMmTPrlMMX2UpKSkxycrKRZLp162bS09M99+Xl5ZmJEycaSUaS+fLLLx3NZowxn332mVm6dKkpKSnx7Nu7d6/n/7N169Zmz549dcpVGffzP/roo1Ues23bNhMVFWUkmaeeesqTMTMz03Tt2tVIMpMmTbKS7UT+ahtOVJNsTrUNtc3mZNvgTb7q+Pv/92TZnG4fapPNGOfaBm+yleXPtuFEJ8vmZNtQ22zG2GsbvEVBFmQee+wxzy94p06dTM+ePT0fZMOHD6/QCGzfvt00b97cc2vQoIGRZKKjo8vtX7lypaPZVq9e7Tk2KSnJDBgwoMqbL9QmW0ZGhufY+Ph4k5KSYnr37m1atGjh2d+5c2ezdetWx7NVxx8Nbm2yrV+/3nNsy5YtTZ8+fUyfPn1Mw4YNPfsnTpxY7sPRyXxuixYt8nxJbt26tenbt6+Ji4szkkzDhg3Nhx9+aC2bMb++NyIjI/3WWNQ221dffWWaNWtmJBmXy2U6dOhgevbs6fk8cX+JsZHtb3/7m5FkmjRpYnr27Gl69OhhIiIijCTTtm1b88033/gk18qVK8t9ZkZHR3t+Z8ru3759e7nHvfrqq578bdu2Nb179zaRkZFGkunTp485fPiwlWxOtQ21zeZk21DbbE63Dd7+zlXGHyfsapPNyfbB29fNibahLv+n/m4bvMnmVNvgTTan2gZfoSALQosXLzYXXnihiYuLMw0bNjS9evUyM2bMqPQLXtkGpLrb8uXLHc22fPnyGuWSfPcrWtNs+fn55p///Ke56qqrTJcuXUxcXJyJiIgwLVu2NBdeeKF57rnnzNGjR32WqzbZquOvM6A1zXbgwAHz6KOPmksuucR07NjRNG7c2ERFRZl27dqZK664wixZssSnuWqbr6yvv/7aXHPNNSYhIcFERkaaNm3amOuuu858++231rNNmjTJSDIjR470aZa6Ztu5c6e56667THJysmnQoIHndbv88svNsmXLrGVbv369ue6668xpp51mGjVqZBo3bmx69uxppkyZYnJzc32WqaafWRkZGRUeu2rVKjNixAgTHx9voqOjTdeuXc2UKVNMfn6+tWxOtQ21zeZk21DbbE63DXX5nTuRr9uH2mZzsn2oy+vm77ahLtn83TZ4m82JtsGbbE61Db7iMibABhIDAAAAQIhgUg8AAAAAsISCDAAAAAAsoSADAAAAAEsoyAAAAADAEgoyAAAAALCEggwAAAAALKEgAwAAAABLKMgAAAAAwBIKMgAAAACwhIIMAIAAc/3118vlcumVV16xHQUA4GcUZACAkNChQwe5XC65XC7dfffd1R777LPPeo51uVwOJQQAhCIKMgBAyJk3b56Ki4urvD81NdXBNACAUEZBBgAIKV27dlVOTo6WLl1a6f1btmzRl19+qa5duzqcDAAQiijIAAAhZdy4cZKq7gWbM2eOJGn8+PGOZQIAhC4KMgBASDn//POVlJSkBQsW6MiRI+XuM8Zo7ty5atCggX73u99V+RzGGL322msaOnSomjdvrujoaHXq1Em33XabcnJyKn3MypUrNXr0aCUkJCgyMlLx8fE6/fTT9Yc//EGff/55lT8rKytLEyZMUGJiomJiYnTGGWfo73//u3f/eABAwKEgAwCEFJfLpbFjx+rIkSNasGBBuftWrlypzMxMXXbZZWrSpEmljy8qKtLVV1+tMWPGaOnSpYqJidHpp5+uXbt2aebMmUpJSdH3339f7jHvvvuuzj//fC1cuFDHjx9Xz5491bp1a+3YsUMvv/yyXnvttUp/1k8//aQ+ffpo/vz5atOmjZo3b67Nmzfrlltu0eOPP+6bFwQAYBUFGQAg5LiHI7qHJ7rVZLjiww8/rDfffFO9e/fW+vXrtXPnTm3YsEF79+7VTTfdpOzsbI0dO7bcY/7yl7+opKRE//jHP7Rr1y599dVX+vbbb3Xo0CEtX75cgwcPrvRnPf744xo4cKCys7P11VdfaefOnfrHP/4hSXrsscd08OBBb18CAECAcBljjO0QAAD4W4cOHfTTTz/ps88+08CBA5WSkqKvv/5aO3bsUGJiogoLC5WQkKCoqCjt3LlTOTk5SkpKklQ6RFGS9uzZo6SkJEVHR2vTpk1q165duZ9RUlKis88+W2vXrtWnn36qc889V5IUExOjhg0bav/+/TXKev311+vVV19VQkKCtm7dqkaNGpW7v0+fPlq3bp3eeecdjR49uq4vDQDAInrIAAAhafz48SouLtb8+fMlSf/5z3908OBBjRkzRhEREZU+5v3331dhYaEuvvjiCsWYJIWFhWnEiBGSpE8++cSzPykpSQcPHtRHH31Uq4xjxoypUIxJUt++fSVJ27Ztq9XzAQACT+UtDgAA9dyYMWN0zz33aM6cObrrrrs8wxXdszBW5ptvvpEkff755xo4cGClx+zatUuStHPnTs++O++8UzfffLMuuugi9enTR0OGDNHAgQN1/vnnV3mtmiR17ty50v2tWrWSJB0+fLiafyEAIBhQkAEAQlJCQoKGDBmiDz/8UJ9++qk++OADdevWTWeddVaVj8nNzZUk7dixQzt27Kj2+fPz8z1/v+mmm9SkSRP93//9n7766it99dVXevLJJxUTE6Px48fr6aefVlxcXIXnqKx3TCrtiZN+HUoJAAheDFkEAIQs9+Qd48eP17Fjx0669ljjxo0lSQ899JCMMdXeXnnllQo/a8OGDcrOztZrr72miRMnKiIiQi+++GK1vXIAgPqNggwAELJGjx6txo0ba/v27Z7p8KuTnJwsSdq4caPXPzMhIUFXX321XnrpJX3xxRcKCwvTf/7zH2VnZ3v9nACA4EVBBgAIWQ0bNtTdd9+twYMH68Ybb9Qpp5xS7fHDhw9XVFSU3n//ff3www91/vnJycmeoYpZWVl1fj4AQPChIAMAhLQpU6Zo6dKlev755096bJs2bXTHHXeoqKhIF198sVasWFHufmOM0tLSNGnSJM8MiHl5ebrmmmu0YsUKlZSUeI4tLi7Wc889pwMHDqhRo0bq2rWrT/9dAIDgwKQeAADUwuOPP66srCylpqbqggsuUEJCgtq3b6/CwkJt27ZNhw4dkiTdfvvtkkrXJnv99df1+uuvq1GjRjr11FMVGRmpzMxM7d27Vy6XSzNmzPBcnwYACC30kAEAUAsRERGaM2eO3nvvPV122WWSpPXr1ys7O1tdunTRLbfcohUrVqhLly6SpCZNmmjOnDkaP368kpKSlJmZqU2bNik+Pl7jxo3T+vXr9Yc//MHivwgAYJPLMGcuAAAAAFhBDxkAAAAAWEJBBgAAAACWUJABAAAAgCUUZAAAAABgCQUZAAAAAFhCQQYAAAAAllCQAQAAAIAlFGQAAAAAYAkFGQAAAABYQkEGAAAAAJZQkAEAAACAJRRkAAAAAGAJBRkAAAAAWEJBBgAAAACW/H+gHMD4U1mKZwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(meshes,R2_leftout.mean(axis=1)[7,:,0].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=1)[7,:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2_leftout.mean(axis=1)[7,:,1].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=1)[7,:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'),fontsize=fontS)\n",
    "plt.xlabel('Mesh',fontsize=fontS)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('figures/WeavingDTLatent800Leftout.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38a08a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120.8960, 119.6680], dtype=torch.float64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_output[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a51b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24ec63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp=17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ab80dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ba538550>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_input[3][:,3],all_output[3][:,0],'o')\n",
    "plt.plot(all_input[comp][:,3],all_output[comp][:,0],'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb25a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.7487,   0.2863,   2.9323,   0.3908,   0.4751,   2.8957, -15.0770,\n",
       "         28.8892,   8.6449,   4.7580,   1.3011,   5.9833, -12.7411,  29.4289,\n",
       "          0.9004,  -3.1176,   5.3607,  -4.3849,  -5.0521,  -0.3224,   1.7646,\n",
       "          0.2812,   0.3799,   1.4111], dtype=torch.float64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_modes[3][6:15][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ace1af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cb89cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2ba4e40d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r2check = torch.zeros(len(meshes),2)\n",
    "for i in range(len(meshes)):\n",
    "    #print(torch.sum(torch.abs(test_input_modes[check][6:15][0]-test_input_modes[i][6:15][0])))\n",
    "    r2check[i]=emulators[i].R2(test_input[check],test_output[check])\n",
    "\n",
    "r2check=r2check.detach().numpy()\n",
    "plt.plot(r2check,'o')\n",
    "plt.legend(('A_TAT','V_TAT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3e1d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "228bcf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(224.9606, dtype=torch.float64)\n",
      "\n",
      "tensor(167.3161, dtype=torch.float64)\n",
      "\n",
      "tensor(203.0570, dtype=torch.float64)\n",
      "\n",
      "tensor(0., dtype=torch.float64)\n",
      "\n",
      "tensor(230.9404, dtype=torch.float64)\n",
      "\n",
      "tensor(241.2827, dtype=torch.float64)\n",
      "\n",
      "tensor(221.6740, dtype=torch.float64)\n",
      "\n",
      "tensor(197.9337, dtype=torch.float64)\n",
      "\n",
      "tensor(257.0497, dtype=torch.float64)\n",
      "\n",
      "tensor(222.4901, dtype=torch.float64)\n",
      "\n",
      "tensor(259.4040, dtype=torch.float64)\n",
      "\n",
      "tensor(257.9458, dtype=torch.float64)\n",
      "\n",
      "tensor(191.6570, dtype=torch.float64)\n",
      "\n",
      "tensor(233.2693, dtype=torch.float64)\n",
      "\n",
      "tensor(210.3870, dtype=torch.float64)\n",
      "\n",
      "tensor(176.0385, dtype=torch.float64)\n",
      "\n",
      "tensor(201.6297, dtype=torch.float64)\n",
      "\n",
      "tensor(227.6242, dtype=torch.float64)\n",
      "\n",
      "tensor(211.0256, dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(meshes)):\n",
    "    print(torch.sum(torch.abs(test_input_modes[3][6:15][0]-test_input_modes[i][6:15][0])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3dbdcc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$R^2$')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(meshes,R2_test.mean(axis=1)[7,:,0].detach().numpy(),fmt='o',yerr=R2_test.std(axis=1)[7,:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2_test.mean(axis=1)[7,:,1].detach().numpy(),fmt='o',yerr=R2_test.std(axis=1)[7,:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d021bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.9974, 0.9951],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.9974, 0.9951],\n",
      "         [0.9972, 0.9958],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.9974, 0.9951],\n",
      "         [0.9972, 0.9958],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.0000, 0.0000]]])\n",
      "0\n",
      "1\n",
      "tensor([[[0.9979, 0.9952],\n",
      "         [0.9975, 0.9956],\n",
      "         [0.9978, 0.9950],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9976, 0.9948]],\n",
      "\n",
      "        [[0.9981, 0.9956],\n",
      "         [0.9978, 0.9955],\n",
      "         [0.9979, 0.9951],\n",
      "         [0.9980, 0.9949],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9978, 0.9952],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9977, 0.9948],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9974, 0.9947]],\n",
      "\n",
      "        [[0.9981, 0.9953],\n",
      "         [0.9977, 0.9957],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9980, 0.9947],\n",
      "         [0.9978, 0.9941]],\n",
      "\n",
      "        [[0.9978, 0.9957],\n",
      "         [0.9978, 0.9961],\n",
      "         [0.9979, 0.9955],\n",
      "         [0.9979, 0.9957],\n",
      "         [0.9979, 0.9957]],\n",
      "\n",
      "        [[0.9977, 0.9951],\n",
      "         [0.9977, 0.9954],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9977, 0.9945],\n",
      "         [0.9975, 0.9941]],\n",
      "\n",
      "        [[0.9980, 0.9949],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9978, 0.9956],\n",
      "         [0.9978, 0.9947],\n",
      "         [0.9979, 0.9945]],\n",
      "\n",
      "        [[0.9978, 0.9945],\n",
      "         [0.9976, 0.9955],\n",
      "         [0.9976, 0.9956],\n",
      "         [0.9979, 0.9950],\n",
      "         [0.9977, 0.9947]],\n",
      "\n",
      "        [[0.9977, 0.9948],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9981, 0.9956],\n",
      "         [0.9977, 0.9946],\n",
      "         [0.9977, 0.9946]],\n",
      "\n",
      "        [[0.9980, 0.9947],\n",
      "         [0.9975, 0.9955],\n",
      "         [0.9978, 0.9959],\n",
      "         [0.9980, 0.9950],\n",
      "         [0.9977, 0.9950]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9976, 0.9948],\n",
      "         [0.9978, 0.9962],\n",
      "         [0.9979, 0.9954],\n",
      "         [0.9979, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9952],\n",
      "         [0.9977, 0.9952],\n",
      "         [0.9977, 0.9964],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9980, 0.9955]],\n",
      "\n",
      "        [[0.9980, 0.9948],\n",
      "         [0.9977, 0.9955],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9981, 0.9952],\n",
      "         [0.9976, 0.9951]],\n",
      "\n",
      "        [[0.9977, 0.9956],\n",
      "         [0.9978, 0.9954],\n",
      "         [0.9979, 0.9960],\n",
      "         [0.9977, 0.9950],\n",
      "         [0.9978, 0.9950]],\n",
      "\n",
      "        [[0.9981, 0.9959],\n",
      "         [0.9978, 0.9958],\n",
      "         [0.9980, 0.9961],\n",
      "         [0.9979, 0.9952],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9979, 0.9964],\n",
      "         [0.9980, 0.9954],\n",
      "         [0.9980, 0.9962],\n",
      "         [0.9976, 0.9957],\n",
      "         [0.9980, 0.9958]],\n",
      "\n",
      "        [[0.9977, 0.9958],\n",
      "         [0.9979, 0.9956],\n",
      "         [0.9975, 0.9953],\n",
      "         [0.9976, 0.9954],\n",
      "         [0.9976, 0.9949]],\n",
      "\n",
      "        [[0.9976, 0.9956],\n",
      "         [0.9977, 0.9953],\n",
      "         [0.9974, 0.9952],\n",
      "         [0.9975, 0.9951],\n",
      "         [0.9981, 0.9951]],\n",
      "\n",
      "        [[0.9974, 0.9952],\n",
      "         [0.9974, 0.9951],\n",
      "         [0.9972, 0.9958],\n",
      "         [0.9978, 0.9952],\n",
      "         [0.9979, 0.9949]]])\n"
     ]
    }
   ],
   "source": [
    "reps = 5\n",
    "\n",
    "R2_test = torch.zeros(len(meshes),reps,2)\n",
    "R2_leftout= torch.zeros(len(meshes),reps,2)\n",
    "for i in range(len(meshes)):\n",
    "    for j in range(reps):\n",
    "        X=torch.cat(train_input_modes[0:i]+train_input_modes[i+1:])[:,0:15]\n",
    "        y=torch.cat(train_output_modes[:i]+train_output_modes[i+1:])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            train_size=800,\n",
    "            random_state=j+1\n",
    "        )\n",
    "        X_test= torch.cat(test_input_modes[0:i]+test_input_modes[i+1:])[:,0:15]\n",
    "        y_test=torch.cat(test_output_modes[:i]+test_output_modes[i+1:])\n",
    "        emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=1000)\n",
    "        \n",
    "        meanR, stdR = emulator.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        R2_test[i,j,:]=meanR\n",
    "        \n",
    "        meanR, stdR=emulator.R2_sample(test_input_modes[i][:,0:15],test_output_modes[i],1000) \n",
    "        R2_leftout[i,j,:] = meanR\n",
    "        print(R2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900, 15])\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9600, 0.9834],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "torch.Size([1800, 15])\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4625552892684937 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.132591724395752 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3149105310440063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1619206666946411 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4455400705337524 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2014954090118408 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2668914794921875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3148146867752075 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.570530652999878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1161659955978394 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0732165575027466 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2088576555252075 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0254359245300293 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7823798656463623 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4442801475524902 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6301296949386597 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.281448245048523 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1493229866027832 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2636395692825317 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6473190784454346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.332882046699524 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.079277515411377 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5930168628692627 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3182892799377441 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6321877241134644 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.903699517250061 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.936606526374817 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.983797550201416 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4051064252853394 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.61078679561615 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4964251518249512 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.532096028327942 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5299042463302612 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7676578760147095 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3296480178833008 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6747504472732544 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.50166654586792 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4260333776474 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4162123203277588 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4662822484970093 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.420390009880066 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4018936157226562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.898108959197998 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.321775197982788 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3911134004592896 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6362839937210083 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3267210721969604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2899376153945923 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5169998407363892 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4665805101394653 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2447763681411743 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5478298664093018 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7467246055603027 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3729939460754395 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8873008489608765 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5388680696487427 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6538021564483643 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8380810022354126 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.47191321849823 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3491942882537842 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5211368799209595 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3823050260543823 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5761760473251343 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1858813762664795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3876798152923584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3156687021255493 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.400022029876709 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4128196239471436 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7509723901748657 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8310847282409668 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.261846661567688 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8687773942947388 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3985919952392578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6744064092636108 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9429470300674438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6357998847961426 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8891105651855469 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2475461959838867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.144946336746216 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4959614276885986 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5753810405731201 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5587964057922363 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.796898365020752 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8383961915969849 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3705317974090576 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1333327293395996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9987274408340454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4007998704910278 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2781435251235962 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6857184171676636 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3569025993347168 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3480974435806274 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1934956312179565 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1144503355026245 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4499258995056152 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6328949928283691 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.945038914680481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4856038093566895 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2291038036346436 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.481848955154419 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3784860372543335 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.50339937210083 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3325891494750977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5475010871887207 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3568294048309326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4583237171173096 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3635421991348267 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2772926092147827 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4573060274124146 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9107714891433716 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.310269832611084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4041587114334106 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6587353944778442 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3707120418548584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8785251379013062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7172088623046875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3394691944122314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3289275169372559 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.392327904701233 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3966524600982666 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5771708488464355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2773733139038086 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4282633066177368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.558749794960022 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2832413911819458 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.897858738899231 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4176381826400757 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4635027647018433 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.589793086051941 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2097134590148926 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6159971952438354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.411194920539856 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5922402143478394 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.520060420036316 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.63733971118927 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4708586931228638 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.372116208076477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4859286546707153 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3939656019210815 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7409358024597168 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3562668561935425 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2659128904342651 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7738391160964966 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2941228151321411 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.576833724975586 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8225234746932983 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3628170490264893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1228914260864258 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2472517490386963 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3493424654006958 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.383994698524475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7185699939727783 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3025702238082886 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7660313844680786 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6145973205566406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2596789598464966 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0149667263031006 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5881400108337402 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5737065076828003 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5121842622756958 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.161685824394226 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1202976703643799 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.444377064704895 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6371185779571533 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.904943823814392 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5699652433395386 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5574885606765747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4072754383087158 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7442044019699097 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5312014818191528 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3654835224151611 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6023997068405151 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.188967227935791 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5306472778320312 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2237547636032104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.157079815864563 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5168406963348389 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5310555696487427 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6026225090026855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6588530540466309 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7063689231872559 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7205817699432373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4621644020080566 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.744606852531433 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.897101879119873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9776701927185059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.041908025741577 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9408611059188843 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7551220655441284 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8979412317276 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1227625608444214 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5100349187850952 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4816908836364746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3439735174179077 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.597388505935669 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5440239906311035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4179881811141968 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3775757551193237 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2448757886886597 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5191341638565063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.896843671798706 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8019013404846191 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4554439783096313 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3961567878723145 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5986206531524658 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4893494844436646 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8999518156051636 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0961798429489136 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6884421110153198 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6303627490997314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2720447778701782 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.044032335281372 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7002476453781128 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5443332195281982 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4740885496139526 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7084442377090454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.040358543395996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1113708019256592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5703147649765015 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.311374306678772 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5733662843704224 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6939235925674438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6929569244384766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8778823614120483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.851263165473938 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7202644348144531 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2999789714813232 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9542025327682495 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4267535209655762 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.340601682662964 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.505669593811035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3197877407073975 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6880491971969604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8936899900436401 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7055026292800903 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.590250015258789 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.355889320373535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8670185804367065 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.762484073638916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7237839698791504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6949782371520996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.944785237312317 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3807411193847656 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5679458379745483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8926440477371216 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9416508674621582 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.114847421646118 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5173771381378174 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.665497899055481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3367526531219482 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9470603466033936 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5380340814590454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4535964727401733 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8614169359207153 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5385489463806152 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1608731746673584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.213977813720703 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9132723808288574 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8014578819274902 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.662862777709961 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.233459234237671 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7273354530334473 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.876763939857483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.091348648071289 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1989588737487793 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7797013521194458 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6999284029006958 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6720402240753174 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0783274173736572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8729878664016724 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6837986707687378 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6590244770050049 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.155486583709717 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.039872884750366 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2042129039764404 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5300227403640747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5921639204025269 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3664950132369995 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5840965509414673 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7458688020706177 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.291245460510254 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8520747423171997 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.791183590888977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.932318091392517 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2083780765533447 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5427311658859253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9736980199813843 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9267691373825073 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1814661026000977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9955815076828003 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4023866653442383 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2858433723449707 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.204343795776367 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0384111404418945 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8752124309539795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1321022510528564 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8701262474060059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6624866724014282 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.344211220741272 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8834941387176514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5699537992477417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9801304340362549 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0810208320617676 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7266944646835327 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4347816705703735 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.440128207206726 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.098736047744751 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0656936168670654 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4803398847579956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8865970373153687 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7230467796325684 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2073858976364136 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3275172710418701 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5832480192184448 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2982559204101562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4225044250488281 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6845874786376953 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5557667016983032 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7235077619552612 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2512767314910889 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5201359987258911 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8127654790878296 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5280593633651733 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7011387348175049 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.109492778778076 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5453342199325562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6668720245361328 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8050510883331299 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1996848583221436 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.509374737739563 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4342191219329834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5636310577392578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4927277565002441 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9934319257736206 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7534053325653076 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3904753923416138 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6023989915847778 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0678133964538574 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.860920786857605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2029318809509277 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4513545036315918 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.780530571937561 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6988216638565063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5973553657531738 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.673784613609314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5315825939178467 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2952407598495483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4129453897476196 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.654703974723816 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.054837226867676 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4436081647872925 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9967708587646484 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7455421686172485 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2106537818908691 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3225958347320557 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.27753746509552 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7525078058242798 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7490779161453247 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9062767028808594 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6134647130966187 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.636654257774353 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4316247701644897 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7101256847381592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5948150157928467 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2280853986740112 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3813326358795166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3547637462615967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4732810258865356 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7237350940704346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.752119779586792 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0564637184143066 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4328805208206177 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0051186084747314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0567514896392822 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3423287868499756 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1261956691741943 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6519602537155151 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4052433967590332 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4557311534881592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5346603393554688 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2721524238586426 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5989402532577515 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8554824590682983 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5119566917419434 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3148772716522217 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9221590757369995 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7464028596878052 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.850676417350769 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7695773839950562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.724771738052368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3066518306732178 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2707083225250244 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6149344444274902 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0587921142578125 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.554145336151123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.864306926727295 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9298774003982544 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.488921880722046 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.914072275161743 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.292527675628662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.013042688369751 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.698734998703003 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.221745491027832 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.598071336746216 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3421051502227783 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7038906812667847 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0244176387786865 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0331199169158936 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8789243698120117 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4280316829681396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3312439918518066 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.877765417098999 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2780416011810303 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.179182529449463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4601852893829346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9552130699157715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7125403881073 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.896360158920288 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.293351888656616 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.319225549697876 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2116448879241943 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.446221113204956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4120090007781982 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.987372636795044 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3452256917953491 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4461055994033813 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0975873470306396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.450713872909546 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0201828479766846 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8658976554870605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5002514123916626 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.11991286277771 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.96168053150177 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.485431432723999 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8789594173431396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2889325618743896 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8974220752716064 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8158985376358032 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8701202869415283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0925283432006836 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1137053966522217 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2128868103027344 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6109790802001953 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5630779266357422 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7400046586990356 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5733810663223267 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6095792055130005 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0884339809417725 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9714452028274536 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5838773250579834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8249999284744263 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3471405506134033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5981570482254028 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0177066326141357 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.162726640701294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.301835536956787 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6894588470458984 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8977749347686768 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7657397985458374 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3139612674713135 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0047414302825928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6817009449005127 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8537694215774536 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9512642621994019 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.880014419555664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0064876079559326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0999491214752197 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6902976036071777 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1809585094451904 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3675389289855957 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.644697427749634 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2905173301696777 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.074939012527466 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.306741952896118 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5544402599334717 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0802175998687744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.916052222251892 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.464897871017456 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5456345081329346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0788304805755615 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1464805603027344 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.827033281326294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3749170303344727 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.172776460647583 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.294766902923584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9408833980560303 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0906982421875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2378876209259033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9029109477996826 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5547550916671753 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2053897380828857 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7397196292877197 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4254095554351807 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.355140447616577 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9347038269042969 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.434032678604126 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.139580726623535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.33015513420105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7241742610931396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.737037420272827 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3995745182037354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2096657752990723 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4620463848114014 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3987646102905273 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.599606513977051 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.150935649871826 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6623629331588745 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3386361598968506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9384584426879883 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0479040145874023 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3568296432495117 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.602842330932617 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.795742988586426 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1101105213165283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.163707971572876 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.325045347213745 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4833812713623047 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2534353733062744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3316755294799805 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3218185901641846 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7958598136901855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3336071968078613 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4695777893066406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2665677070617676 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1840434074401855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.858435034751892 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9180353879928589 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2546160221099854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0397515296936035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8122388124465942 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.251619338989258 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.594313144683838 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.079232931137085 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.534722089767456 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.742008924484253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6786084175109863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.695833683013916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9056334495544434 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4049509763717651 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.88258695602417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.863990306854248 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8193697929382324 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2804698944091797 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4355664253234863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7942185401916504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2363221645355225 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.815860629081726 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4691033363342285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3757197856903076 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7989203929901123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7100467681884766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.073322057723999 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.677842617034912 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9003840684890747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2024991512298584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.446212887763977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0313432216644287 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3210339546203613 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3577566146850586 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9705203771591187 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0455620288848877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4257192611694336 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7795307636260986 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8847030401229858 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7581722736358643 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.228996515274048 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.403557062149048 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.882375717163086 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.997697353363037 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3497955799102783 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.722330093383789 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9656500816345215 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8610584735870361 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1811089515686035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8182048797607422 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7874783277511597 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.90520441532135 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.166551113128662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9379520416259766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5132901668548584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.195101022720337 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.926832675933838 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8688945770263672 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.017758369445801 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7727909088134766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9852045774459839 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8912562131881714 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4869728088378906 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7753167152404785 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1318814754486084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7053563594818115 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8436598777770996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8351248502731323 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5909885168075562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5500988960266113 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.408113956451416 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.005478620529175 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.781924843788147 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0339303016662598 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6089122295379639 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4235520362854004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.519721031188965 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7202484607696533 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7268693447113037 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.48648738861084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8250923156738281 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6814767122268677 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5575684309005737 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8848309516906738 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1248667240142822 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3490827083587646 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8575160503387451 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8022621870040894 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9696682691574097 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4255645275115967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8434855937957764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8330551385879517 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2990193367004395 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5399081707000732 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6215277910232544 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.269151210784912 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.260078191757202 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.730763554573059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6026474237442017 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5414433479309082 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8160518407821655 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.850890040397644 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0329983234405518 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.955670952796936 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8054660558700562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3177716732025146 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.139662742614746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5678484439849854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.251235008239746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3061439990997314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6778782606124878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2668917179107666 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9817239046096802 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.629364490509033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7800383567810059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4897208213806152 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.073235511779785 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6089224815368652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4374990463256836 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.933424472808838 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.986764907836914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.400916814804077 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9636237621307373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5972177982330322 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0201966762542725 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2584002017974854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.433518409729004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2540676593780518 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.04231333732605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9967097043991089 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4666049480438232 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7053508758544922 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2808611392974854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.043732166290283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.790611982345581 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8474361896514893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2433440685272217 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9119805097579956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.085249662399292 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.69035005569458 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.420957326889038 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6597909927368164 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6432651281356812 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.832143783569336 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0376741886138916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4118645191192627 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5904346704483032 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6734650135040283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7829114198684692 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.522078514099121 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.381815195083618 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.054316520690918 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2197425365448 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8561094999313354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4637534618377686 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9412739276885986 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3026556968688965 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.940070390701294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2052927017211914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3269760608673096 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7008895874023438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.030654191970825 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0158421993255615 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9089152812957764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9920296669006348 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7400959730148315 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5689914226531982 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8190934658050537 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.033801794052124 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8970386981964111 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.247685194015503 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9558018445968628 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6696701049804688 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.905513048171997 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5784976482391357 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0165281295776367 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7879875898361206 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8746856451034546 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4507784843444824 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7323001623153687 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2470812797546387 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.499743700027466 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9236668348312378 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9756461381912231 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6174449920654297 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8248251676559448 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7798094749450684 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8107956647872925 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7389734983444214 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5283260345458984 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.139071464538574 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1490933895111084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7103636264801025 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9432501792907715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2880659103393555 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.542941093444824 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6428333520889282 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.155548334121704 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8847945928573608 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.194692850112915 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.677776575088501 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9410099983215332 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8922263383865356 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.644450306892395 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4856088161468506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2182681560516357 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.071150064468384 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2690958976745605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7745438814163208 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2057504653930664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0501973628997803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9423394203186035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7088406085968018 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.764437198638916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.059222936630249 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.793748378753662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.994359016418457 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.581688642501831 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8351094722747803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2480692863464355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.149195671081543 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4554176330566406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9082027673721313 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.711787223815918 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.643360137939453 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5865824222564697 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.188767433166504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6581907272338867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.544426202774048 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.520688772201538 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.537414789199829 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0800249576568604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.072842597961426 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.497920036315918 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.226316213607788 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.098219871520996 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9487435817718506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.799689292907715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.096334218978882 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.012223243713379 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8008711338043213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.559096336364746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.500701904296875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8088645935058594 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2749826908111572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3188464641571045 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2340807914733887 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.768873929977417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9145522117614746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.653366804122925 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5544016361236572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.176614284515381 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7836623191833496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.34299898147583 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2162303924560547 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.843956232070923 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3292717933654785 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0713677406311035 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0415289402008057 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4891111850738525 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.4976866245269775 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.301008701324463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.467606544494629 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8330435752868652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7436232566833496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.556941509246826 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.380281448364258 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9018019437789917 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9571053981781006 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.431123971939087 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8045296669006348 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4583866596221924 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.068295955657959 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3096699714660645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9914391040802 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0417611598968506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.290057897567749 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.85921311378479 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.963194727897644 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.291820049285889 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2845706939697266 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4628093242645264 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2195591926574707 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0125861167907715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.299164056777954 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.440981388092041 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2544667720794678 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9667840003967285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2292115688323975 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6828677654266357 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.414893865585327 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.61568284034729 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.34741473197937 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6444051265716553 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.11551833152771 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.486266851425171 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4189412593841553 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8771464824676514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.292691946029663 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2788736820220947 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1281578540802 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.564653158187866 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2130212783813477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.34871244430542 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.940629005432129 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4060280323028564 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.892001152038574 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.653571844100952 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7330873012542725 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.111645460128784 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3081789016723633 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2953569889068604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0136070251464844 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.771225690841675 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8006844520568848 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6901419162750244 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7725555896759033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8594815731048584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.222510576248169 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0296530723571777 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.011212110519409 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.862474203109741 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.46105694770813 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.968768358230591 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8596785068511963 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2502694129943848 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.285919666290283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7402384281158447 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8706815242767334 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.601966619491577 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1348698139190674 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6034600734710693 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.55915904045105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1642580032348633 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.426215171813965 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.549410820007324 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4337189197540283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2881650924682617 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.4604499340057373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7393381595611572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4818081855773926 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1564416885375977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.608165740966797 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4719066619873047 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7108993530273438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.776047706604004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.558455467224121 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6127068996429443 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.451889991760254 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2079789638519287 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.386016845703125 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.261230707168579 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.963427782058716 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.64528226852417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9679250717163086 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.011821746826172 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7382192611694336 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.187544822692871 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.317514657974243 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.925668239593506 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.127171277999878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.224393606185913 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3949687480926514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7865049839019775 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.451775550842285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1841437816619873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.186021327972412 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1308858394622803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6911799907684326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0771753787994385 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.394824504852295 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5582289695739746 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.518484592437744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8088083267211914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.895681858062744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.998687982559204 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.986786961555481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8138132095336914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9230589866638184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6370255947113037 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3430817127227783 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8755173683166504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.817768096923828 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.045499324798584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6706383228302 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.51175856590271 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8639769554138184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.102179765701294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2304434776306152 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.292673587799072 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.737030267715454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2705423831939697 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.175091505050659 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1288719177246094 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8180861473083496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.668515682220459 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6762328147888184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.541015863418579 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6200242042541504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3986544609069824 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0745694637298584 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5383245944976807 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.011484384536743 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.562319040298462 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7258076667785645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2658445835113525 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9153196811676025 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4457144737243652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.307483673095703 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6215168237686157 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7245798110961914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.303006410598755 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.539292335510254 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.349428653717041 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6527867317199707 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.177349328994751 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.160442590713501 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3627560138702393 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.677031993865967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.078791856765747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7180500030517578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4157955646514893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9791784286499023 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1423134803771973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9938651323318481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.322009801864624 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3615705966949463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3632781505584717 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.353644609451294 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.42568039894104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6160287857055664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.95584774017334 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8231956958770752 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.436344861984253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0113563537597656 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0478310585021973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9814515113830566 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7598652839660645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.046243667602539 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7864456176757812 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.756201982498169 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1398465633392334 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1691324710845947 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5424556732177734 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1179580688476562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5416016578674316 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.578901767730713 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3397390842437744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.907826542854309 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.752246379852295 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.900629997253418 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0484206676483154 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2524824142456055 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7358245849609375 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.108060359954834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.053574562072754 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.809037446975708 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.396759033203125 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6743993759155273 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.443018913269043 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7178714275360107 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.097003936767578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4997775554656982 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.066812753677368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2784838676452637 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1229066848754883 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.782360315322876 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.217041015625 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.506253957748413 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6549172401428223 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2304630279541016 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2890472412109375 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.662309408187866 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2985665798187256 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.405975103378296 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2661402225494385 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.301973581314087 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3553662300109863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7862417697906494 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.316617488861084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2105443477630615 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5944902896881104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6167500019073486 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5476036071777344 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.627467393875122 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.692821741104126 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.018725633621216 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6411924362182617 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.360565185546875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6696062088012695 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4907419681549072 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.561756134033203 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.986454963684082 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8927533626556396 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6295663118362427 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7781074047088623 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6186609268188477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.262583017349243 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.848418951034546 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.457477569580078 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4469799995422363 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.560633659362793 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.493461847305298 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2726356983184814 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1151299476623535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.790008783340454 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.469194173812866 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8983349800109863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0322561264038086 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9216485023498535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5968852043151855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.83754563331604 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.839869499206543 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2556612491607666 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.976689100265503 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7246904373168945 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3762505054473877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1413862705230713 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4684083461761475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.635056257247925 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.695089101791382 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.731376886367798 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.790360927581787 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8393044471740723 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4758636951446533 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7769012451171875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.748302936553955 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0363564491271973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.783245086669922 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2205841541290283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.080169439315796 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.600038528442383 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7179369926452637 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0196235179901123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.955827474594116 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0513391494750977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3456785678863525 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9707884788513184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2239181995391846 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.342350721359253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5173110961914062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6414711475372314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3417367935180664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7581419944763184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.095970392227173 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.432940721511841 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.846996545791626 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.656362771987915 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.801513433456421 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3842687606811523 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.52207088470459 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.9600, 0.9834],\n",
      "         [0.9689, 0.9896],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]])\n",
      "torch.Size([2700, 15])\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2309849262237549 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2950711250305176 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1838092803955078 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2731975317001343 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5795749425888062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.666306972503662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.026660680770874 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.116829752922058 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2325230836868286 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1213454008102417 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.076943039894104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.153770923614502 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2110724449157715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0735396146774292 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5482045412063599 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7343776226043701 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2102493047714233 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.071963906288147 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6434425115585327 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6272257566452026 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2149181365966797 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.783032178878784 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8785187005996704 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6898200511932373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.765656590461731 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9521515369415283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9085124731063843 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1044926643371582 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4637640714645386 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6132495403289795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4789241552352905 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0785044431686401 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.024691104888916 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.375118613243103 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9710991382598877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5679066181182861 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.43466317653656 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.724473714828491 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9751933813095093 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5520577430725098 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5787220001220703 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2622954845428467 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.755782961845398 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6658767461776733 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6041370630264282 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.754699468612671 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0117578506469727 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5352057218551636 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7490806579589844 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1113213300704956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.889215350151062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.568551778793335 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.022585868835449 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0494606494903564 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.696707248687744 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2652459144592285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7401034832000732 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.727709174156189 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0294365882873535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.551837921142578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9193198680877686 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9881092309951782 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.247692823410034 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.87766432762146 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7351943254470825 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0179593563079834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9984891414642334 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9599661827087402 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.904186964035034 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3267688751220703 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.67389714717865 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.253202199935913 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.286790609359741 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1485595703125 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.661716103553772 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.624361276626587 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3210082054138184 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.307438611984253 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.393821954727173 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.827615737915039 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.130066990852356 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3331096172332764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.784684419631958 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.2963454723358154 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.302626848220825 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.694986581802368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7580739259719849 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6189286708831787 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1426336765289307 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.22512149810791 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5306379795074463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.252901315689087 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6071412563323975 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1220510005950928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.140299081802368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.116481304168701 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9248392581939697 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4495954513549805 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7483091354370117 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.285853862762451 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.614727020263672 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.127879619598389 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.091058254241943 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5935680866241455 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6567631959915161 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9871599674224854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.61871075630188 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.712291717529297 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.134185552597046 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2749907970428467 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.442927837371826 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7189371585845947 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.574577808380127 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.4508109092712402 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.916429042816162 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8495118618011475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3209023475646973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.1844096183776855 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6507651805877686 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.005069255828857 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.504822254180908 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.272993803024292 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3343420028686523 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.382354259490967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7332417964935303 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.432473659515381 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3678207397460938 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.608421802520752 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.178816318511963 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6247804164886475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.185932636260986 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.661544322967529 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.369323253631592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.0158467292785645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8820860385894775 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5196797847747803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.72172212600708 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1476309299468994 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7954567670822144 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.793773889541626 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3164050579071045 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.86056661605835 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0053603649139404 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9133951663970947 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.34311056137085 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5289604663848877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.062891006469727 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9832228422164917 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.242844581604004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.157318353652954 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0347795486450195 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.09396505355835 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.506963729858398 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.954483985900879 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3581154346466064 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.155812740325928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.216677665710449 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.418456554412842 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.049587726593018 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0557150840759277 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.466264724731445 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6261439323425293 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7736666202545166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.209377765655518 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.9806365966796875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.597549915313721 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.680464029312134 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.631136894226074 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.562463283538818 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.914478778839111 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.697429180145264 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.344892501831055 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.660362720489502 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.706432342529297 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.296590805053711 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.5997161865234375 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.542601108551025 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.254159927368164 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.621078014373779 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.216159343719482 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.92536735534668 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.566519737243652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.922095775604248 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.983786106109619 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.003537178039551 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.457202434539795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.888575553894043 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.491391181945801 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.768244743347168 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.99308443069458 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.648205757141113 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6428728103637695 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.866603374481201 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.596574783325195 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.407346725463867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.639838218688965 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.3262786865234375 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.849704265594482 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.781339645385742 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.948731422424316 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.437728404998779 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.5317583084106445 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.470001697540283 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.102295398712158 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.208213806152344 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.396842956542969 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.1296796798706055 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.538236618041992 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.091653347015381 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.167872905731201 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.807194232940674 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.2515950202941895 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 9.498760223388672 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.539027690887451 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.50099778175354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6309494972229004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.098584175109863 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9209136962890625 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.761391043663025 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.757007122039795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.377554893493652 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.164789199829102 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.906801223754883 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.700036525726318 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.678764343261719 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.091287136077881 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.173388957977295 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8616530895233154 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6288504600524902 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.383009910583496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.164034843444824 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8259973526000977 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.801673412322998 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.538633346557617 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.8342204093933105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.305229663848877 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.709549427032471 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.946803569793701 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.995602130889893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.574620246887207 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.642133712768555 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.8532843589782715 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1791868209838867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.753501892089844 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.296660900115967 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.350996494293213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.5111753940582275 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.0210418701171875 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.09614086151123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.736504554748535 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.488227367401123 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.545495986938477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.440654277801514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.199103832244873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.22359561920166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.552327632904053 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.005727767944336 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.917771339416504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.6442694664001465 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.778757095336914 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.720926761627197 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.760716438293457 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.776191234588623 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.785745620727539 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.551332473754883 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.448212146759033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.399126052856445 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.737106800079346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.319139003753662 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.011897563934326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.7486555576324463 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.906189441680908 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.10975456237793 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.891479730606079 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.412258625030518 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.253632068634033 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.185446262359619 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.266439437866211 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9195592403411865 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.373198509216309 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.840335845947266 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 9.246081352233887 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.084449768066406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.42322301864624 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.7167649269104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.857380390167236 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.684889316558838 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.592999458312988 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.500860214233398 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.495929718017578 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.176487684249878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.163661479949951 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.3828444480896 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.930462837219238 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3918983936309814 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.725203514099121 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.397834300994873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.481225967407227 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8755853176116943 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.208805561065674 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.387300968170166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.121760845184326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.700952529907227 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.891415596008301 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.307953357696533 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.420260906219482 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.273263454437256 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.305157661437988 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.1279215812683105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.596443176269531 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.5935282707214355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.491422176361084 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.70602035522461 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.860839366912842 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.828268051147461 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.976170063018799 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.39323616027832 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.695310592651367 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.252292633056641 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.902225971221924 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.264398574829102 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.646096229553223 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.745605945587158 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.189659595489502 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 9.946061134338379 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.883677959442139 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 8.960862159729004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.310976028442383 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.523402690887451 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.459255695343018 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.470667839050293 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.587211608886719 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.130735397338867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.448935031890869 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 11.098639488220215 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.861426830291748 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.425339221954346 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.228089809417725 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.690291881561279 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.171877861022949 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.989083766937256 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.207509517669678 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 7.190942287445068 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.413920879364014 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.297163009643555 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.448283672332764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.306084156036377 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.885621547698975 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 6.250957012176514 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.0217673778533936 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.35591721534729 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.141060709953308 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.494790554046631 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4002410173416138 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3050854206085205 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.242073655128479 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8161091804504395 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.05247494578361511 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.07178482413291931 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9.6002e-01,  9.8342e-01],\n",
      "         [ 9.6889e-01,  9.8955e-01],\n",
      "         [-3.4147e+03,  9.1754e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]]])\n",
      "torch.Size([3600, 15])\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4473837614059448 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2865245342254639 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3390322923660278 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1872363090515137 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1952146291732788 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0652501583099365 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.622771143913269 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.099994421005249 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3737345933914185 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3013731241226196 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3417079448699951 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3711512088775635 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2610878944396973 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.132704257965088 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0403733253479004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2472076416015625 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4770561456680298 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3612607717514038 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1568204164505005 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5397361516952515 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.743123173713684 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0718814134597778 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1217446327209473 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7075344324111938 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2259392738342285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5848606824874878 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2400888204574585 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0847450494766235 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1330363750457764 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8984853029251099 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1715736389160156 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.133371114730835 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4255156517028809 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1563903093338013 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9359078407287598 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1589884757995605 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5481549501419067 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1671663522720337 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.173221468925476 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1704187393188477 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.229885458946228 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8002619743347168 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8446277379989624 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1111392974853516 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1553326845169067 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.605444312095642 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.539906620979309 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0279273986816406 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6436419486999512 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1714131832122803 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.108327031135559 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6961476802825928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6335948705673218 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2949358224868774 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6081619262695312 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5703572034835815 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.08478045463562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.295704960823059 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5089889764785767 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4130507707595825 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4591633081436157 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.94231116771698 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.747708797454834 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2979962825775146 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4892874956130981 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4859864711761475 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3100289106369019 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2046977281570435 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3624049425125122 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5958997011184692 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7833678722381592 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3284748792648315 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.059418797492981 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2496211528778076 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3855406045913696 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.045834541320801 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0526455640792847 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3527638912200928 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8751219511032104 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3598792552947998 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.114367961883545 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.316432237625122 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0501558780670166 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.473688006401062 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1967048645019531 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8670876026153564 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4344325065612793 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4423798322677612 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5715560913085938 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9814915657043457 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1193519830703735 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6838321685791016 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3583133220672607 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1681078672409058 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8253183364868164 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2160208225250244 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0316017866134644 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4349652528762817 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6813836097717285 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7093113660812378 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1676528453826904 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4712119102478027 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.251128911972046 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.423264741897583 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3992559909820557 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5790276527404785 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.030466318130493 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5262985229492188 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0258665084838867 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1420862674713135 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6183102130889893 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6266762018203735 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.468433141708374 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.619355320930481 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.462383270263672 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4255238771438599 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.1853926181793213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.580559492111206 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4778335094451904 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8020364046096802 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.933844804763794 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7589561939239502 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5184948444366455 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7374916076660156 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0944862365722656 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4035109281539917 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.184004306793213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.051765203475952 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.235465168952942 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3979605436325073 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2610087394714355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.2752459049224854 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1167893409729004 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6966238021850586 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5018086433410645 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.408137559890747 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.04783034324646 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.8658738136291504 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0267187356948853 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.369513988494873 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.497981548309326 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.4040164947509766 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.8837589025497437 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.293280839920044 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.1148300170898438 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7530570030212402 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.4137489795684814 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.751044511795044 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.464849591255188 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.6426918506622314 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.095278024673462 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9490959644317627 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.959629774093628 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.5148508548736572 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6248879432678223 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.8942296504974365 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.7519357204437256 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.245861053466797 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3817520141601562 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.876579523086548 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.213315963745117 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.300501823425293 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3837822675704956 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.096118450164795 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.620988607406616 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2911187410354614 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.034003734588623 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.948505163192749 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.9580636024475098 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.341200351715088 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.002416610717773 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.7593183517456055 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.082108497619629 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.719164252281189 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.439410448074341 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.3660597801208496 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.9509401321411133 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4994633197784424 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.967935800552368 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.6868059635162354 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.0781168937683105 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 5.593642711639404 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.772334337234497 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9111911058425903 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.707195997238159 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.255224227905273 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.126585841178894 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.955301523208618 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3915233612060547 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 4.128305435180664 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.997175455093384 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 3.591473340988159 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1942700147628784 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.39008548855781555 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.19255974888801575 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.026656869798898697 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 0.18597199022769928 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9.6002e-01,  9.8342e-01],\n",
      "         [ 9.6889e-01,  9.8955e-01],\n",
      "         [-3.4147e+03,  9.1754e-01],\n",
      "         [-2.4586e+00,  9.4299e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]]])\n",
      "torch.Size([4500, 15])\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0556652545928955 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6479984521865845 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2626482248306274 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.0442605018615723 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.0473194122314453 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.2080250978469849 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6533793210983276 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.303242802619934 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6209629774093628 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6045883893966675 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.287673830986023 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3102854490280151 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.7090884447097778 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1183241605758667 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.138137698173523 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6477482318878174 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.9603095054626465 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3433842658996582 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6098716259002686 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3801831007003784 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.643303632736206 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5762584209442139 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5187995433807373 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5667179822921753 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3652621507644653 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.424780011177063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6995060443878174 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4380794763565063 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.419169306755066 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.5265135765075684 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.658369541168213 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.4188203811645508 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.3760501146316528 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.012994408607483 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 2.3309948444366455 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.132585048675537 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.6179360151290894 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 1.1454081535339355 which is larger than the tolerance of 1 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reps = 5\n",
    "\n",
    "R2_test = torch.zeros(len(meshes),reps,2)\n",
    "R2_leftout= torch.zeros(len(meshes),reps,2)\n",
    "for i in range(len(meshes)):\n",
    "    Xs=[]\n",
    "    Ys=[]\n",
    "    for j in range(reps):\n",
    "        for k in range(len(meshes)):\n",
    "            if k!=i:\n",
    "                X=train_input_modes[k][:,0:15]\n",
    "                y=train_output_modes[k]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X,\n",
    "                    y,\n",
    "                    train_size=50,\n",
    "                    random_state=j+k\n",
    "                )\n",
    "                Xs.append(X_train)\n",
    "                Ys.append(y_train)\n",
    "\n",
    "        X_train=torch.cat(Xs,)\n",
    "        y_train=torch.cat(Ys,)\n",
    "        print(X_train.shape)\n",
    "        X_test= torch.cat(test_input_modes[0:i]+test_input_modes[i+1:])[:,0:15]\n",
    "        y_test=torch.cat(test_output_modes[:i]+test_output_modes[i+1:])\n",
    "        emulator=GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=2000)\n",
    "        \n",
    "        meanR, stdR = emulator.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        R2_test[i,j,:]=meanR\n",
    "        \n",
    "        meanR, stdR=emulator.R2_sample(test_input_modes[i][:,0:15],test_output_modes[i],1000) \n",
    "        R2_leftout[i,j,:] = meanR\n",
    "        print(R2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(meshes,R2_test.mean(axis=1)[:,0].detach().numpy(),fmt='o',yerr=R2_test.std(axis=1)[:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2_test.mean(axis=1)[:,1].detach().numpy(),fmt='o',yerr=R2_test.std(axis=1)[:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(meshes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d99a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plt.plot(t_size,R2.mean(axis=1).detach().numpy())\n",
    "plt.errorbar(meshes,R2_leftout.mean(axis=1)[:,0].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=1)[:,0].detach().numpy())\n",
    "plt.errorbar(meshes,R2_leftout.mean(axis=1)[:,1].detach().numpy(),fmt='o',yerr=R2_leftout.std(axis=1)[:,1].detach().numpy())\n",
    "plt.legend(('A_TAT','V_TAT'))\n",
    "plt.xlabel('Mesh')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.legend(['A_TAT','V_TAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array(R2_leftout),axis=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fb7a5",
   "metadata": {},
   "source": [
    "# Discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafeff3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " value = ('likelihood.noise_covar.raw_noise', Parameter containing:\n",
      "tensor([-13.0669], requires_grad=True))\n",
      " value = ('covar_module.raw_outputscale', Parameter containing:\n",
      "tensor(0.4679, requires_grad=True))\n",
      " value = ('covar_module.base_kernel.raw_lengthscale', Parameter containing:\n",
      "tensor([[32.8634, 24.1502, 26.6298,  1.3238,  6.8045,  1.6815]],\n",
      "       requires_grad=True))\n",
      " value = ('mean_module.weights', Parameter containing:\n",
      "tensor([[ 7.6845e-03],\n",
      "        [-5.6172e-03],\n",
      "        [ 1.5326e-03],\n",
      "        [-1.8826e+00],\n",
      "        [-2.5208e-01],\n",
      "        [-6.6706e-01]], requires_grad=True))\n",
      " value = ('mean_module.bias', Parameter containing:\n",
      "tensor([2.4689], requires_grad=True))\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:208: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3618.)\n",
      "  prediction=torch.stack(prediction).T\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  MSE_mean = torch.tensor(MSE_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:225: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  MSE_std = torch.tensor(MSE_score.std(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m\n\u001b[1;32m     40\u001b[0m         m0 \u001b[38;5;241m=\u001b[39m emulators[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(X_train[a,:])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#         y_adjust = torch.tensor(y_train[a] - m0)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#         delta_1 = GPE.ensemble(X_train[a,:],y_adjust,mean_func=\"linear\",training_iter=500)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#         MSE[i,j] += ((emulator_0.predict(X_test)+delta_1.predict(X_test)-torch.tensor(y_test))**2).mean(axis=0).detach().numpy()\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#         R2[i,j] += (1-((emulator_0.predict(X_test)+delta_1.predict(X_test)-y_test)**2).mean(axis=0)/y_test.var(axis=0)).detach().numpy()\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m         delta_1\u001b[38;5;241m=\u001b[39mGPE\u001b[38;5;241m.\u001b[39mensemble(X_train[a,:],y_train[a],mean_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrepancy_cohort\u001b[39m\u001b[38;5;124m\"\u001b[39m,training_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,ref_emulator\u001b[38;5;241m=\u001b[39memulators[\u001b[38;5;241m1\u001b[39m:],a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m         MSEaM,MSEaSTD \u001b[38;5;241m=\u001b[39m delta_1\u001b[38;5;241m.\u001b[39mMSE_sample(X_test,y_test,\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     49\u001b[0m         R2aM,R2aSTD \u001b[38;5;241m=\u001b[39m delta_1\u001b[38;5;241m.\u001b[39mR2_sample(X_test,y_test,\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:22\u001b[0m, in \u001b[0;36mensemble.__init__\u001b[0;34m(self, X_train, y_train, mean_func, training_iter, kernel, kernel_params, ref_emulator, a, a_indicator)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m=\u001b[39mkernel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_params\u001b[38;5;241m=\u001b[39mkernel_params\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_ensemble()\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:105\u001b[0m, in \u001b[0;36mensemble.create_ensemble\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Output from model\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m models[i](X)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, Y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:268\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    265\u001b[0m             torch\u001b[38;5;241m.\u001b[39mequal(train_input, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m train_input, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m length_safe_zip(train_inputs, inputs)\n\u001b[1;32m    266\u001b[0m         ):\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must train on the training inputs!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Prior mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GP_functions.py:70\u001b[0m, in \u001b[0;36mExactGPModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 70\u001b[0m     mean_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_module(x)\n\u001b[1;32m     71\u001b[0m     covar_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module(x)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpytorch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mMultivariateNormal(mean_x, covar_x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/means/mean.py:22\u001b[0m, in \u001b[0;36mMean.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(Mean, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GP_functions.py:149\u001b[0m, in \u001b[0;36mDiscrepancyMeanCohort.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model[i]\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_likelihood[i]\u001b[38;5;241m.\u001b[39meval() \n\u001b[0;32m--> 149\u001b[0m     res2\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_likelihood[i](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_model[i](x))\u001b[38;5;241m.\u001b[39mmean\n\u001b[1;32m    151\u001b[0m res\u001b[38;5;241m=\u001b[39mres1\u001b[38;5;241m+\u001b[39mres2\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[1;32m    330\u001b[0m     (\n\u001b[1;32m    331\u001b[0m         predictive_mean,\n\u001b[1;32m    332\u001b[0m         predictive_covar,\n\u001b[0;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_strategy\u001b[38;5;241m.\u001b[39mexact_prediction(full_mean, full_covar)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[1;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:290\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[0;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[1;32m    285\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[1;32m    286\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m joint_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_mean(test_mean, test_train_covar),\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexact_predictive_covar(test_test_covar, test_train_covar),\n\u001b[1;32m    291\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/models/exact_prediction_strategies.py:342\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_predictive_covar\u001b[0;34m(self, test_test_covar, test_train_covar)\u001b[0m\n\u001b[1;32m    340\u001b[0m test_train_covar \u001b[38;5;241m=\u001b[39m to_dense(test_train_covar)\n\u001b[1;32m    341\u001b[0m train_test_covar \u001b[38;5;241m=\u001b[39m test_train_covar\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m covar_correction_rhs \u001b[38;5;241m=\u001b[39m train_train_covar\u001b[38;5;241m.\u001b[39msolve(train_test_covar)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# For efficiency\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(test_test_covar):\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# We can use addmm in the 2d case\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2334\u001b[0m, in \u001b[0;36mLinearOperator.solve\u001b[0;34m(self, right_tensor, left_tensor)\u001b[0m\n\u001b[1;32m   2332\u001b[0m func \u001b[38;5;241m=\u001b[39m Solve\n\u001b[1;32m   2333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(), \u001b[38;5;28;01mFalse\u001b[39;00m, right_tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(),\n\u001b[1;32m   2338\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2341\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation(),\n\u001b[1;32m   2342\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/functions/_solve.py:53\u001b[0m, in \u001b[0;36mSolve.forward\u001b[0;34m(ctx, representation_tree, has_left, *args)\u001b[0m\n\u001b[1;32m     51\u001b[0m     res \u001b[38;5;241m=\u001b[39m left_tensor \u001b[38;5;241m@\u001b[39m res\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     solves \u001b[38;5;241m=\u001b[39m _solve(linear_op, right_tensor)\n\u001b[1;32m     54\u001b[0m     res \u001b[38;5;241m=\u001b[39m solves\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mis_vector:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/functions/_solve.py:17\u001b[0m, in \u001b[0;36m_solve\u001b[0;34m(linear_op, rhs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39msolve(rhs)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfast_computations\u001b[38;5;241m.\u001b[39msolves\u001b[38;5;241m.\u001b[39moff() \u001b[38;5;129;01mor\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mmax_cholesky_size\u001b[38;5;241m.\u001b[39mvalue():\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39mcholesky()\u001b[38;5;241m.\u001b[39m_cholesky_solve(rhs)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:1303\u001b[0m, in \u001b[0;36mLinearOperator.cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;129m@_implements\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky)\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cholesky(upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m   1305\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:522\u001b[0m, in \u001b[0;36mLinearOperator._cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(evaluated_mat\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# contiguous call is necessary here\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m cholesky \u001b[38;5;241m=\u001b[39m psd_safe_cholesky(evaluated_mat, upper\u001b[38;5;241m=\u001b[39mupper)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(cholesky, upper\u001b[38;5;241m=\u001b[39mupper)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:65\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpsd_safe_cholesky\u001b[39m(A, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     L \u001b[38;5;241m=\u001b[39m _psd_safe_cholesky(A, out\u001b[38;5;241m=\u001b[39mout, jitter\u001b[38;5;241m=\u001b[39mjitter, max_tries\u001b[38;5;241m=\u001b[39mmax_tries)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:20\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m (out, torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32, device\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m---> 20\u001b[0m L, info \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky_ex(A, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mtrace_mode\u001b[38;5;241m.\u001b[39mon() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(info):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m L\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# x_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/input/xlabels_EP.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "# y_labels=pd.read_csv(r'/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/output/ylabels.txt',delim_whitespace=True,header=None)\n",
    "\n",
    "# y_labels\n",
    "\n",
    "# inputData_0 = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/01/X_EP.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "# outputData_0 = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/01/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "\n",
    "# inputData_1 = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/02/X_EP.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "# outputData_1 = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/EP_healthy/02/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "\n",
    "X0 = train_input[0]\n",
    "Y0 = train_output[0]\n",
    "X1 = train_input[1]\n",
    "Y1 = train_output[1]\n",
    "\n",
    "\n",
    "for param in emulators[0].models[0].named_parameters():\n",
    "    print(f' value = {param}')\n",
    "\n",
    "# split original dataset in training, validation and testing sets\n",
    "X_train=X0\n",
    "y_train=Y0\n",
    "X_test=test_input[0]\n",
    "y_test=test_output[0]\n",
    "\n",
    "p = int(X1.shape[0]*0.05)\n",
    "n = int(X_train.shape[0]/p)\n",
    "reps = 5\n",
    "MSE = torch.zeros((n,reps,2))\n",
    "R2 = torch.zeros((n,reps,2))\n",
    "MSE_p = torch.zeros((n,reps,2))\n",
    "R2_p = torch.zeros((n,reps,2))\n",
    "MSEa = torch.zeros((n,reps,2))\n",
    "R2a = torch.zeros((n,reps,2))\n",
    "for i in range(n):\n",
    "    for j in range(reps):\n",
    "        a=np.random.choice(range(X_train.shape[0]),(i+1)*p,replace=False)\n",
    "        m0 = emulators[0].predict(X_train[a,:])\n",
    "#         y_adjust = torch.tensor(y_train[a] - m0)\n",
    "#         delta_1 = GPE.ensemble(X_train[a,:],y_adjust,mean_func=\"linear\",training_iter=500)\n",
    "#         MSE[i,j] += ((emulator_0.predict(X_test)+delta_1.predict(X_test)-torch.tensor(y_test))**2).mean(axis=0).detach().numpy()\n",
    "#         R2[i,j] += (1-((emulator_0.predict(X_test)+delta_1.predict(X_test)-y_test)**2).mean(axis=0)/y_test.var(axis=0)).detach().numpy()\n",
    "        \n",
    "        \n",
    "        delta_1=GPE.ensemble(X_train[a,:],y_train[a],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators[1:],a=None)\n",
    "        MSEaM,MSEaSTD = delta_1.MSE_sample(X_test,y_test,1000)\n",
    "        R2aM,R2aSTD = delta_1.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        MSE[i,j] += MSEaM\n",
    "        R2[i,j] += R2aM\n",
    "        \n",
    "        \n",
    "        delta_1=GPE.ensemble(X_train[a,:],y_train[a],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators[1]],a=None)\n",
    "        MSEaM,MSEaSTD = delta_1.MSE_sample(X_test,y_test,1000)\n",
    "        R2aM,R2aSTD = delta_1.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        MSEa[i,j] += MSEaM\n",
    "        R2a[i,j] += R2aM\n",
    "        \n",
    "        delta_1p = GPE.ensemble(X_train[a,:],y_train[a],mean_func=\"linear\",training_iter=500)\n",
    "        MSEaM,MSEaSTD = delta_1p.MSE_sample(X_test,y_test,1000)\n",
    "        R2aM,R2aSTD = delta_1p.R2_sample(X_test,y_test,1000)\n",
    "        \n",
    "        MSE_p[i,j] += MSEaM\n",
    "        R2_p[i,j] += R2aM\n",
    "\n",
    "x = np.linspace(9,162,18)\n",
    "\n",
    "x\n",
    "\n",
    "MSE.mean(axis=1)[0]\n",
    "\n",
    "MSE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed33931",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(7,144,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afc7fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$m$')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,MSE.mean(axis=1))\n",
    "plt.fill_between(x, MSE.mean(axis=1)[:,0]+MSE.std(axis=1)[:,0], y2=MSE.mean(axis=1)[:,0]-MSE.std(axis=1)[:,0],alpha=0.4)\n",
    "plt.fill_between(x, MSE.mean(axis=1)[:,1]+MSE.std(axis=1)[:,1], y2=MSE.mean(axis=1)[:,1]-MSE.std(axis=1)[:,1],alpha=0.4)\n",
    "plt.legend(y_labels.values)\n",
    "\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "#plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea26be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0053e+03, 4.4139e+02],\n",
       "         [3.5017e+03, 9.0324e+02],\n",
       "         [1.2513e+03, 1.5707e+02],\n",
       "         [1.2043e+05, 1.0591e+04],\n",
       "         [1.7407e+03, 9.0006e+01]],\n",
       "\n",
       "        [[2.0532e+02, 1.6733e+02],\n",
       "         [5.4058e+02, 7.5744e+01],\n",
       "         [7.2628e+02, 8.4713e+01],\n",
       "         [8.1053e+02, 1.7521e+02],\n",
       "         [1.7129e+03, 7.9795e+01]],\n",
       "\n",
       "        [[7.4946e+02, 1.7684e+02],\n",
       "         [1.1037e+02, 4.9126e+01],\n",
       "         [1.5816e+03, 7.4548e+01],\n",
       "         [4.3494e+01, 9.8884e+01],\n",
       "         [6.6827e+01, 1.4606e+02]],\n",
       "\n",
       "        [[8.6498e+01, 7.5162e+01],\n",
       "         [8.9355e+01, 7.4566e+01],\n",
       "         [5.1005e+01, 2.8990e+01],\n",
       "         [1.2719e+02, 3.4361e+01],\n",
       "         [8.9162e+01, 3.0088e+01]],\n",
       "\n",
       "        [[1.2944e+02, 1.7104e+01],\n",
       "         [5.1697e+01, 1.3738e+01],\n",
       "         [3.8570e+01, 3.5743e+01],\n",
       "         [3.3745e+01, 1.3179e+01],\n",
       "         [7.2499e+01, 2.7964e+01]],\n",
       "\n",
       "        [[2.1092e+01, 1.8025e+01],\n",
       "         [3.5971e+01, 1.0903e+01],\n",
       "         [3.7181e+01, 1.3848e+01],\n",
       "         [1.4264e+01, 1.1445e+01],\n",
       "         [2.9713e+01, 1.0910e+01]],\n",
       "\n",
       "        [[2.7014e+01, 2.3552e+01],\n",
       "         [1.6403e+01, 9.3167e+00],\n",
       "         [1.6788e+01, 1.9167e+01],\n",
       "         [2.3071e+01, 1.2372e+01],\n",
       "         [2.5421e+01, 1.4559e+01]],\n",
       "\n",
       "        [[2.6143e+01, 5.5196e+00],\n",
       "         [1.8508e+01, 1.2842e+01],\n",
       "         [1.2904e+01, 5.3217e+00],\n",
       "         [3.1449e+01, 5.6140e+00],\n",
       "         [1.4481e+01, 2.4161e+01]],\n",
       "\n",
       "        [[1.2947e+01, 4.7685e+00],\n",
       "         [3.1480e+01, 3.4169e+00],\n",
       "         [1.8848e+01, 4.5880e+00],\n",
       "         [1.1944e+01, 8.9958e+00],\n",
       "         [2.7563e+01, 5.4206e+00]],\n",
       "\n",
       "        [[7.1000e+00, 4.4458e+00],\n",
       "         [1.6906e+01, 4.9741e+00],\n",
       "         [8.7950e+00, 6.8949e+00],\n",
       "         [1.9972e+01, 1.0886e+01],\n",
       "         [9.1423e+00, 3.6059e+00]],\n",
       "\n",
       "        [[1.3355e+01, 3.0664e+00],\n",
       "         [9.4912e+00, 3.7495e+00],\n",
       "         [9.7016e+00, 2.8478e+00],\n",
       "         [1.5395e+01, 4.4468e+00],\n",
       "         [2.3764e+01, 3.3102e+00]],\n",
       "\n",
       "        [[1.0877e+01, 3.3919e+00],\n",
       "         [6.7196e+00, 4.5628e+00],\n",
       "         [8.2384e+00, 2.3935e+00],\n",
       "         [1.0873e+01, 6.3427e+00],\n",
       "         [7.3827e+00, 2.6047e+00]],\n",
       "\n",
       "        [[1.2919e+01, 2.7290e+00],\n",
       "         [7.4372e+00, 3.6573e+00],\n",
       "         [6.3350e+00, 2.9353e+00],\n",
       "         [5.5454e+00, 3.9927e+00],\n",
       "         [9.6886e+00, 4.3525e+00]],\n",
       "\n",
       "        [[1.3912e+01, 3.7539e+00],\n",
       "         [1.3012e+01, 2.8719e+00],\n",
       "         [7.2153e+00, 2.5166e+00],\n",
       "         [9.2395e+00, 2.5724e+00],\n",
       "         [1.9121e+01, 3.2068e+00]],\n",
       "\n",
       "        [[7.3895e+00, 2.9830e+00],\n",
       "         [5.5064e+00, 3.5055e+00],\n",
       "         [6.0174e+00, 2.3664e+00],\n",
       "         [7.3022e+00, 2.6336e+00],\n",
       "         [6.7025e+00, 2.3824e+00]],\n",
       "\n",
       "        [[7.3279e+00, 2.4586e+00],\n",
       "         [7.5969e+00, 1.9485e+00],\n",
       "         [7.8020e+00, 2.9870e+00],\n",
       "         [9.5943e+00, 2.3441e+00],\n",
       "         [4.9980e+00, 2.3096e+00]],\n",
       "\n",
       "        [[5.9309e+00, 2.1835e+00],\n",
       "         [4.1038e+00, 2.6204e+00],\n",
       "         [6.6806e+00, 3.0402e+00],\n",
       "         [6.9768e+00, 2.1905e+00],\n",
       "         [4.1995e+00, 2.1789e+00]],\n",
       "\n",
       "        [[4.6404e+00, 2.1350e+00],\n",
       "         [6.4411e+00, 2.3611e+00],\n",
       "         [4.5094e+00, 2.1972e+00],\n",
       "         [6.5910e+00, 1.6137e+00],\n",
       "         [5.6537e+00, 1.8290e+00]],\n",
       "\n",
       "        [[4.8037e+00, 1.8246e+00],\n",
       "         [5.4191e+00, 2.0841e+00],\n",
       "         [4.7112e+00, 3.1480e+00],\n",
       "         [4.8532e+00, 2.3028e+00],\n",
       "         [5.5880e+00, 1.9339e+00]],\n",
       "\n",
       "        [[5.0640e+00, 1.7976e+00],\n",
       "         [4.6904e+00, 1.9077e+00],\n",
       "         [5.0408e+00, 2.0327e+00],\n",
       "         [4.7101e+00, 1.7274e+00],\n",
       "         [6.1555e+00, 1.9489e+00]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MSE.mean(axis=1)[:,0]+MSE.std(axis=1)[:,0]\n",
    "\n",
    "MSE.mean(axis=1)[:,0]-MSE.std(axis=1)[:,0]\n",
    "\n",
    "delta_1p.MSE(X_test,y_test)/reps\n",
    "\n",
    "plt.plot(x,MSE.mean(axis=1),'o') \n",
    "plt.legend(y_labels.values)\n",
    "\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "plt.yscale('log')\n",
    "\n",
    "np.hstack((MSE_p,MSE))\n",
    "\n",
    "MSE_p.mean(axis=1)[:,0]+MSE_p.std(axis=1)[:,0]\n",
    "\n",
    "MSE_p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e880643a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.3029e+04, 4.5698e+03],\n",
       "        [5.6121e+02, 5.0124e+01],\n",
       "        [6.6699e+02, 5.2082e+01],\n",
       "        [2.6961e+01, 2.4030e+01],\n",
       "        [3.8924e+01, 9.9179e+00],\n",
       "        [9.8304e+00, 3.0475e+00],\n",
       "        [4.9026e+00, 5.6294e+00],\n",
       "        [7.8964e+00, 8.1764e+00],\n",
       "        [8.7092e+00, 2.1163e+00],\n",
       "        [5.6862e+00, 2.9046e+00],\n",
       "        [5.8292e+00, 6.3382e-01],\n",
       "        [1.9533e+00, 1.6281e+00],\n",
       "        [2.9756e+00, 6.8957e-01],\n",
       "        [4.6004e+00, 5.1059e-01],\n",
       "        [8.1565e-01, 4.7906e-01],\n",
       "        [1.6409e+00, 3.7506e-01],\n",
       "        [1.3574e+00, 3.8370e-01],\n",
       "        [9.7432e-01, 3.0096e-01],\n",
       "        [3.9897e-01, 5.2850e-01],\n",
       "        [5.9862e-01, 1.2132e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_p.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c80519db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 237.9059, -156.6279,   61.6800,   26.2652,   17.8138,   16.8370,\n",
       "          12.8005,   11.8472,    6.6968,    8.5121,    6.8649,    5.4093,\n",
       "           7.8996,    5.7679,    5.8229,    4.2209,    4.5928,    4.6761,\n",
       "           4.5335])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_p.mean(axis=1)[1:,0]-MSE_p.std(axis=1)[1:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a8961da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x[1:],np.hstack((MSE.mean(axis=1),MSE_p.mean(axis=1),MSEa.mean(axis=1)))[1:,[0,2,4]],'o') \n",
    "plt.fill_between(x[1:], MSE.mean(axis=1)[1:,0]+MSE.std(axis=1)[1:,0], y2=MSE.mean(axis=1)[1:,0]-MSE.std(axis=1)[1:,0],alpha=0.4)\n",
    "plt.fill_between(x[1:], MSE_p.mean(axis=1)[1:,0]+MSE_p.std(axis=1)[1:,0], y2=MSE_p.mean(axis=1)[1:,0]-MSE_p.std(axis=1)[1:,0],alpha=0.4)\n",
    "plt.fill_between(x[1:], MSEa.mean(axis=1)[1:,0]+MSEa.std(axis=1)[1:,0], y2=MSEa.mean(axis=1)[1:,0]-MSEa.std(axis=1)[1:,0],alpha=0.4)\n",
    "plt.legend(['$\\delta_a$','$f_1$','$\\delta_1$'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "plt.yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d559838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.7694,   0.3104],\n",
       "         [  0.1963,  -0.4105],\n",
       "         [  0.7127,   0.7558],\n",
       "         [-26.6294, -15.5394],\n",
       "         [  0.6007,   0.8595]],\n",
       "\n",
       "        [[  0.9535,   0.7383],\n",
       "         [  0.8762,   0.8811],\n",
       "         [  0.8316,   0.8678],\n",
       "         [  0.8151,   0.7263],\n",
       "         [  0.6067,   0.8759]],\n",
       "\n",
       "        [[  0.8289,   0.7240],\n",
       "         [  0.9748,   0.9236],\n",
       "         [  0.6373,   0.8837],\n",
       "         [  0.9901,   0.8446],\n",
       "         [  0.9848,   0.7714]],\n",
       "\n",
       "        [[  0.9801,   0.8822],\n",
       "         [  0.9796,   0.8842],\n",
       "         [  0.9883,   0.9546],\n",
       "         [  0.9707,   0.9463],\n",
       "         [  0.9796,   0.9531]],\n",
       "\n",
       "        [[  0.9703,   0.9735],\n",
       "         [  0.9881,   0.9782],\n",
       "         [  0.9908,   0.9448],\n",
       "         [  0.9922,   0.9793],\n",
       "         [  0.9829,   0.9563]],\n",
       "\n",
       "        [[  0.9952,   0.9719],\n",
       "         [  0.9916,   0.9827],\n",
       "         [  0.9916,   0.9781],\n",
       "         [  0.9968,   0.9821],\n",
       "         [  0.9932,   0.9830]],\n",
       "\n",
       "        [[  0.9938,   0.9632],\n",
       "         [  0.9962,   0.9854],\n",
       "         [  0.9961,   0.9700],\n",
       "         [  0.9948,   0.9808],\n",
       "         [  0.9942,   0.9772]],\n",
       "\n",
       "        [[  0.9939,   0.9913],\n",
       "         [  0.9957,   0.9801],\n",
       "         [  0.9970,   0.9916],\n",
       "         [  0.9928,   0.9913],\n",
       "         [  0.9967,   0.9621]],\n",
       "\n",
       "        [[  0.9971,   0.9925],\n",
       "         [  0.9928,   0.9947],\n",
       "         [  0.9957,   0.9927],\n",
       "         [  0.9972,   0.9860],\n",
       "         [  0.9937,   0.9916]],\n",
       "\n",
       "        [[  0.9983,   0.9930],\n",
       "         [  0.9961,   0.9923],\n",
       "         [  0.9980,   0.9894],\n",
       "         [  0.9956,   0.9827],\n",
       "         [  0.9979,   0.9944]],\n",
       "\n",
       "        [[  0.9969,   0.9951],\n",
       "         [  0.9978,   0.9941],\n",
       "         [  0.9978,   0.9956],\n",
       "         [  0.9964,   0.9930],\n",
       "         [  0.9945,   0.9949]],\n",
       "\n",
       "        [[  0.9975,   0.9946],\n",
       "         [  0.9985,   0.9929],\n",
       "         [  0.9981,   0.9963],\n",
       "         [  0.9975,   0.9898],\n",
       "         [  0.9983,   0.9959]],\n",
       "\n",
       "        [[  0.9971,   0.9957],\n",
       "         [  0.9983,   0.9943],\n",
       "         [  0.9985,   0.9953],\n",
       "         [  0.9987,   0.9938],\n",
       "         [  0.9978,   0.9932]],\n",
       "\n",
       "        [[  0.9969,   0.9942],\n",
       "         [  0.9971,   0.9956],\n",
       "         [  0.9983,   0.9960],\n",
       "         [  0.9979,   0.9960],\n",
       "         [  0.9956,   0.9950]],\n",
       "\n",
       "        [[  0.9983,   0.9953],\n",
       "         [  0.9987,   0.9944],\n",
       "         [  0.9986,   0.9962],\n",
       "         [  0.9983,   0.9958],\n",
       "         [  0.9985,   0.9964]],\n",
       "\n",
       "        [[  0.9982,   0.9962],\n",
       "         [  0.9982,   0.9969],\n",
       "         [  0.9982,   0.9954],\n",
       "         [  0.9978,   0.9963],\n",
       "         [  0.9989,   0.9964]],\n",
       "\n",
       "        [[  0.9986,   0.9965],\n",
       "         [  0.9991,   0.9960],\n",
       "         [  0.9985,   0.9953],\n",
       "         [  0.9984,   0.9966],\n",
       "         [  0.9990,   0.9966]],\n",
       "\n",
       "        [[  0.9989,   0.9967],\n",
       "         [  0.9985,   0.9963],\n",
       "         [  0.9989,   0.9965],\n",
       "         [  0.9985,   0.9974],\n",
       "         [  0.9987,   0.9971]],\n",
       "\n",
       "        [[  0.9989,   0.9972],\n",
       "         [  0.9988,   0.9968],\n",
       "         [  0.9989,   0.9952],\n",
       "         [  0.9989,   0.9965],\n",
       "         [  0.9987,   0.9970]],\n",
       "\n",
       "        [[  0.9988,   0.9972],\n",
       "         [  0.9989,   0.9970],\n",
       "         [  0.9989,   0.9968],\n",
       "         [  0.9989,   0.9973],\n",
       "         [  0.9986,   0.9970]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cddf2fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4280, -1.6653],\n",
       "         [-0.7949,  0.6093],\n",
       "         [-0.1380,  0.7408],\n",
       "         [-1.1618,  0.0952],\n",
       "         [ 0.3337,  0.8395]],\n",
       "\n",
       "        [[ 0.9749,  0.9596],\n",
       "         [ 0.9832,  0.8988],\n",
       "         [ 0.9810,  0.8144],\n",
       "         [ 0.9646,  0.8687],\n",
       "         [ 0.9875,  0.9861]],\n",
       "\n",
       "        [[ 0.9933,  0.8202],\n",
       "         [ 0.9923,  0.9873],\n",
       "         [ 0.9960,  0.9785],\n",
       "         [ 0.9487,  0.9942],\n",
       "         [ 0.9690,  0.9152]],\n",
       "\n",
       "        [[ 0.9964,  0.9930],\n",
       "         [ 0.9907,  0.9944],\n",
       "         [ 0.9851,  0.9883],\n",
       "         [ 0.9715,  0.9693],\n",
       "         [ 0.9898,  0.9788]],\n",
       "\n",
       "        [[ 0.9909,  0.9945],\n",
       "         [ 0.9961,  0.9954],\n",
       "         [ 0.9930,  0.9961],\n",
       "         [ 0.9962,  0.9943],\n",
       "         [ 0.9948,  0.9934]],\n",
       "\n",
       "        [[ 0.9958,  0.9891],\n",
       "         [ 0.9966,  0.9960],\n",
       "         [ 0.9975,  0.9949],\n",
       "         [ 0.9989,  0.9959],\n",
       "         [ 0.9940,  0.9947]],\n",
       "\n",
       "        [[ 0.9948,  0.9943],\n",
       "         [ 0.9982,  0.9897],\n",
       "         [ 0.9977,  0.9972],\n",
       "         [ 0.9965,  0.9952],\n",
       "         [ 0.9976,  0.9974]],\n",
       "\n",
       "        [[ 0.9956,  0.9946],\n",
       "         [ 0.9967,  0.9963],\n",
       "         [ 0.9984,  0.9961],\n",
       "         [ 0.9966,  0.9966],\n",
       "         [ 0.9974,  0.9933]],\n",
       "\n",
       "        [[ 0.9979,  0.9963],\n",
       "         [ 0.9958,  0.9981],\n",
       "         [ 0.9964,  0.9954],\n",
       "         [ 0.9977,  0.9976],\n",
       "         [ 0.9972,  0.9969]],\n",
       "\n",
       "        [[ 0.9973,  0.9955],\n",
       "         [ 0.9973,  0.9965],\n",
       "         [ 0.9984,  0.9969],\n",
       "         [ 0.9966,  0.9955],\n",
       "         [ 0.9977,  0.9954]],\n",
       "\n",
       "        [[ 0.9973,  0.9967],\n",
       "         [ 0.9978,  0.9957],\n",
       "         [ 0.9977,  0.9956],\n",
       "         [ 0.9978,  0.9960],\n",
       "         [ 0.9970,  0.9964]],\n",
       "\n",
       "        [[ 0.9977,  0.9977],\n",
       "         [ 0.9984,  0.9978],\n",
       "         [ 0.9982,  0.9969],\n",
       "         [ 0.9980,  0.9975],\n",
       "         [ 0.9979,  0.9962]],\n",
       "\n",
       "        [[ 0.9984,  0.9980],\n",
       "         [ 0.9984,  0.9968],\n",
       "         [ 0.9981,  0.9975],\n",
       "         [ 0.9982,  0.9962],\n",
       "         [ 0.9976,  0.9953]],\n",
       "\n",
       "        [[ 0.9977,  0.9969],\n",
       "         [ 0.9976,  0.9974],\n",
       "         [ 0.9983,  0.9979],\n",
       "         [ 0.9982,  0.9980],\n",
       "         [ 0.9979,  0.9977]],\n",
       "\n",
       "        [[ 0.9981,  0.9962],\n",
       "         [ 0.9981,  0.9977],\n",
       "         [ 0.9983,  0.9978],\n",
       "         [ 0.9984,  0.9974],\n",
       "         [ 0.9982,  0.9980]],\n",
       "\n",
       "        [[ 0.9981,  0.9980],\n",
       "         [ 0.9980,  0.9978],\n",
       "         [ 0.9983,  0.9976],\n",
       "         [ 0.9982,  0.9980],\n",
       "         [ 0.9982,  0.9982]],\n",
       "\n",
       "        [[ 0.9980,  0.9971],\n",
       "         [ 0.9988,  0.9980],\n",
       "         [ 0.9979,  0.9971],\n",
       "         [ 0.9981,  0.9977],\n",
       "         [ 0.9985,  0.9982]],\n",
       "\n",
       "        [[ 0.9984,  0.9981],\n",
       "         [ 0.9983,  0.9983],\n",
       "         [ 0.9984,  0.9980],\n",
       "         [ 0.9984,  0.9980],\n",
       "         [ 0.9984,  0.9983]],\n",
       "\n",
       "        [[ 0.9984,  0.9981],\n",
       "         [ 0.9981,  0.9980],\n",
       "         [ 0.9984,  0.9972],\n",
       "         [ 0.9983,  0.9980],\n",
       "         [ 0.9983,  0.9981]],\n",
       "\n",
       "        [[ 0.9984,  0.9981],\n",
       "         [ 0.9985,  0.9983],\n",
       "         [ 0.9982,  0.9982],\n",
       "         [ 0.9984,  0.9983],\n",
       "         [ 0.9984,  0.9981]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ff6e831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.7299, -13.4359],\n",
       "         [ -0.4653,   0.3035],\n",
       "         [ -0.9375,  -1.4426],\n",
       "         [  0.7532,  -1.3478],\n",
       "         [  0.7942,   0.5478]],\n",
       "\n",
       "        [[  0.9748,   0.8843],\n",
       "         [  0.9621,   0.5900],\n",
       "         [  0.8393,   0.9504],\n",
       "         [  0.9729,   0.9679],\n",
       "         [  0.9557,   0.9612]],\n",
       "\n",
       "        [[  0.9910,   0.9662],\n",
       "         [  0.9803,   0.9930],\n",
       "         [  0.9909,   0.9862],\n",
       "         [  0.9709,   0.9916],\n",
       "         [  0.9886,   0.9889]],\n",
       "\n",
       "        [[  0.9972,   0.9931],\n",
       "         [  0.9828,   0.9924],\n",
       "         [  0.9957,   0.9945],\n",
       "         [  0.9942,   0.9855],\n",
       "         [  0.9826,   0.9708]],\n",
       "\n",
       "        [[  0.9940,   0.9897],\n",
       "         [  0.9971,   0.9981],\n",
       "         [  0.9752,   0.9925],\n",
       "         [  0.9982,   0.9943],\n",
       "         [  0.9899,   0.9912]],\n",
       "\n",
       "        [[  0.9951,   0.9945],\n",
       "         [  0.9914,   0.9988],\n",
       "         [  0.9973,   0.9972],\n",
       "         [  0.9956,   0.9978],\n",
       "         [  0.9873,   0.9955]],\n",
       "\n",
       "        [[  0.9929,   0.9978],\n",
       "         [  0.9978,   0.9916],\n",
       "         [  0.9978,   0.9979],\n",
       "         [  0.9987,   0.9948],\n",
       "         [  0.9976,   0.9975]],\n",
       "\n",
       "        [[  0.9954,   0.9965],\n",
       "         [  0.9987,   0.9984],\n",
       "         [  0.9984,   0.9970],\n",
       "         [  0.9986,   0.9968],\n",
       "         [  0.9980,   0.9981]],\n",
       "\n",
       "        [[  0.9960,   0.9960],\n",
       "         [  0.9964,   0.9870],\n",
       "         [  0.9965,   0.9982],\n",
       "         [  0.9989,   0.9866],\n",
       "         [  0.9991,   0.9981]],\n",
       "\n",
       "        [[  0.9986,   0.9982],\n",
       "         [  0.9989,   0.9930],\n",
       "         [  0.9990,   0.9966],\n",
       "         [  0.9986,   0.9977],\n",
       "         [  0.9992,   0.9978]],\n",
       "\n",
       "        [[  0.9988,   0.9981],\n",
       "         [  0.9992,   0.9977],\n",
       "         [  0.9990,   0.9966],\n",
       "         [  0.9983,   0.9980],\n",
       "         [  0.9990,   0.9986]],\n",
       "\n",
       "        [[  0.9994,   0.9977],\n",
       "         [  0.9994,   0.9978],\n",
       "         [  0.9992,   0.9967],\n",
       "         [  0.9991,   0.9988],\n",
       "         [  0.9994,   0.9982]],\n",
       "\n",
       "        [[  0.9993,   0.9925],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9992,   0.9983],\n",
       "         [  0.9993,   0.9973],\n",
       "         [  0.9991,   0.9979]],\n",
       "\n",
       "        [[  0.9993,   0.9979],\n",
       "         [  0.9985,   0.9981],\n",
       "         [  0.9995,   0.9979],\n",
       "         [  0.9989,   0.9972],\n",
       "         [  0.9994,   0.9969]],\n",
       "\n",
       "        [[  0.9992,   0.9982],\n",
       "         [  0.9992,   0.9983],\n",
       "         [  0.9992,   0.9984],\n",
       "         [  0.9994,   0.9982],\n",
       "         [  0.9994,   0.9985]],\n",
       "\n",
       "        [[  0.9992,   0.9984],\n",
       "         [  0.9995,   0.9982],\n",
       "         [  0.9992,   0.9978],\n",
       "         [  0.9987,   0.9983],\n",
       "         [  0.9992,   0.9974]],\n",
       "\n",
       "        [[  0.9989,   0.9985],\n",
       "         [  0.9995,   0.9986],\n",
       "         [  0.9984,   0.9982],\n",
       "         [  0.9995,   0.9981],\n",
       "         [  0.9993,   0.9987]],\n",
       "\n",
       "        [[  0.9994,   0.9984],\n",
       "         [  0.9993,   0.9945],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9994,   0.9984],\n",
       "         [  0.9995,   0.9987]],\n",
       "\n",
       "        [[  0.9993,   0.9985],\n",
       "         [  0.9991,   0.9979],\n",
       "         [  0.9995,   0.9984],\n",
       "         [  0.9989,   0.9985],\n",
       "         [  0.9993,   0.9986]],\n",
       "\n",
       "        [[  0.9992,   0.9978],\n",
       "         [  0.9995,   0.9984],\n",
       "         [  0.9993,   0.9983],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9994,   0.9986]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c8ef4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.7299, -13.4359],\n",
       "         [ -0.4653,   0.3035],\n",
       "         [ -0.9375,  -1.4426],\n",
       "         [  0.7532,  -1.3478],\n",
       "         [  0.7942,   0.5478]],\n",
       "\n",
       "        [[  0.9748,   0.8843],\n",
       "         [  0.9621,   0.5900],\n",
       "         [  0.8393,   0.9504],\n",
       "         [  0.9729,   0.9679],\n",
       "         [  0.9557,   0.9612]],\n",
       "\n",
       "        [[  0.9910,   0.9662],\n",
       "         [  0.9803,   0.9930],\n",
       "         [  0.9909,   0.9862],\n",
       "         [  0.9709,   0.9916],\n",
       "         [  0.9886,   0.9889]],\n",
       "\n",
       "        [[  0.9972,   0.9931],\n",
       "         [  0.9828,   0.9924],\n",
       "         [  0.9957,   0.9945],\n",
       "         [  0.9942,   0.9855],\n",
       "         [  0.9826,   0.9708]],\n",
       "\n",
       "        [[  0.9940,   0.9897],\n",
       "         [  0.9971,   0.9981],\n",
       "         [  0.9752,   0.9925],\n",
       "         [  0.9982,   0.9943],\n",
       "         [  0.9899,   0.9912]],\n",
       "\n",
       "        [[  0.9951,   0.9945],\n",
       "         [  0.9914,   0.9988],\n",
       "         [  0.9973,   0.9972],\n",
       "         [  0.9956,   0.9978],\n",
       "         [  0.9873,   0.9955]],\n",
       "\n",
       "        [[  0.9929,   0.9978],\n",
       "         [  0.9978,   0.9916],\n",
       "         [  0.9978,   0.9979],\n",
       "         [  0.9987,   0.9948],\n",
       "         [  0.9976,   0.9975]],\n",
       "\n",
       "        [[  0.9954,   0.9965],\n",
       "         [  0.9987,   0.9984],\n",
       "         [  0.9984,   0.9970],\n",
       "         [  0.9986,   0.9968],\n",
       "         [  0.9980,   0.9981]],\n",
       "\n",
       "        [[  0.9960,   0.9960],\n",
       "         [  0.9964,   0.9870],\n",
       "         [  0.9965,   0.9982],\n",
       "         [  0.9989,   0.9866],\n",
       "         [  0.9991,   0.9981]],\n",
       "\n",
       "        [[  0.9986,   0.9982],\n",
       "         [  0.9989,   0.9930],\n",
       "         [  0.9990,   0.9966],\n",
       "         [  0.9986,   0.9977],\n",
       "         [  0.9992,   0.9978]],\n",
       "\n",
       "        [[  0.9988,   0.9981],\n",
       "         [  0.9992,   0.9977],\n",
       "         [  0.9990,   0.9966],\n",
       "         [  0.9983,   0.9980],\n",
       "         [  0.9990,   0.9986]],\n",
       "\n",
       "        [[  0.9994,   0.9977],\n",
       "         [  0.9994,   0.9978],\n",
       "         [  0.9992,   0.9967],\n",
       "         [  0.9991,   0.9988],\n",
       "         [  0.9994,   0.9982]],\n",
       "\n",
       "        [[  0.9993,   0.9925],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9992,   0.9983],\n",
       "         [  0.9993,   0.9973],\n",
       "         [  0.9991,   0.9979]],\n",
       "\n",
       "        [[  0.9993,   0.9979],\n",
       "         [  0.9985,   0.9981],\n",
       "         [  0.9995,   0.9979],\n",
       "         [  0.9989,   0.9972],\n",
       "         [  0.9994,   0.9969]],\n",
       "\n",
       "        [[  0.9992,   0.9982],\n",
       "         [  0.9992,   0.9983],\n",
       "         [  0.9992,   0.9984],\n",
       "         [  0.9994,   0.9982],\n",
       "         [  0.9994,   0.9985]],\n",
       "\n",
       "        [[  0.9992,   0.9984],\n",
       "         [  0.9995,   0.9982],\n",
       "         [  0.9992,   0.9978],\n",
       "         [  0.9987,   0.9983],\n",
       "         [  0.9992,   0.9974]],\n",
       "\n",
       "        [[  0.9989,   0.9985],\n",
       "         [  0.9995,   0.9986],\n",
       "         [  0.9984,   0.9982],\n",
       "         [  0.9995,   0.9981],\n",
       "         [  0.9993,   0.9987]],\n",
       "\n",
       "        [[  0.9994,   0.9984],\n",
       "         [  0.9993,   0.9945],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9994,   0.9984],\n",
       "         [  0.9995,   0.9987]],\n",
       "\n",
       "        [[  0.9993,   0.9985],\n",
       "         [  0.9991,   0.9979],\n",
       "         [  0.9995,   0.9984],\n",
       "         [  0.9989,   0.9985],\n",
       "         [  0.9993,   0.9986]],\n",
       "\n",
       "        [[  0.9992,   0.9978],\n",
       "         [  0.9995,   0.9984],\n",
       "         [  0.9993,   0.9983],\n",
       "         [  0.9994,   0.9979],\n",
       "         [  0.9994,   0.9986]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c07bf934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x[1:],np.hstack((MSE.mean(axis=1),MSE_p.mean(axis=1),MSEa.mean(axis=1)))[1:,[1,3,5]],'o') \n",
    "plt.fill_between(x[1:], MSE.mean(axis=1)[1:,1]+MSE.std(axis=1)[1:,1], y2=MSE.mean(axis=1)[1:,1]-MSE.std(axis=1)[1:,1],alpha=0.4)\n",
    "plt.fill_between(x[1:], MSE_p.mean(axis=1)[1:,1]+MSE_p.std(axis=1)[1:,1], y2=MSE_p.mean(axis=1)[1:,1]-MSE_p.std(axis=1)[1:,1],alpha=0.4)\n",
    "plt.fill_between(x[1:], MSEa.mean(axis=1)[1:,1]+MSEa.std(axis=1)[1:,1], y2=MSEa.mean(axis=1)[1:,1]-MSEa.std(axis=1)[1:,1],alpha=0.4)\n",
    "plt.legend(['$\\delta_a$','$f_1$','$\\delta_1$'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "plt.yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a34fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,np.hstack((R2.mean(axis=1),R2_p.mean(axis=1)))[:,[0,2]],'o') \n",
    "plt.legend(['$\\delta_1$','$f_1$'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,np.hstack((R2.mean(axis=1),R2_p.mean(axis=1)))[:,[1,3]],'o') \n",
    "plt.legend(['$\\delta_1$','$f_1$'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$m$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,MSE_p.mean(axis=1)-MSE.mean(axis=1),'o')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,R2_p.mean(axis=1)-R2.mean(axis=1),'o')\n",
    "plt.legend(y_labels.values)\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xlabel('$m$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_p\n",
    "\n",
    "R2_p-R2\n",
    "\n",
    "plt.plot(x,R2.mean(axis=1),'o')\n",
    "plt.legend(y_labels.values)\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xlabel('$m$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6081e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13668c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d76d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec2e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b626b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    #('scaler', StandardScaler()),\n",
    "    ('preprocessor', PolynomialFeatures(degree=1, include_bias=False,interaction_only=False)),\n",
    "    ('lasso', LassoCV(n_alphas=1000,max_iter=10000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b84a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m0_mat(y_test,emulators,x_test,output):\n",
    "\n",
    "    m0=torch.zeros((y_test.shape[0],len(emulators)))\n",
    "    for i in range(len(emulators)):\n",
    "        m0[:,i]=(emulators[i].predict(x_test)[:,output]-y_train.mean(axis=0)[output])/y_train.std(axis=0)[output]\n",
    "\n",
    "\n",
    "    return m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f336bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy(a,y_train,m0,output):\n",
    "    m_t = (m0-y_train.mean(axis=0))/y_train.std(axis=0)\n",
    "    y_t = (y_train-y_train.mean(axis=0))/y_train.std(axis=0)\n",
    "    a=torch.tensor(a)\n",
    "    res = ((a*m_t-y_t)**2).mean(axis=0).detach().numpy()\n",
    "    return res[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080fcbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:208: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3618.)\n",
      "  prediction=torch.stack(prediction).T\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "tensor([0.9552, 0.8688], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0454, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.78643296]\n",
      "[1.03861305]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9629, 0.9253], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0505, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0454, 0.0486, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.71415911]\n",
      "[0.99313263]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9864, 0.9912], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0505, 0.0518, 0.0000, 0.0000],\n",
      "         [0.0454, 0.0486, 0.0521, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.34170325]\n",
      "[0.96508579]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9594, 0.9342], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0505, 0.0518, 0.0499, 0.0000],\n",
      "         [0.0454, 0.0486, 0.0521, 0.0489, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.35915821]\n",
      "[0.89885475]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.6255, 0.7594], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.0499, 0.0505, 0.0518, 0.0499, 0.0326],\n",
      "         [0.0454, 0.0486, 0.0521, 0.0489, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.75881644]\n",
      "[0.94657515]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9882, 0.6934], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0505, 0.0518, 0.0499, 0.0326],\n",
      "         [0.0813, 0.0486, 0.0521, 0.0489, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.96189814]\n",
      "[0.97596303]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.7960, 0.9484], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0919, 0.0518, 0.0499, 0.0326],\n",
      "         [0.0813, 0.0978, 0.0521, 0.0489, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.89271044]\n",
      "[0.99160148]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9180, 0.5819], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0919, 0.1000, 0.0499, 0.0326],\n",
      "         [0.0813, 0.0978, 0.0826, 0.0489, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.97157977]\n",
      "[0.91008065]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.7494, 0.8679], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0919, 0.1000, 0.0859, 0.0326],\n",
      "         [0.0813, 0.0978, 0.0826, 0.0941, 0.0397]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.88437692]\n",
      "[1.01081197]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9312, 0.9868], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1019, 0.0919, 0.1000, 0.0859, 0.0814],\n",
      "         [0.0813, 0.0978, 0.0826, 0.0941, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.84492806]\n",
      "[1.01515787]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.8359, 0.9575], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.0919, 0.1000, 0.0859, 0.0814],\n",
      "         [0.1312, 0.0978, 0.0826, 0.0941, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.81394902]\n",
      "[0.8382223]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9611, 0.9568], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.1422, 0.1000, 0.0859, 0.0814],\n",
      "         [0.1312, 0.1480, 0.0826, 0.0941, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.78299223]\n",
      "[0.94273276]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9675, 0.9875], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.1422, 0.1507, 0.0859, 0.0814],\n",
      "         [0.1312, 0.1480, 0.1345, 0.0941, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.99151738]\n",
      "[0.96837806]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9761, 0.9703], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.1422, 0.1507, 0.1370, 0.0814],\n",
      "         [0.1312, 0.1480, 0.1345, 0.1451, 0.0915]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.79199399]\n",
      "[0.57044319]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9586, 0.8748], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1454, 0.1422, 0.1507, 0.1370, 0.1318],\n",
      "         [0.1312, 0.1480, 0.1345, 0.1451, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.60603136]\n",
      "[0.52425433]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9534, 0.8806], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1422, 0.1507, 0.1370, 0.1318],\n",
      "         [0.1765, 0.1480, 0.1345, 0.1451, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.82920999]\n",
      "[1.06142236]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.6862, 0.9870], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1779, 0.1507, 0.1370, 0.1318],\n",
      "         [0.1765, 0.1996, 0.1345, 0.1451, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[1.01726546]\n",
      "[0.67125115]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.4974, 0.9368], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1779, 0.1766, 0.1370, 0.1318],\n",
      "         [0.1765, 0.1996, 0.1837, 0.1451, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[1.00381223]\n",
      "[0.4073458]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9897, 0.9805], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1779, 0.1766, 0.1890, 0.1318],\n",
      "         [0.1765, 0.1996, 0.1837, 0.1958, 0.1366]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[1.14658024]\n",
      "[0.95680637]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9293, 0.9246], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.1944, 0.1779, 0.1766, 0.1890, 0.1803],\n",
      "         [0.1765, 0.1996, 0.1837, 0.1958, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[1.03248437]\n",
      "[1.04499828]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9596, 0.9911], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.1779, 0.1766, 0.1890, 0.1803],\n",
      "         [0.2286, 0.1996, 0.1837, 0.1958, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.55192009]\n",
      "[0.73466716]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9733, 0.8583], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.2290, 0.1766, 0.1890, 0.1803],\n",
      "         [0.2286, 0.2423, 0.1837, 0.1958, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.84243681]\n",
      "[0.96488054]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9014, 0.9884], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.2290, 0.2235, 0.1890, 0.1803],\n",
      "         [0.2286, 0.2423, 0.2357, 0.1958, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.97034059]\n",
      "[0.94393583]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.6555, 0.9703], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.2290, 0.2235, 0.2227, 0.1803],\n",
      "         [0.2286, 0.2423, 0.2357, 0.2467, 0.1848]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.96805405]\n",
      "[0.53001674]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9095, 0.9569], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2447, 0.2290, 0.2235, 0.2227, 0.2281],\n",
      "         [0.2286, 0.2423, 0.2357, 0.2467, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.71278732]\n",
      "[0.98381221]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9186, 0.9852], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2290, 0.2235, 0.2227, 0.2281],\n",
      "         [0.2803, 0.2423, 0.2357, 0.2467, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.69985945]\n",
      "[0.85794203]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.8810, 0.9608], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2752, 0.2235, 0.2227, 0.2281],\n",
      "         [0.2803, 0.2928, 0.2357, 0.2467, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.82644116]\n",
      "[0.61731617]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.7579, 0.9913], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2752, 0.2628, 0.2227, 0.2281],\n",
      "         [0.2803, 0.2928, 0.2877, 0.2467, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.35464363]\n",
      "[0.8292417]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9930, 0.9789], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2752, 0.2628, 0.2749, 0.2281],\n",
      "         [0.2803, 0.2928, 0.2877, 0.2981, 0.2347]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.57237759]\n",
      "[0.76894515]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9095, 0.9845], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.2928, 0.2752, 0.2628, 0.2749, 0.2754],\n",
      "         [0.2803, 0.2928, 0.2877, 0.2981, 0.2864]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.77282944]\n",
      "[0.58204038]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "tensor([0.9688, 0.9671], dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[0.3435, 0.2752, 0.2628, 0.2749, 0.2754],\n",
      "         [0.3311, 0.2928, 0.2877, 0.2981, 0.2864]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "[0.93810264]\n",
      "[0.55569326]\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m Ti[\u001b[38;5;241m2\u001b[39m,num,i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m(end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(emulators))\n\u001b[1;32m     69\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 70\u001b[0m model_dc_learned \u001b[38;5;241m=\u001b[39m GPE\u001b[38;5;241m.\u001b[39mensemble(X_train1,y_train1,mean_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrepancy_cohort\u001b[39m\u001b[38;5;124m\"\u001b[39m,training_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,ref_emulator\u001b[38;5;241m=\u001b[39m[emulators2[em]])\n\u001b[1;32m     71\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     72\u001b[0m R2temp,R2std\u001b[38;5;241m=\u001b[39mmodel_dc_learned\u001b[38;5;241m.\u001b[39mR2_sample(X_test,y_test,\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:22\u001b[0m, in \u001b[0;36mensemble.__init__\u001b[0;34m(self, X_train, y_train, mean_func, training_iter, kernel, kernel_params, ref_emulator, a, a_indicator)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m=\u001b[39mkernel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_params\u001b[38;5;241m=\u001b[39mkernel_params\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_ensemble()\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:108\u001b[0m, in \u001b[0;36mensemble.create_ensemble\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, Y)\n\u001b[0;32m--> 108\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# print('Iter %d/%d - Loss: %.3f' % (\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#     j + 1, training_iter, loss.item()\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# ))\u001b[39;00m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reps=5\n",
    "nn=[10,15,20,25,30,35,40,45,50,55,60,80,100,120,140]\n",
    "R2=torch.zeros(7,len(nn),2,reps)\n",
    "ISE=torch.zeros(7,len(nn),2,reps)\n",
    "Ti=torch.zeros(7,len(nn),reps)\n",
    "\n",
    "for num, n in enumerate(nn):\n",
    "    for k in range(len(emulators)):\n",
    "        emulators2=emulators.copy()\n",
    "        emulators2.pop(k)\n",
    "        print(len(emulators2))\n",
    "\n",
    "        X_train = train_input[k]\n",
    "        y_train = train_output[k]\n",
    "        X_test = test_input[k]\n",
    "        y_test = test_output[k]\n",
    "        \n",
    "        for i in range(reps):\n",
    "\n",
    "            #b=np.random.choice(range(X_train.shape[0]),n,replace=False)\n",
    "            \n",
    "            X=X_train\n",
    "            y=y_train \n",
    "            X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                train_size=n,\n",
    "                random_state=i\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            model_f=GPE.ensemble(X_train1,y_train1,mean_func=\"linear\",training_iter=500)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_f.R2_sample(X_test,y_test,1000)\n",
    "            R2[0,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[0,num,:,i]+=model_f.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[0,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "\n",
    "            em=np.random.randint(len(emulators2))\n",
    "            start = time.time()\n",
    "            model_dc_1 = GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]],a=torch.tensor([[1],[1]]))\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_1.R2_sample(X_test,y_test,1000)\n",
    "            R2[1,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[1,num,:,i]+=model_dc_1.ISE(X_test,y_test)/(len(emulators))\n",
    "            print(model_dc_1.R2(X_test,y_test))\n",
    "            print(R2[1])\n",
    "\n",
    "            Ti[1,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            m0 = emulators2[em].predict(X_train1)\n",
    "            a_d=np.zeros((y_train.shape[1],1))\n",
    "            for l in range(y_train.shape[1]):\n",
    "                result = scipy.optimize.minimize(proxy, 1, args=(y_train1,m0,l), method='Nelder-Mead', tol=1e-8)\n",
    "                print(result.x)\n",
    "                a_d[l]=result.x\n",
    "            a_d=torch.tensor(a_d)\n",
    "            model_dc_reg = GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]],a=a_d)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_reg.R2_sample(X_test,y_test,1000)\n",
    "            R2[2,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[2,num,:,i]+=model_dc_reg.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[2,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_learned = GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]])\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[3,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[3,num,:,i]+=model_dc_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[3,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_all = GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_all.R2_sample(X_test,y_test,1000)\n",
    "            R2[4,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[4,num,:,i]+=model_dc_all.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[4,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            a_d=torch.zeros((y_train1.shape[1],len(emulators2)))\n",
    "            for j in range(y_train1.shape[1]):\n",
    "                m0=m0_mat(y_train1,emulators2,X_train1,j)\n",
    "                # fit to an order-3 polynomial data\n",
    "                y_t=(y_train1[:,j]-y_train1.mean(axis=0)[j])/y_train1.std(axis=0)[j]\n",
    "                model = model.fit(m0.detach().numpy(), y_t.detach().numpy())\n",
    "                a_d[j]=torch.tensor(model.named_steps['lasso'].coef_)\n",
    "\n",
    "\n",
    "            model_dc_lasso=GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_lasso.R2_sample(X_test,y_test,1000)\n",
    "            R2[5,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[5,num,:,i]+=model_dc_lasso.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[5,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_lasso_learned=GPE.ensemble(X_train1,y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d,a_indicator=True)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_lasso_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[6,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[6,num,:,i]+=model_dc_lasso_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[6,num,i]+=(end-start)/(len(emulators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame((R2[:,0].mean(axis=2)))\n",
    "\n",
    "results.index=['$f_1$','$f_\\delta$, a=1','$f_\\delta$, regression a','$f_\\delta$, learned a','$f_{\\delta c}$, all','$f_{\\delta c}$, lasso','$f_{\\delta c}$, lasso indicator']\n",
    "\n",
    "results.columns=['A_TAT','V_TAT']\n",
    "\n",
    "results.style.highlight_min(color = 'pink', axis = 0).highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd065459",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame((ISE[:,14].mean(axis=2)))\n",
    "\n",
    "results.index=['$f_1$','$f_\\delta$, a=1','$f_\\delta$, regression a','$f_\\delta$, learned a','$f_{\\delta c}$, all','$f_{\\delta c}$, lasso','$f_{\\delta c}$, lasso indicator']\n",
    "\n",
    "results.columns=['A_TAT','V_TAT']\n",
    "\n",
    "results.style.highlight_min(color = 'pink', axis = 0).highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df34a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=0\n",
    "lim=10\n",
    "y_lim=[0.7,1.01]\n",
    "plt.plot(nn[lim:],R2.mean(axis=3)[:,lim:,o].T)\n",
    "#plt.ylim(y_lim)\n",
    "plt.legend(['$f_1$','$f_\\delta$, a=1','$f_\\delta$, regression a','$f_\\delta$, learned a','$f_{\\delta c}$, all','$f_{\\delta c}$, lasso','$f_{\\delta c}$, lasso indicator'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], R2.mean(axis=3)[i,lim:,o]+R2.std(axis=3)[i,lim:,o], R2.mean(axis=3)[i,lim:,o]-R2.std(axis=3)[i,lim:,o],alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4567a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_save = R2.reshape(7,len(nn)*reps*y_train.shape[1])\n",
    "\n",
    "np.savetxt(\"DiscrepR2TrainNVaryDefinitive.csv\", R2_save.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6878161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "707212a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_s=pd.read_csv(\"DiscrepR2TrainNVaryDefinitive.csv\",header=None).values.reshape(7,len(nn),y_train.shape[1],reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "d8abc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "fontS=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "31e45151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=0\n",
    "lim=1\n",
    "y_lim=[0.75,1.01]\n",
    "plt.plot(nn[lim:],R2_s.mean(axis=3)[:,lim:,o].T)\n",
    "plt.ylim(y_lim)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xlabel('training points',fontsize=fontS)\n",
    "plt.legend(['$g_1$','$g_{\\delta}:a=1$','$g_{\\delta}:a=a_r$','$g_{\\delta h}:a=a_h$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}$','$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}$'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], R2_s.mean(axis=3)[i,lim:,o]+R2_s.std(axis=3)[i,lim:,o], R2_s.mean(axis=3)[i,lim:,o]-R2_s.std(axis=3)[i,lim:,o],alpha=0.4)\n",
    "    plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTDiscrepATAT.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "c11db18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=1\n",
    "y_lim=[0.75,1.01]\n",
    "plt.plot(nn[lim:],R2_s.mean(axis=3)[:,lim:,o].T)\n",
    "plt.ylim(y_lim)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xlabel('training points',fontsize=fontS)\n",
    "plt.legend(['$g_1$','$g_{\\delta}:a=1$','$g_{\\delta}:a=a_r$','$g_{\\delta h}:a=a_h$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}$','$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}$'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], R2_s.mean(axis=3)[i,lim:,o]+R2_s.std(axis=3)[i,lim:,o], R2_s.mean(axis=3)[i,lim:,o]-R2_s.std(axis=3)[i,lim:,o],alpha=0.4)\n",
    "    plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTDiscrepVTAT.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "7398961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 2, 5)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_s[:,3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "7880e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ee7f5ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 80, 100, 120, 140]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "1c84885b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[396], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mdataframe([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'dataframe'"
     ]
    }
   ],
   "source": [
    "pd.dataframe(['&','&','&','&','&','&','&'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "17b8900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "41e1b73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a9b96_row0_col0, #T_a9b96_row0_col1 {\n",
       "  background-color: pink;\n",
       "}\n",
       "#T_a9b96_row5_col1, #T_a9b96_row6_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a9b96\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a9b96_level0_col0\" class=\"col_heading level0 col0\" >A_TAT</th>\n",
       "      <th id=\"T_a9b96_level0_col1\" class=\"col_heading level0 col1\" >V_TAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row0\" class=\"row_heading level0 row0\" >\\$g_1\\$</th>\n",
       "      <td id=\"T_a9b96_row0_col0\" class=\"data row0 col0\" >0.988466</td>\n",
       "      <td id=\"T_a9b96_row0_col1\" class=\"data row0 col1\" >0.977145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row1\" class=\"row_heading level0 row1\" >\\$g_{\\delta}:a=1\\$</th>\n",
       "      <td id=\"T_a9b96_row1_col0\" class=\"data row1 col0\" >0.995786</td>\n",
       "      <td id=\"T_a9b96_row1_col1\" class=\"data row1 col1\" >0.988301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row2\" class=\"row_heading level0 row2\" >\\$g_{\\delta}:a=a_r\\$</th>\n",
       "      <td id=\"T_a9b96_row2_col0\" class=\"data row2 col0\" >0.996202</td>\n",
       "      <td id=\"T_a9b96_row2_col1\" class=\"data row2 col1\" >0.988720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row3\" class=\"row_heading level0 row3\" >\\$g_{\\delta h}:a=a_h\\$</th>\n",
       "      <td id=\"T_a9b96_row3_col0\" class=\"data row3 col0\" >0.996413</td>\n",
       "      <td id=\"T_a9b96_row3_col1\" class=\"data row3 col1\" >0.991275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row4\" class=\"row_heading level0 row4\" >\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row4_col0\" class=\"data row4 col0\" >0.994823</td>\n",
       "      <td id=\"T_a9b96_row4_col1\" class=\"data row4 col1\" >0.993605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row5\" class=\"row_heading level0 row5\" >\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row5_col0\" class=\"data row5 col0\" >0.997331</td>\n",
       "      <td id=\"T_a9b96_row5_col1\" class=\"data row5 col1\" >0.994397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row6\" class=\"row_heading level0 row6\" >\\$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row6_col0\" class=\"data row6 col0\" >0.997876</td>\n",
       "      <td id=\"T_a9b96_row6_col1\" class=\"data row6 col1\" >0.994373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2e5886e10>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame((R2_s[:,6].mean(axis=2)))\n",
    "\n",
    "results.index=['\\$g_1\\$','\\$g_{\\delta}:a=1\\$','\\$g_{\\delta}:a=a_r\\$','\\$g_{\\delta h}:a=a_h\\$','\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}\\$','\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}\\$','\\$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}\\$']\n",
    "\n",
    "results.columns=['A_TAT','V_TAT']\n",
    "\n",
    "results.style.highlight_min(color = 'pink', axis = 0).highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "2a6bf275",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISE_save = ISE.reshape(7,len(nn)*reps*y_train.shape[1])\n",
    "\n",
    "np.savetxt(\"DiscrepISETrainNVaryDefinitive.csv\", ISE_save.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3f12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "c5da4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISE_s=pd.read_csv(\"DiscrepISETrainNVaryDefinitive.csv\",header=None).values.reshape(7,len(nn),y_train.shape[1],reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b49a76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "fontS=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "d59c2b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=0\n",
    "lim=0\n",
    "y_lim=[0.75,1.01]\n",
    "plt.plot(nn[lim:],ISE_s.mean(axis=3)[:,lim:,o].T)\n",
    "#plt.ylim(y_lim)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xlabel('training points',fontsize=fontS)\n",
    "plt.legend(['$g_1$','$g_{\\delta}:a=1$','$g_{\\delta}:a=a_r$','$g_{\\delta h}:a=a_h$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}$','$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}$'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], ISE_s.mean(axis=3)[i,lim:,o]+ISE_s.std(axis=3)[i,lim:,o], ISE_s.mean(axis=3)[i,lim:,o]-ISE_s.std(axis=3)[i,lim:,o],alpha=0.4)\n",
    "    plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTDiscrepATATISE.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "37bb95f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=1\n",
    "y_lim=[0.75,1.01]\n",
    "plt.plot(nn[lim:],ISE_s.mean(axis=3)[:,lim:,o].T)\n",
    "#plt.ylim(y_lim)\n",
    "plt.ylabel('$R^2$',fontsize=fontS)\n",
    "plt.xlabel('training points',fontsize=fontS)\n",
    "plt.legend(['$g_1$','$g_{\\delta}:a=1$','$g_{\\delta}:a=a_r$','$g_{\\delta h}:a=a_h$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}$','$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}$','$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}$'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], ISE_s.mean(axis=3)[i,lim:,o]+ISE_s.std(axis=3)[i,lim:,o], ISE_s.mean(axis=3)[i,lim:,o]-R2_s.std(axis=3)[i,lim:,o],alpha=0.4)\n",
    "    plt.xticks(fontsize=fontS)\n",
    "plt.yticks(fontsize=fontS)\n",
    "plt.savefig('WeavingDTDiscrepVTATISE.pdf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "f440a490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 2, 5)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_s[:,3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "c0197b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "91204688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 80, 100, 120, 140]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "31f56fa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[396], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mdataframe([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'dataframe'"
     ]
    }
   ],
   "source": [
    "pd.dataframe(['&','&','&','&','&','&','&'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "923428bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "ba11b09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a9b96_row0_col0, #T_a9b96_row0_col1 {\n",
       "  background-color: pink;\n",
       "}\n",
       "#T_a9b96_row5_col1, #T_a9b96_row6_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a9b96\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a9b96_level0_col0\" class=\"col_heading level0 col0\" >A_TAT</th>\n",
       "      <th id=\"T_a9b96_level0_col1\" class=\"col_heading level0 col1\" >V_TAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row0\" class=\"row_heading level0 row0\" >\\$g_1\\$</th>\n",
       "      <td id=\"T_a9b96_row0_col0\" class=\"data row0 col0\" >0.988466</td>\n",
       "      <td id=\"T_a9b96_row0_col1\" class=\"data row0 col1\" >0.977145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row1\" class=\"row_heading level0 row1\" >\\$g_{\\delta}:a=1\\$</th>\n",
       "      <td id=\"T_a9b96_row1_col0\" class=\"data row1 col0\" >0.995786</td>\n",
       "      <td id=\"T_a9b96_row1_col1\" class=\"data row1 col1\" >0.988301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row2\" class=\"row_heading level0 row2\" >\\$g_{\\delta}:a=a_r\\$</th>\n",
       "      <td id=\"T_a9b96_row2_col0\" class=\"data row2 col0\" >0.996202</td>\n",
       "      <td id=\"T_a9b96_row2_col1\" class=\"data row2 col1\" >0.988720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row3\" class=\"row_heading level0 row3\" >\\$g_{\\delta h}:a=a_h\\$</th>\n",
       "      <td id=\"T_a9b96_row3_col0\" class=\"data row3 col0\" >0.996413</td>\n",
       "      <td id=\"T_a9b96_row3_col1\" class=\"data row3 col1\" >0.991275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row4\" class=\"row_heading level0 row4\" >\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row4_col0\" class=\"data row4 col0\" >0.994823</td>\n",
       "      <td id=\"T_a9b96_row4_col1\" class=\"data row4 col1\" >0.993605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row5\" class=\"row_heading level0 row5\" >\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row5_col0\" class=\"data row5 col0\" >0.997331</td>\n",
       "      <td id=\"T_a9b96_row5_col1\" class=\"data row5 col1\" >0.994397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a9b96_level0_row6\" class=\"row_heading level0 row6\" >\\$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}\\$</th>\n",
       "      <td id=\"T_a9b96_row6_col0\" class=\"data row6 col0\" >0.997876</td>\n",
       "      <td id=\"T_a9b96_row6_col1\" class=\"data row6 col1\" >0.994373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2e5886e10>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame((R2_s[:,6].mean(axis=2)))\n",
    "\n",
    "results.index=['\\$g_1\\$','\\$g_{\\delta}:a=1\\$','\\$g_{\\delta}:a=a_r\\$','\\$g_{\\delta h}:a=a_h\\$','\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nh}\\}\\$','\\$g_{\\delta c}:\\{a_n\\}=\\{a_{nl}\\}\\$','\\$g_{\\delta c}: \\{a_n\\}=\\{a_{I}\\}\\$']\n",
    "\n",
    "results.columns=['A_TAT','V_TAT']\n",
    "\n",
    "results.style.highlight_min(color = 'pink', axis = 0).highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61ccd047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 24])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_modes[me][b].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af67512e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcat(train_input_modes[me][b])\n",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "torch.cat(train_input_modes[me][b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "151702df",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_input_modes[me,b]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "train_input_modes[me,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba0e9e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.4669,   0.3702,   4.3650,   0.7938,   0.4850,   1.9110, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6276,   0.4758,   7.9716,   0.3821,   0.4486,   2.7628, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6329,   0.2225,   6.9175,   0.8074,   0.4780,   4.4947, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5747,   0.4909,   6.0289,   0.3929,   0.4847,   4.2005, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5416,   0.4729,   5.4991,   0.9151,   0.4372,   1.9766, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7430,   0.4339,   7.3738,   0.6394,   0.3395,   2.8911, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5910,   0.3815,   7.9287,   0.8875,   0.2485,   2.8018, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.4898,   0.4993,   5.1489,   0.8714,   0.2246,   4.0750, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7489,   0.3853,   5.9028,   0.8629,   0.2786,   4.3073, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6975,   0.4939,   2.2900,   0.7766,   0.3535,   4.8623, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.4006,   0.3066,   3.4291,   0.6241,   0.2358,   5.6113, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7773,   0.4405,   6.8365,   0.3150,   0.3999,   2.2361, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.4075,   0.2369,   6.5366,   0.4636,   0.3033,   2.2870, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6223,   0.2108,   3.9212,   0.3294,   0.3687,   1.3961, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6857,   0.3305,   8.2270,   0.7041,   0.3591,   5.2390, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7874,   0.3593,   5.6652,   0.6707,   0.4963,   1.5075, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7030,   0.3530,   5.8316,   1.0252,   0.2604,   3.5431, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7513,   0.2459,   2.7602,   0.3035,   0.2979,   5.1735, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6606,   0.3272,   1.6185,   0.5019,   0.2505,   1.1333, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.4842,   0.2695,   4.8734,   0.7614,   0.3285,   1.7084, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.4653,   0.3547,   6.9905,   0.9830,   0.4218,   1.4759, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7795,   0.4666,   1.4285,   0.5277,   0.4725,   4.1667, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5785,   0.4172,   8.0745,   0.9887,   0.4570,   5.3487, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6556,   0.2564,   6.2705,   0.6673,   0.2920,   4.8677, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7348,   0.3148,   3.5989,   0.3671,   0.4018,   5.1309, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6112,   0.3492,   3.7630,   0.4926,   0.3657,   5.2119, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.4105,   0.2422,   5.6013,   1.0285,   0.3269,   1.8868, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6998,   0.2882,   7.2607,   1.0010,   0.3877,   3.1463, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7084,   0.3557,   3.8859,   0.7373,   0.3609,   1.3080, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5286,   0.2522,   3.8229,   0.7949,   0.3209,   2.3185, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5167,   0.3343,   5.2187,   0.8231,   0.4341,   2.9839, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7336,   0.3226,   7.6859,   0.6830,   0.2894,   1.8393, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5590,   0.2674,   1.5795,   1.0207,   0.3707,   5.4967, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7671,   0.3837,   2.5323,   0.7879,   0.2374,   3.1976, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5852,   0.4669,   7.3007,   0.5913,   0.4791,   4.5521, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.4583,   0.4001,   6.6143,   0.8460,   0.3376,   3.2562, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7400,   0.4566,   2.8697,   0.5339,   0.4907,   3.4433, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.6045,   0.3987,   2.9356,   0.4742,   0.2096,   4.8994, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.7123,   0.2332,   2.5703,   0.4250,   0.4755,   4.8191, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947],\n",
       "        [  0.5097,   0.4594,   4.0428,   0.8358,   0.4325,   1.8287, -26.5263,\n",
       "         -21.9875,  45.2987,   6.9643,   1.2645, -14.7101,   2.9262,   0.1750,\n",
       "          23.6947]], dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_modes[me][b,0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f9380f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=X.copy()\n",
    "y2=y.copy()\n",
    "\n",
    "X2.pop(k)\n",
    "y2.pop(k)\n",
    "\n",
    "X2=torch.concatenate(X2,axis=0)\n",
    "y2=torch.concatenate(y2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a19978d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([720, 15])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d970857f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0472, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0474, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.0522, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0517, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0984, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0910, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.1043, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1008, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1498, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1421, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.1564, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1527, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1954, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1893, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.2086, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2048, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2339, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2383, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.2606, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2565, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2834, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2875, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.3127, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3087, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3285, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3383, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.3648, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3607, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3764, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3899, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.4170, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4130, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4261, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4400, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.4687, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4651, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4695, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4913, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.5203, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5171, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5199, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5417, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.5724, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5689, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5707, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5919, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.6244, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6208, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6194, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6411, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.6764, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6729, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6689, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6858, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.7285, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7238, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7272, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.7805, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7759, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7633, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7713, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.8326, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8264, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8025, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8181, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.8846, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8779, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8485, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8686, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9357, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9296, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([180, 2]) torch.Size([180, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0517, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0495, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0524, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0521, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1038, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0904, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1049, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1043, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1560, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1418, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1574, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1567, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2079, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1911, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2097, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2091, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2603, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2419, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2621, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2611, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3122, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2922, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3146, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3136, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3637, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3439, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3671, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3660, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4151, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3956, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4196, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4183, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4664, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4477, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4719, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4706, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5181, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4978, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5243, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5230, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5699, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5485, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5768, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5753, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6219, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5992, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6292, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6278, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6737, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6498, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6816, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6801, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7254, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6935, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7341, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7322, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7776, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7389, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7866, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7845, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.8298, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7847, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.8388, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8365, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.8819, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8353, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.8913, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8888, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9330, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8857, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9435, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9410, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([560, 2]) torch.Size([560, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9837, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9376, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9960, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9934, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "torch.Size([1130, 2]) torch.Size([1130, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9837, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9376, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0522, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0513, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9960, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9934, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0520, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0520, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([1130, 2]) torch.Size([1130, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9837, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9376, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1045, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1016, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9960, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9934, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1037, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1041, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([1130, 2]) torch.Size([1130, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/pmzcwl/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8885, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9202, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9837, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9376, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1568, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1535, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[[[0.9878, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9819, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9960, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.9934, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1555, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1562, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n",
      "torch.Size([1130, 2]) torch.Size([1130, 15])\n",
      "18\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m X2\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mconcatenate((X2,X_train1),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     69\u001b[0m y2\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mconcatenate((y2,y_train1),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m emulator_latent\u001b[38;5;241m=\u001b[39mGPE\u001b[38;5;241m.\u001b[39mensemble(X2,y2,mean_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,training_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     73\u001b[0m R2temp,R2std\u001b[38;5;241m=\u001b[39mmodel_dc_lasso_learned\u001b[38;5;241m.\u001b[39mR2_sample(X_test,y_test,\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     74\u001b[0m R2[num,: ,:,i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mR2temp\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(emulators))\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:22\u001b[0m, in \u001b[0;36mensemble.__init__\u001b[0;34m(self, X_train, y_train, mean_func, training_iter, kernel, kernel_params, ref_emulator, a, a_indicator)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m=\u001b[39mkernel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_params\u001b[38;5;241m=\u001b[39mkernel_params\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_ensemble()\n",
      "File \u001b[0;32m~/Documents/GitHub/Calibration/GPE_ensemble.py:108\u001b[0m, in \u001b[0;36mensemble.create_ensemble\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, Y)\n\u001b[0;32m--> 108\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# print('Iter %d/%d - Loss: %.3f' % (\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#     j + 1, training_iter, loss.item()\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# ))\u001b[39;00m\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:288\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/functions/_inv_quad_logdet.py:209\u001b[0m, in \u001b[0;36mInvQuadLogdet.backward\u001b[0;34m(ctx, inv_quad_grad_output, logdet_grad_output)\u001b[0m\n\u001b[1;32m    207\u001b[0m left_factors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(left_factors_list, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    208\u001b[0m right_factors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(right_factors_list, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 209\u001b[0m matrix_arg_grads \u001b[38;5;241m=\u001b[39m linear_op\u001b[38;5;241m.\u001b[39m_bilinear_derivative(left_factors, right_factors)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# precond gradient\u001b[39;00m\n\u001b[1;32m    212\u001b[0m precond_arg_grads \u001b[38;5;241m=\u001b[39m precond_lt\u001b[38;5;241m.\u001b[39m_bilinear_derivative(\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;241m-\u001b[39mprecond_probe_vectors \u001b[38;5;241m*\u001b[39m coef, precond_probe_vectors \u001b[38;5;241m*\u001b[39m logdet_grad_output\n\u001b[1;32m    214\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/sum_linear_operator.py:58\u001b[0m, in \u001b[0;36mSumLinearOperator._bilinear_derivative\u001b[0;34m(self, left_vecs, right_vecs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bilinear_derivative\u001b[39m(\u001b[38;5;28mself\u001b[39m, left_vecs: Tensor, right_vecs: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Tensor], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m     59\u001b[0m         var \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39m_bilinear_derivative(left_vecs, right_vecs)\n\u001b[1;32m     60\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/sum_linear_operator.py:59\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bilinear_derivative\u001b[39m(\u001b[38;5;28mself\u001b[39m, left_vecs: Tensor, right_vecs: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Tensor], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m---> 59\u001b[0m         var \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m linear_op\u001b[38;5;241m.\u001b[39m_bilinear_derivative(left_vecs, right_vecs)\n\u001b[1;32m     60\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/dense_linear_operator.py:71\u001b[0m, in \u001b[0;36mDenseLinearOperator._bilinear_derivative\u001b[0;34m(self, left_vecs, right_vecs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_bilinear_derivative\u001b[39m(\u001b[38;5;28mself\u001b[39m, left_vecs: Tensor, right_vecs: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Tensor], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m---> 71\u001b[0m     res \u001b[38;5;241m=\u001b[39m left_vecs\u001b[38;5;241m.\u001b[39mmatmul(right_vecs\u001b[38;5;241m.\u001b[39mmT)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (res,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reps=5\n",
    "nn=[10,20,30,40]\n",
    "n1=[40]\n",
    "R2=torch.zeros(len(nn),len(n1),2,reps)\n",
    "ISE=torch.zeros(len(nn),len(n1),2,reps)\n",
    "\n",
    "R2_latent=torch.zeros(len(nn),len(n1),2,reps)\n",
    "ISE_latent=torch.zeros(len(nn),len(n1),2,reps)\n",
    "\n",
    "Ti=torch.zeros(len(nn),len(n1),reps)\n",
    "Xs=[]\n",
    "ys=[]\n",
    "for i in range(reps):\n",
    "    for num, n in enumerate(nn):\n",
    "        emulators=[]\n",
    "        for me in range(len(meshes)):\n",
    "            b=np.random.choice(range(train_input[me].shape[0]),n,replace=False)\n",
    "            emulators.append(GPE.ensemble(train_input[me][b],train_output[me][b],mean_func=\"linear\",training_iter=1000))\n",
    "        \n",
    "            Xs.append(train_input_modes[me][b,0:15])\n",
    "            ys.append(train_output_modes[me][b])\n",
    "\n",
    "        for k in range(len(emulators)):\n",
    "            emulators2=emulators.copy()\n",
    "            emulators2.pop(k)\n",
    "            \n",
    "            X2=Xs.copy()\n",
    "            y2=ys.copy()\n",
    "            \n",
    "            X2.pop(k)\n",
    "            y2.pop(k)\n",
    "            \n",
    "            X2=torch.concatenate(X2,axis=0)\n",
    "            y2=torch.concatenate(y2,axis=0)\n",
    "            print(y2.shape,X2.shape)\n",
    "            \n",
    "            print(len(emulators2))\n",
    "            \n",
    "            emulator_latent=GPE.ensemble(X2,y2,mean_func=\"linear\",training_iter=500)\n",
    "            \n",
    "\n",
    "            X_train = train_input_modes[k][:,0:15]\n",
    "            y_train = train_output_modes[k]\n",
    "            X_test = test_input_modes[k][:,0:6]\n",
    "            y_test = test_output_modes[k]\n",
    "            X_test_l = test_input_modes[k][:,0:15]\n",
    "            y_test_l = test_output_modes[k]\n",
    "            #b=np.random.choice(range(X_train.shape[0]),n,replace=False)\n",
    "\n",
    "            X=X_train\n",
    "            y=y_train \n",
    "            X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                train_size=20,\n",
    "                random_state=i\n",
    "            )\n",
    "            a_d=torch.zeros((y_train1.shape[1],len(emulators2)))\n",
    "            for j in range(y_train1.shape[1]):\n",
    "                m0=m0_mat(y_train1,emulators2,X_train1[:,0:6],j)\n",
    "                # fit to an order-3 polynomial data\n",
    "                y_t=(y_train1[:,j]-y_train1.mean(axis=0)[j])/y_train1.std(axis=0)[j]\n",
    "                model = model.fit(m0.detach().numpy(), y_t.detach().numpy())\n",
    "                a_d[j]=torch.tensor(model.named_steps['lasso'].coef_)\n",
    "            \n",
    "            model_dc_lasso_learned=GPE.ensemble(X_train1[:,0:6],y_train1,mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d,a_indicator=True)\n",
    "            \n",
    "            X2=torch.concatenate((X2,X_train1),axis=0)\n",
    "            y2=torch.concatenate((y2,y_train1),axis=0)\n",
    "            \n",
    "            emulator_latent=GPE.ensemble(X2,y2,mean_func=\"linear\",training_iter=500)\n",
    "            \n",
    "            R2temp,R2std=model_dc_lasso_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[num,: ,:,i]+=R2temp/(len(emulators))\n",
    "            print(R2)\n",
    "            ISE[num,:,:,i]+=model_dc_lasso_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "            \n",
    "            \n",
    "            R2temp,R2std=emulator_latent.R2_sample(X_test_l,y_test_l,1000)\n",
    "            R2_latent[num,: ,:,i]+=R2temp/(len(emulators))\n",
    "            print(R2_latent)\n",
    "            ISE_latent[num,:,:,i]+=emulator_latent.ISE(X_test_l,y_test_l)/(len(emulators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e8d1078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12dcff190>,\n",
       " <matplotlib.lines.Line2D at 0x12dccdfd0>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn,R2.mean(axis=3).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3c31f",
   "metadata": {},
   "source": [
    "# Atrial Stiffness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee892899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b4ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meshes=['01','02','03','04','05','06']\n",
    "\n",
    "Ys=[]\n",
    "Xs=[]\n",
    "\n",
    "for i in range(len(meshes)):\n",
    "    val=meshes[i]\n",
    "    \n",
    "    inputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/LA_data/case\"+val+\"/X.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "    outputData = pd.read_csv(\"/Users/pmzcwl/Library/CloudStorage/OneDrive-TheUniversityofNottingham/shared_simulations/LA_data/case\"+val+\"/Y.txt\",index_col=None,delim_whitespace=True,header=None).values\n",
    "\n",
    "    \n",
    "\n",
    "    Xs.append(torch.tensor(inputData[0:200]))\n",
    "    Ys.append(torch.tensor(outputData[0:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9b3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "train_input=[]\n",
    "test_input = []\n",
    "train_output=[]\n",
    "test_output = []\n",
    "emulators=[]\n",
    "\n",
    "for i in range(len(meshes)):\n",
    "\n",
    "    X=Xs[i]\n",
    "    y=Ys[i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=seed+i\n",
    "    )\n",
    "    train_input.append(X_train)\n",
    "    test_input.append(X_test)\n",
    "    train_output.append(y_train)\n",
    "    test_output.append(y_test)\n",
    "    emulator = GPE.ensemble(X_train,y_train,mean_func=\"linear\",training_iter=500)\n",
    "    emulators.append(emulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0841332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([160, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bb6a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -9.0876,  -1.7839, -13.9763,  -6.8036,  -5.3839, -28.4096,   0.7142],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2980,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.3310,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1354,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9003,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-4.7392,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1186,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n",
      "[0.42850708]\n",
      "[0.16977268]\n",
      "[0.18070077]\n",
      "[0.48470408]\n",
      "[0.06471149]\n",
      "[0.30723889]\n",
      "[0.00915806]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.5460,  0.7232, -0.4243,  0.7317,  0.4467, -0.9929,  0.9197],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159,  0.0892,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2980,  0.1193,  0.0000,  0.0000,  0.0000],\n",
      "         [-2.3310, -0.0738,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1354,  0.1175,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9003,  0.0718,  0.0000,  0.0000,  0.0000],\n",
      "         [-4.7392, -0.1692,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1186,  0.1523,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n",
      "[0.44011486]\n",
      "[0.35291706]\n",
      "[0.31281794]\n",
      "[0.64680969]\n",
      "[0.07741194]\n",
      "[0.17071068]\n",
      "[0.03316094]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-1.1040, -0.8396, -0.2666,  0.5166,  0.1814, -3.6879,  0.8796],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159,  0.0892, -0.1893,  0.0000,  0.0000],\n",
      "         [-0.2980,  0.1193, -0.1452,  0.0000,  0.0000],\n",
      "         [-2.3310, -0.0738, -0.0516,  0.0000,  0.0000],\n",
      "         [-1.1354,  0.1175,  0.0817,  0.0000,  0.0000],\n",
      "         [-0.9003,  0.0718,  0.0278,  0.0000,  0.0000],\n",
      "         [-4.7392, -0.1692, -0.6288,  0.0000,  0.0000],\n",
      "         [ 0.1186,  0.1523,  0.1439,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n",
      "[0.64439529]\n",
      "[0.08674326]\n",
      "[0.04759069]\n",
      "[0.66768465]\n",
      "[0.56556691]\n",
      "[0.17467271]\n",
      "[0.21179094]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 2.9886e-02, -9.8268e-01, -7.7263e+00, -8.4969e+01, -2.2269e+00,\n",
      "        -2.5796e+00, -1.3139e-01], dtype=torch.float64,\n",
      "       grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159e+00,  8.9238e-02, -1.8929e-01, -3.1074e-04,  0.0000e+00],\n",
      "         [-2.9798e-01,  1.1934e-01, -1.4525e-01, -1.6743e-01,  0.0000e+00],\n",
      "         [-2.3310e+00, -7.3837e-02, -5.1640e-02, -1.2959e+00,  0.0000e+00],\n",
      "         [-1.1354e+00,  1.1755e-01,  8.1675e-02, -1.4165e+01,  0.0000e+00],\n",
      "         [-9.0030e-01,  7.1777e-02,  2.7786e-02, -3.7606e-01,  0.0000e+00],\n",
      "         [-4.7392e+00, -1.6921e-01, -6.2883e-01, -4.3527e-01,  0.0000e+00],\n",
      "         [ 1.1855e-01,  1.5228e-01,  1.4393e-01, -2.3658e-02,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.02861606]\n",
      "[0.00496223]\n",
      "[0.03694755]\n",
      "[0.71150105]\n",
      "[0.03157613]\n",
      "[0.0565942]\n",
      "[0.12495688]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-0.1265,  0.6150, -0.2116,  0.6184, -0.1561, -0.0845,  0.6877],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.5159e+00,  8.9238e-02, -1.8929e-01, -3.1074e-04, -2.8553e-02],\n",
      "         [-2.9798e-01,  1.1934e-01, -1.4525e-01, -1.6743e-01,  8.3507e-02],\n",
      "         [-2.3310e+00, -7.3837e-02, -5.1640e-02, -1.2959e+00, -3.9182e-02],\n",
      "         [-1.1354e+00,  1.1755e-01,  8.1675e-02, -1.4165e+01,  9.0978e-02],\n",
      "         [-9.0030e-01,  7.1777e-02,  2.7786e-02, -3.7606e-01, -3.3002e-02],\n",
      "         [-4.7392e+00, -1.6921e-01, -6.2883e-01, -4.3527e-01, -2.7121e-02],\n",
      "         [ 1.1855e-01,  1.5228e-01,  1.4393e-01, -2.3658e-02,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.07271767]\n",
      "[0.04698787]\n",
      "[0.04238831]\n",
      "[0.74447277]\n",
      "[0.06359182]\n",
      "[0.13098816]\n",
      "[0.18652486]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-0.9011, -0.5603, -1.2820,  0.7567,  0.8196, -3.8143,  0.6895],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  8.9238e-02, -1.8929e-01, -3.1074e-04, -2.8553e-02],\n",
      "         [-3.9845e-01,  1.1934e-01, -1.4525e-01, -1.6743e-01,  8.3507e-02],\n",
      "         [-2.5616e+00, -7.3837e-02, -5.1640e-02, -1.2959e+00, -3.9182e-02],\n",
      "         [-1.0136e+00,  1.1755e-01,  8.1675e-02, -1.4165e+01,  9.0978e-02],\n",
      "         [-7.7255e-01,  7.1777e-02,  2.7786e-02, -3.7606e-01, -3.3002e-02],\n",
      "         [-5.3844e+00, -1.6921e-01, -6.2883e-01, -4.3527e-01, -2.7121e-02],\n",
      "         [ 2.3120e-01,  1.5228e-01,  1.4393e-01, -2.3658e-02,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.13029052]\n",
      "[0.88796357]\n",
      "[1.04526361]\n",
      "[0.87840017]\n",
      "[0.56479376]\n",
      "[0.71230068]\n",
      "[0.34944377]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.5147, -0.1967,  0.4627,  0.4926,  0.3607, -0.8251,  0.6441],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  1.6980e-01, -1.8929e-01, -3.1074e-04, -2.8553e-02],\n",
      "         [-3.9845e-01,  8.1723e-02, -1.4525e-01, -1.6743e-01,  8.3507e-02],\n",
      "         [-2.5616e+00, -3.7197e-03, -5.1640e-02, -1.2959e+00, -3.9182e-02],\n",
      "         [-1.0136e+00,  1.9827e-01,  8.1675e-02, -1.4165e+01,  9.0978e-02],\n",
      "         [-7.7255e-01,  1.2487e-01,  2.7786e-02, -3.7606e-01, -3.3002e-02],\n",
      "         [-5.3844e+00, -3.1508e-01, -6.2883e-01, -4.3527e-01, -2.7121e-02],\n",
      "         [ 2.3120e-01,  2.5773e-01,  1.4393e-01, -2.3658e-02,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.02008228]\n",
      "[0.92773732]\n",
      "[0.5248596]\n",
      "[0.97486199]\n",
      "[0.75069551]\n",
      "[0.97609093]\n",
      "[0.56423686]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -3.3725, -27.0962, -12.8221,  -3.0624,  -4.5058,  -1.8377,   0.3056],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  1.6980e-01, -7.6336e-01, -3.1074e-04, -2.8553e-02],\n",
      "         [-3.9845e-01,  8.1723e-02, -4.6718e+00, -1.6743e-01,  8.3507e-02],\n",
      "         [-2.5616e+00, -3.7197e-03, -2.2151e+00, -1.2959e+00, -3.9182e-02],\n",
      "         [-1.0136e+00,  1.9827e-01, -4.3525e-01, -1.4165e+01,  9.0978e-02],\n",
      "         [-7.7255e-01,  1.2487e-01, -7.3310e-01, -3.7606e-01, -3.3002e-02],\n",
      "         [-5.3844e+00, -3.1508e-01, -9.5026e-01, -4.3527e-01, -2.7121e-02],\n",
      "         [ 2.3120e-01,  2.5773e-01,  1.9112e-01, -2.3658e-02,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.07645264]\n",
      "[0.82026949]\n",
      "[1.0644201]\n",
      "[0.96888561]\n",
      "[0.48250508]\n",
      "[1.02134132]\n",
      "[0.63585567]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -3.9390,  -7.8289, -45.9384,  -9.2830, -11.6866, -23.0289, -14.8757],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  1.6980e-01, -7.6336e-01, -7.4473e-01, -2.8553e-02],\n",
      "         [-3.9845e-01,  8.1723e-02, -4.6718e+00, -1.6129e+00,  8.3507e-02],\n",
      "         [-2.5616e+00, -3.7197e-03, -2.2151e+00, -9.0295e+00, -3.9182e-02],\n",
      "         [-1.0136e+00,  1.9827e-01, -4.3525e-01, -1.5751e+01,  9.0978e-02],\n",
      "         [-7.7255e-01,  1.2487e-01, -7.3310e-01, -2.3574e+00, -3.3002e-02],\n",
      "         [-5.3844e+00, -3.1508e-01, -9.5026e-01, -4.3203e+00, -2.7121e-02],\n",
      "         [ 2.3120e-01,  2.5773e-01,  1.9112e-01, -2.5165e+00,  1.1002e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.45529429]\n",
      "[1.65206165]\n",
      "[0.40189023]\n",
      "[1.41990188]\n",
      "[0.35982506]\n",
      "[1.27250719]\n",
      "[0.07092059]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-0.0077, -4.5067, -1.4140,  0.6690, -5.5467, -0.7967, -4.5380],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.6736e+00,  1.6980e-01, -7.6336e-01, -7.4473e-01, -3.7971e-02],\n",
      "         [-3.9845e-01,  8.1723e-02, -4.6718e+00, -1.6129e+00, -6.7470e-01],\n",
      "         [-2.5616e+00, -3.7197e-03, -2.2151e+00, -9.0295e+00, -2.9150e-01],\n",
      "         [-1.0136e+00,  1.9827e-01, -4.3525e-01, -1.5751e+01,  1.9450e-01],\n",
      "         [-7.7255e-01,  1.2487e-01, -7.3310e-01, -2.3574e+00, -9.7383e-01],\n",
      "         [-5.3844e+00, -3.1508e-01, -9.5026e-01, -4.3203e+00, -1.7452e-01],\n",
      "         [ 2.3120e-01,  2.5773e-01,  1.9112e-01, -2.5165e+00, -6.4878e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.54948695]\n",
      "[0.1307068]\n",
      "[0.45193608]\n",
      "[1.68455612]\n",
      "[0.571211]\n",
      "[0.52624828]\n",
      "[0.03160185]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-1.8201, -0.8780,  0.0804,  0.5079, -5.1852, -0.5305,  0.0234],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.9893e+00,  1.6980e-01, -7.6336e-01, -7.4473e-01, -3.7971e-02],\n",
      "         [-5.5799e-01,  8.1723e-02, -4.6718e+00, -1.6129e+00, -6.7470e-01],\n",
      "         [-2.5574e+00, -3.7197e-03, -2.2151e+00, -9.0295e+00, -2.9150e-01],\n",
      "         [-9.3094e-01,  1.9827e-01, -4.3525e-01, -1.5751e+01,  1.9450e-01],\n",
      "         [-1.6622e+00,  1.2487e-01, -7.3310e-01, -2.3574e+00, -9.7383e-01],\n",
      "         [-5.5121e+00, -3.1508e-01, -9.5026e-01, -4.3203e+00, -1.7452e-01],\n",
      "         [ 2.2975e-01,  2.5773e-01,  1.9112e-01, -2.5165e+00, -6.4878e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.2223029]\n",
      "[1.03344418]\n",
      "[1.17388989]\n",
      "[1.47676235]\n",
      "[1.14969611]\n",
      "[0.9811797]\n",
      "[0.44833741]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -0.5592,  -0.6650,  -0.0874,   0.5549,  -0.5536, -48.9892,  -0.4980],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9893,   0.0641,  -0.7634,  -0.7447,  -0.0380],\n",
      "         [ -0.5580,  -0.0368,  -4.6718,  -1.6129,  -0.6747],\n",
      "         [ -2.5574,  -0.0292,  -2.2151,  -9.0295,  -0.2915],\n",
      "         [ -0.9309,   0.2890,  -0.4353, -15.7510,   0.1945],\n",
      "         [ -1.6622,   0.0210,  -0.7331,  -2.3574,  -0.9738],\n",
      "         [ -5.5121,  -8.5128,  -0.9503,  -4.3203,  -0.1745],\n",
      "         [  0.2297,   0.1709,   0.1911,  -2.5165,  -0.6488]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.13373025]\n",
      "[1.32248917]\n",
      "[1.11305912]\n",
      "[0.92121302]\n",
      "[1.17055445]\n",
      "[1.04957312]\n",
      "[0.52580177]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-1.4955, -1.4294,  0.1148,  0.2980, -1.3630, -3.6188, -2.2182],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9893,   0.0641,  -1.0452,  -0.7447,  -0.0380],\n",
      "         [ -0.5580,  -0.0368,  -4.9599,  -1.6129,  -0.6747],\n",
      "         [ -2.5574,  -0.0292,  -2.2142,  -9.0295,  -0.2915],\n",
      "         [ -0.9309,   0.2890,  -0.4061, -15.7510,   0.1945],\n",
      "         [ -1.6622,   0.0210,  -0.9823,  -2.3574,  -0.9738],\n",
      "         [ -5.5121,  -8.5128,  -1.5887,  -4.3203,  -0.1745],\n",
      "         [  0.2297,   0.1709,  -0.2005,  -2.5165,  -0.6488]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.9599092]\n",
      "[1.01336825]\n",
      "[0.84972161]\n",
      "[0.96835811]\n",
      "[0.75670898]\n",
      "[0.73550715]\n",
      "[0.27096455]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8759, 0.8812, 0.4617, 0.5173, 0.7389, 0.8296, 0.9020],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9893,   0.0641,  -1.0452,  -0.6012,  -0.0380],\n",
      "         [ -0.5580,  -0.0368,  -4.9599,  -1.4674,  -0.6747],\n",
      "         [ -2.5574,  -0.0292,  -2.2142,  -8.9586,  -0.2915],\n",
      "         [ -0.9309,   0.2890,  -0.4061, -15.6657,   0.1945],\n",
      "         [ -1.6622,   0.0210,  -0.9823,  -2.2393,  -0.9738],\n",
      "         [ -5.5121,  -8.5128,  -1.5887,  -4.1893,  -0.1745],\n",
      "         [  0.2297,   0.1709,  -0.2005,  -2.3671,  -0.6488]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.75894311]\n",
      "[1.19966229]\n",
      "[1.06858971]\n",
      "[0.98402007]\n",
      "[0.76041896]\n",
      "[0.65378435]\n",
      "[0.33374144]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.0101, -2.1094, -5.7402, -2.1136, -1.0794,  0.7227,  0.7524],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9893,   0.0641,  -1.0452,  -0.6012,  -0.0393],\n",
      "         [ -0.5580,  -0.0368,  -4.9599,  -1.4674,  -1.0300],\n",
      "         [ -2.5574,  -0.0292,  -2.2142,  -8.9586,  -1.2533],\n",
      "         [ -0.9309,   0.2890,  -0.4061, -15.6657,  -0.1989],\n",
      "         [ -1.6622,   0.0210,  -0.9823,  -2.2393,  -1.1578],\n",
      "         [ -5.5121,  -8.5128,  -1.5887,  -4.1893,  -0.0557],\n",
      "         [  0.2297,   0.1709,  -0.2005,  -2.3671,  -0.5255]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.02600945]\n",
      "[-0.00712538]\n",
      "[0.0440953]\n",
      "[1.80280321]\n",
      "[0.00164515]\n",
      "[0.07408324]\n",
      "[0.02372122]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.2507, -3.5920, -1.2771, -1.5240, -0.1049,  0.5904, -0.7743],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9530,   0.0641,  -1.0452,  -0.6012,  -0.0393],\n",
      "         [ -1.1628,  -0.0368,  -4.9599,  -1.4674,  -1.0300],\n",
      "         [ -2.7801,  -0.0292,  -2.2142,  -8.9586,  -1.2533],\n",
      "         [ -1.1951,   0.2890,  -0.4061, -15.6657,  -0.1989],\n",
      "         [ -1.6912,   0.0210,  -0.9823,  -2.2393,  -1.1578],\n",
      "         [ -5.4192,  -8.5128,  -1.5887,  -4.1893,  -0.0557],\n",
      "         [  0.0974,   0.1709,  -0.2005,  -2.3671,  -0.5255]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.12159441]\n",
      "[0.03086853]\n",
      "[0.17833149]\n",
      "[0.587976]\n",
      "[0.06118822]\n",
      "[0.1205446]\n",
      "[0.23965024]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -4.4481, -15.6362,  -5.2597,  -7.6927, -10.4462,  -0.7631,  -2.3826],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9530,  -0.6824,  -1.0452,  -0.6012,  -0.0393],\n",
      "         [ -1.1628,  -2.6534,  -4.9599,  -1.4674,  -1.0300],\n",
      "         [ -2.7801,  -0.9098,  -2.2142,  -8.9586,  -1.2533],\n",
      "         [ -1.1951,  -0.9967,  -0.4061, -15.6657,  -0.1989],\n",
      "         [ -1.6912,  -1.7323,  -0.9823,  -2.2393,  -1.1578],\n",
      "         [ -5.4192,  -8.6466,  -1.5887,  -4.1893,  -0.0557],\n",
      "         [  0.0974,  -0.2318,  -0.2005,  -2.3671,  -0.5255]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.093986]\n",
      "[0.19462666]\n",
      "[0.02443466]\n",
      "[0.1126737]\n",
      "[0.02831467]\n",
      "[0.16848996]\n",
      "[0.06810595]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-1.1429e+02, -3.3096e+01, -4.1948e+02, -6.2976e+00,  2.4170e-01,\n",
      "        -4.2526e+01, -5.3397e+01], dtype=torch.float64,\n",
      "       grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.9530e+00, -6.8238e-01, -2.0111e+01, -6.0120e-01, -3.9328e-02],\n",
      "         [-1.1628e+00, -2.6534e+00, -1.0484e+01, -1.4674e+00, -1.0300e+00],\n",
      "         [-2.7801e+00, -9.0976e-01, -7.2283e+01, -8.9586e+00, -1.2533e+00],\n",
      "         [-1.1951e+00, -9.9669e-01, -1.4569e+00, -1.5666e+01, -1.9887e-01],\n",
      "         [-1.6912e+00, -1.7323e+00, -9.4971e-01, -2.2393e+00, -1.1578e+00],\n",
      "         [-5.4192e+00, -8.6466e+00, -8.7190e+00, -4.1893e+00, -5.5703e-02],\n",
      "         [ 9.7369e-02, -2.3182e-01, -9.0961e+00, -2.3671e+00, -5.2546e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.04617866]\n",
      "[0.18357426]\n",
      "[0.00443144]\n",
      "[0.0333105]\n",
      "[0.05584475]\n",
      "[0.03238173]\n",
      "[0.85556338]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.1938, -0.4693,  0.6190,  0.5562,  0.5081,  0.3070,  0.8376],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.9530e+00, -6.8238e-01, -2.0111e+01, -5.9788e-01, -3.9328e-02],\n",
      "         [-1.1628e+00, -2.6534e+00, -1.0484e+01, -1.5772e+00, -1.0300e+00],\n",
      "         [-2.7801e+00, -9.0976e-01, -7.2283e+01, -8.8736e+00, -1.2533e+00],\n",
      "         [-1.1951e+00, -9.9669e-01, -1.4569e+00, -1.5581e+01, -1.9887e-01],\n",
      "         [-1.6912e+00, -1.7323e+00, -9.4971e-01, -2.1775e+00, -1.1578e+00],\n",
      "         [-5.4192e+00, -8.6466e+00, -8.7190e+00, -4.1631e+00, -5.5703e-02],\n",
      "         [ 9.7369e-02, -2.3182e-01, -9.0961e+00, -2.2306e+00, -5.2546e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.04022648]\n",
      "[0.01869117]\n",
      "[0.06738706]\n",
      "[0.35855708]\n",
      "[0.02742423]\n",
      "[0.07474823]\n",
      "[0.1309245]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -2.1043,  -1.4666,  -2.1456, -13.0647,  -4.6588,  -3.6519,  -5.0345],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.9530,  -0.6824, -20.1109,  -0.5979,  -0.3950],\n",
      "         [ -1.1628,  -2.6534, -10.4843,  -1.5772,  -1.2840],\n",
      "         [ -2.7801,  -0.9098, -72.2828,  -8.8736,  -1.6200],\n",
      "         [ -1.1951,  -0.9967,  -1.4569, -15.5815,  -2.3964],\n",
      "         [ -1.6912,  -1.7323,  -0.9497,  -2.1775,  -1.9525],\n",
      "         [ -5.4192,  -8.6466,  -8.7190,  -4.1631,  -0.6713],\n",
      "         [  0.0974,  -0.2318,  -9.0961,  -2.2306,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.02837794]\n",
      "[-0.01931925]\n",
      "[0.02790187]\n",
      "[0.28639597]\n",
      "[0.00327114]\n",
      "[0.14100475]\n",
      "[0.09565693]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.7856, -0.1464, -0.4788, -0.4424,  0.6392,  0.4986,  0.8500],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -0.6824, -20.1109,  -0.5979,  -0.3950],\n",
      "         [ -1.1877,  -2.6534, -10.4843,  -1.5772,  -1.2840],\n",
      "         [ -2.8644,  -0.9098, -72.2828,  -8.8736,  -1.6200],\n",
      "         [ -1.2723,  -0.9967,  -1.4569, -15.5815,  -2.3964],\n",
      "         [ -1.5888,  -1.7323,  -0.9497,  -2.1775,  -1.9525],\n",
      "         [ -5.3403,  -8.6466,  -8.7190,  -4.1631,  -0.6713],\n",
      "         [  0.2379,  -0.2318,  -9.0961,  -2.2306,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.37187161]\n",
      "[0.14164157]\n",
      "[0.11595056]\n",
      "[0.44432908]\n",
      "[0.03590506]\n",
      "[0.44437426]\n",
      "[0.0212642]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-3.5449, -3.9422, -6.4219, -0.1840, -0.7939, -2.9104,  0.2943],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -1.3061, -20.1109,  -0.5979,  -0.3950],\n",
      "         [ -1.1877,  -3.3354, -10.4843,  -1.5772,  -1.2840],\n",
      "         [ -2.8644,  -2.0310, -72.2828,  -8.8736,  -1.6200],\n",
      "         [ -1.2723,  -1.0388,  -1.4569, -15.5815,  -2.3964],\n",
      "         [ -1.5888,  -1.8891,  -0.9497,  -2.1775,  -1.9525],\n",
      "         [ -5.3403,  -9.1803,  -8.7190,  -4.1631,  -0.6713],\n",
      "         [  0.2379,  -0.1890,  -9.0961,  -2.2306,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.0870387]\n",
      "[0.09508985]\n",
      "[0.11081348]\n",
      "[0.53238232]\n",
      "[1.89823918]\n",
      "[1.70403277]\n",
      "[0.25674953]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.0134,  0.4340, -3.0033, -1.6794, -0.3272,  0.4908,  0.9357],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -1.3061, -20.1128,  -0.5979,  -0.3950],\n",
      "         [ -1.1877,  -3.3354, -10.4144,  -1.5772,  -1.2840],\n",
      "         [ -2.8644,  -2.0310, -72.7936,  -8.8736,  -1.6200],\n",
      "         [ -1.2723,  -1.0388,  -1.7385, -15.5815,  -2.3964],\n",
      "         [ -1.5888,  -1.8891,  -1.0156,  -2.1775,  -1.9525],\n",
      "         [ -5.3403,  -9.1803,  -8.6396,  -4.1631,  -0.6713],\n",
      "         [  0.2379,  -0.1890,  -8.9415,  -2.2306,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.44508698]\n",
      "[0.08639242]\n",
      "[0.22692524]\n",
      "[0.48468619]\n",
      "[0.32815288]\n",
      "[0.21465635]\n",
      "[0.03568206]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -6.7260,   0.3728,  -2.8444,   0.5183,  -6.7835, -80.8901,  -2.4390],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -1.3061, -20.1128,  -1.7299,  -0.3950],\n",
      "         [ -1.1877,  -3.3354, -10.4144,  -1.5197,  -1.2840],\n",
      "         [ -2.8644,  -2.0310, -72.7936,  -9.3572,  -1.6200],\n",
      "         [ -1.2723,  -1.0388,  -1.7385, -15.4982,  -2.3964],\n",
      "         [ -1.5888,  -1.8891,  -1.0156,  -3.3190,  -1.9525],\n",
      "         [ -5.3403,  -9.1803,  -8.6396, -17.6866,  -0.6713],\n",
      "         [  0.2379,  -0.1890,  -8.9415,  -2.6413,  -1.3710]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.24969766]\n",
      "[0.05726541]\n",
      "[0.0794225]\n",
      "[0.2864663]\n",
      "[1.93992311]\n",
      "[1.17877238]\n",
      "[0.26881902]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.2933,  0.1265, -3.6262, -2.7459, -5.8919, -0.6997,  0.4026],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.8233,  -1.3061, -20.1128,  -1.7299,  -0.3805],\n",
      "         [ -1.1877,  -3.3354, -10.4144,  -1.5197,  -1.2777],\n",
      "         [ -2.8644,  -2.0310, -72.7936,  -9.3572,  -2.3357],\n",
      "         [ -1.2723,  -1.0388,  -1.7385, -15.4982,  -2.8747],\n",
      "         [ -1.5888,  -1.8891,  -1.0156,  -3.3190,  -2.9550],\n",
      "         [ -5.3403,  -9.1803,  -8.6396, -17.6866,  -0.8475],\n",
      "         [  0.2379,  -0.1890,  -8.9415,  -2.6413,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.51690834]\n",
      "[0.32208738]\n",
      "[0.26289084]\n",
      "[0.5283942]\n",
      "[0.19525998]\n",
      "[0.49129982]\n",
      "[0.1694067]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6805,  0.4406, -0.3171,  0.8583, -9.5466, -1.4022,  0.6088],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.3061, -20.1128,  -1.7299,  -0.3805],\n",
      "         [ -1.1192,  -3.3354, -10.4144,  -1.5197,  -1.2777],\n",
      "         [ -2.9225,  -2.0310, -72.7936,  -9.3572,  -2.3357],\n",
      "         [ -1.1343,  -1.0388,  -1.7385, -15.4982,  -2.8747],\n",
      "         [ -3.1955,  -1.8891,  -1.0156,  -3.3190,  -2.9550],\n",
      "         [ -5.5799,  -9.1803,  -8.6396, -17.6866,  -0.8475],\n",
      "         [  0.3381,  -0.1890,  -8.9415,  -2.6413,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.16722734]\n",
      "[1.01227756]\n",
      "[0.70057038]\n",
      "[1.06654775]\n",
      "[0.39109629]\n",
      "[0.60871071]\n",
      "[0.01950765]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.7626,  0.6268, -0.0332,  0.3281, -4.1906, -1.2206,  0.2983],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.1128,  -1.7299,  -0.3805],\n",
      "         [ -1.1192,  -3.2352, -10.4144,  -1.5197,  -1.2777],\n",
      "         [ -2.9225,  -2.0566, -72.7936,  -9.3572,  -2.3357],\n",
      "         [ -1.1343,  -0.9854,  -1.7385, -15.4982,  -2.8747],\n",
      "         [ -3.1955,  -2.6328,  -1.0156,  -3.3190,  -2.9550],\n",
      "         [ -5.5799,  -9.3941,  -8.6396, -17.6866,  -0.8475],\n",
      "         [  0.3381,  -0.1431,  -8.9415,  -2.6413,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.89446774]\n",
      "[1.0803753]\n",
      "[0.90863949]\n",
      "[0.78449876]\n",
      "[1.19923744]\n",
      "[1.01769571]\n",
      "[0.82837117]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -5.2869,  -2.6783,  -6.1776,  -1.6678,  -0.3701, -24.8489,  -4.3627],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -1.7299,  -0.3805],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.5197,  -1.2777],\n",
      "         [ -2.9225,  -2.0566, -73.8253,  -9.3572,  -2.3357],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -15.4982,  -2.8747],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -3.3190,  -2.9550],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.6866,  -0.8475],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -2.6413,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.80224752]\n",
      "[0.82240349]\n",
      "[0.40737712]\n",
      "[0.61016257]\n",
      "[0.84480228]\n",
      "[0.87678608]\n",
      "[0.49163966]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ -8.1717,  -0.6281, -31.2821,  -7.4206,  -5.4186,  -1.7935,  -3.8388],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -0.3805],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.2777],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -2.3357],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -2.8747],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9550],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -0.8475],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.3194]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.79953251]\n",
      "[0.93108516]\n",
      "[0.33224576]\n",
      "[0.97285615]\n",
      "[0.61338463]\n",
      "[0.79583521]\n",
      "[0.33812692]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([-6.7244, -0.9733, -5.6126, -1.5493, -0.0678, -4.6641, -0.8753],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06753412]\n",
      "[-0.00210624]\n",
      "[0.12207116]\n",
      "[1.73398405]\n",
      "[-0.00217742]\n",
      "[0.08100743]\n",
      "[0.09760005]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8407, 0.8562, 0.8567, 0.8610, 0.6903, 0.6076, 0.9493],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.1365,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1396,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1344,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1378,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1128,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0884,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1567,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.48678024]\n",
      "[0.02027401]\n",
      "[0.04018183]\n",
      "[0.8352402]\n",
      "[0.31689832]\n",
      "[0.04516649]\n",
      "[0.11265719]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7549, 0.4645, 0.7203, 0.8640, 0.6839, 0.7537, 0.9336],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.1365,   0.1224,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1396,   0.0748,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1344,   0.1160,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1378,   0.1406,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1128,   0.1114,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0884,   0.1194,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1567,   0.1537,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.02988786]\n",
      "[0.01539893]\n",
      "[0.02362704]\n",
      "[0.14436269]\n",
      "[0.03109449]\n",
      "[0.08020165]\n",
      "[0.09087409]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.3667,  0.4313,  0.2338,  0.5438,  0.5501, -0.2800,  0.9027],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 1.3647e-01,  1.2243e-01,  4.4471e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3964e-01,  7.4815e-02,  4.4581e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3443e-01,  1.1603e-01, -5.1384e-03,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3778e-01,  1.4057e-01,  5.8175e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1284e-01,  1.1141e-01,  7.3503e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 8.8413e-02,  1.1937e-01, -7.4798e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.5669e-01,  1.5369e-01,  1.4673e-01,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.08993642]\n",
      "[0.0593278]\n",
      "[0.07933902]\n",
      "[0.72921224]\n",
      "[0.03813742]\n",
      "[0.14108795]\n",
      "[0.20134647]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8749, 0.7657, 0.8487, 0.9103, 0.7130, 0.7869, 0.9638],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 1.3647e-01,  1.2243e-01,  4.4471e-02,  1.3846e-01,  0.0000e+00],\n",
      "         [ 1.3964e-01,  7.4815e-02,  4.4581e-02,  1.1398e-01,  0.0000e+00],\n",
      "         [ 1.3443e-01,  1.1603e-01, -5.1384e-03,  1.3440e-01,  0.0000e+00],\n",
      "         [ 1.3778e-01,  1.4057e-01,  5.8175e-02,  1.4664e-01,  0.0000e+00],\n",
      "         [ 1.1284e-01,  1.1141e-01,  7.3503e-02,  1.1276e-01,  0.0000e+00],\n",
      "         [ 8.8413e-02,  1.1937e-01, -7.4798e-02,  1.1775e-01,  0.0000e+00],\n",
      "         [ 1.5669e-01,  1.5369e-01,  1.4673e-01,  1.5928e-01,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.48877575]\n",
      "[0.28176903]\n",
      "[0.33036919]\n",
      "[0.64979236]\n",
      "[0.06505375]\n",
      "[0.33640006]\n",
      "[0.02875275]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7660, 0.7687, 0.5146, 0.7317, 0.6976, 0.7185, 0.9459],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 1.3647e-01,  1.2243e-01,  4.4471e-02,  1.3846e-01,  1.1880e-01],\n",
      "         [ 1.3964e-01,  7.4815e-02,  4.4581e-02,  1.1398e-01,  1.1427e-01],\n",
      "         [ 1.3443e-01,  1.1603e-01, -5.1384e-03,  1.3440e-01,  7.7610e-02],\n",
      "         [ 1.3778e-01,  1.4057e-01,  5.8175e-02,  1.4664e-01,  1.0748e-01],\n",
      "         [ 1.1284e-01,  1.1141e-01,  7.3503e-02,  1.1276e-01,  1.0674e-01],\n",
      "         [ 8.8413e-02,  1.1937e-01, -7.4798e-02,  1.1775e-01,  9.9964e-02],\n",
      "         [ 1.5669e-01,  1.5369e-01,  1.4673e-01,  1.5928e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.06462405]\n",
      "[0.02361037]\n",
      "[0.06712862]\n",
      "[0.81683831]\n",
      "[0.02210141]\n",
      "[0.10578768]\n",
      "[0.13772177]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.8296,  0.9020, -0.7002,  0.8115,  0.5256,  0.6164,  0.9526],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  1.2243e-01,  4.4471e-02,  1.3846e-01,  1.1880e-01],\n",
      "         [ 2.8038e-01,  7.4815e-02,  4.4581e-02,  1.1398e-01,  1.1427e-01],\n",
      "         [-3.2841e-03,  1.1603e-01, -5.1384e-03,  1.3440e-01,  7.7610e-02],\n",
      "         [ 2.7086e-01,  1.4057e-01,  5.8175e-02,  1.4664e-01,  1.0748e-01],\n",
      "         [ 1.7148e-01,  1.1141e-01,  7.3503e-02,  1.1276e-01,  1.0674e-01],\n",
      "         [ 1.8011e-01,  1.1937e-01, -7.4798e-02,  1.1775e-01,  9.9964e-02],\n",
      "         [ 3.1440e-01,  1.5369e-01,  1.4673e-01,  1.5928e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.26103902]\n",
      "[0.10827204]\n",
      "[0.58715234]\n",
      "[1.69695888]\n",
      "[0.23418756]\n",
      "[0.95062301]\n",
      "[0.04678736]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7548, 0.4550, 0.7461, 0.6447, 0.7267, 0.5476, 0.8277],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  2.3537e-01,  4.4471e-02,  1.3846e-01,  1.1880e-01],\n",
      "         [ 2.8038e-01,  1.3773e-01,  4.4581e-02,  1.1398e-01,  1.1427e-01],\n",
      "         [-3.2841e-03,  2.2478e-01, -5.1384e-03,  1.3440e-01,  7.7610e-02],\n",
      "         [ 2.7086e-01,  2.1950e-01,  5.8175e-02,  1.4664e-01,  1.0748e-01],\n",
      "         [ 1.7148e-01,  2.1585e-01,  7.3503e-02,  1.1276e-01,  1.0674e-01],\n",
      "         [ 1.8011e-01,  1.8685e-01, -7.4798e-02,  1.1775e-01,  9.9964e-02],\n",
      "         [ 3.1440e-01,  2.8409e-01,  1.4673e-01,  1.5928e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.04337353]\n",
      "[0.02727942]\n",
      "[0.02011746]\n",
      "[0.52408721]\n",
      "[0.00855577]\n",
      "[0.15192741]\n",
      "[0.06487831]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.0781,  0.3257, -0.0357,  0.7626,  0.2667, -0.5539,  0.7664],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  2.3537e-01,  3.3457e-02,  1.3846e-01,  1.1880e-01],\n",
      "         [ 2.8038e-01,  1.3773e-01,  8.0757e-02,  1.1398e-01,  1.1427e-01],\n",
      "         [-3.2841e-03,  2.2478e-01, -4.7600e-02,  1.3440e-01,  7.7610e-02],\n",
      "         [ 2.7086e-01,  2.1950e-01,  1.6940e-01,  1.4664e-01,  1.0748e-01],\n",
      "         [ 1.7148e-01,  2.1585e-01,  8.9606e-02,  1.1276e-01,  1.0674e-01],\n",
      "         [ 1.8011e-01,  1.8685e-01, -2.6397e-01,  1.1775e-01,  9.9964e-02],\n",
      "         [ 3.1440e-01,  2.8409e-01,  2.6808e-01,  1.5928e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[2.09295063]\n",
      "[1.18465052]\n",
      "[0.65564006]\n",
      "[1.70624804]\n",
      "[0.48156195]\n",
      "[0.94541316]\n",
      "[0.03833585]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.4250,  0.6683,  0.1773,  0.8247, -0.6545,  0.2295,  0.8998],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  2.3537e-01,  3.3457e-02,  1.5817e-01,  1.1880e-01],\n",
      "         [ 2.8038e-01,  1.3773e-01,  8.0757e-02,  1.8391e-01,  1.1427e-01],\n",
      "         [-3.2841e-03,  2.2478e-01, -4.7600e-02,  1.0439e-01,  7.7610e-02],\n",
      "         [ 2.7086e-01,  2.1950e-01,  1.6940e-01,  2.7009e-01,  1.0748e-01],\n",
      "         [ 1.7148e-01,  2.1585e-01,  8.9606e-02, -6.9823e-02,  1.0674e-01],\n",
      "         [ 1.8011e-01,  1.8685e-01, -2.6397e-01,  1.3101e-01,  9.9964e-02],\n",
      "         [ 3.1440e-01,  2.8409e-01,  2.6808e-01,  2.9374e-01,  1.5447e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.69875276]\n",
      "[1.60643547]\n",
      "[0.67117455]\n",
      "[1.59227076]\n",
      "[0.51694791]\n",
      "[0.78852496]\n",
      "[0.04686707]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6968, 0.7176, 0.6032, 0.7515, 0.6070, 0.5912, 0.9045],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 2.6966e-01,  2.3537e-01,  3.3457e-02,  1.5817e-01,  2.2860e-01],\n",
      "         [ 2.8038e-01,  1.3773e-01,  8.0757e-02,  1.8391e-01,  2.3139e-01],\n",
      "         [-3.2841e-03,  2.2478e-01, -4.7600e-02,  1.0439e-01,  1.5891e-01],\n",
      "         [ 2.7086e-01,  2.1950e-01,  1.6940e-01,  2.7009e-01,  2.2826e-01],\n",
      "         [ 1.7148e-01,  2.1585e-01,  8.9606e-02, -6.9823e-02,  1.9659e-01],\n",
      "         [ 1.8011e-01,  1.8685e-01, -2.6397e-01,  1.3101e-01,  1.8450e-01],\n",
      "         [ 3.1440e-01,  2.8409e-01,  2.6808e-01,  2.9374e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.83003266]\n",
      "[0.71995571]\n",
      "[0.61962114]\n",
      "[0.94127292]\n",
      "[0.51911389]\n",
      "[0.46526092]\n",
      "[0.24697493]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8382, 0.6012, 0.6842, 0.7191, 0.7742, 0.6989, 0.9555],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  2.3537e-01,  3.3457e-02,  1.5817e-01,  2.2860e-01],\n",
      "         [ 3.7544e-01,  1.3773e-01,  8.0757e-02,  1.8391e-01,  2.3139e-01],\n",
      "         [ 1.0124e-01,  2.2478e-01, -4.7600e-02,  1.0439e-01,  1.5891e-01],\n",
      "         [ 3.8276e-01,  2.1950e-01,  1.6940e-01,  2.7009e-01,  2.2826e-01],\n",
      "         [ 2.9787e-01,  2.1585e-01,  8.9606e-02, -6.9823e-02,  1.9659e-01],\n",
      "         [ 2.8277e-01,  1.8685e-01, -2.6397e-01,  1.3101e-01,  1.8450e-01],\n",
      "         [ 4.7174e-01,  2.8409e-01,  2.6808e-01,  2.9374e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.23707335]\n",
      "[0.10378951]\n",
      "[0.37566919]\n",
      "[1.41906959]\n",
      "[0.13401207]\n",
      "[0.77640194]\n",
      "[0.03246373]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8923, 0.8154, 0.8694, 0.9536, 0.6640, 0.8303, 0.9440],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  3.7885e-01,  3.3457e-02,  1.5817e-01,  2.2860e-01],\n",
      "         [ 3.7544e-01,  2.6410e-01,  8.0757e-02,  1.8391e-01,  2.3139e-01],\n",
      "         [ 1.0124e-01,  3.6795e-01, -4.7600e-02,  1.0439e-01,  1.5891e-01],\n",
      "         [ 3.8276e-01,  3.7623e-01,  1.6940e-01,  2.7009e-01,  2.2826e-01],\n",
      "         [ 2.9787e-01,  3.1166e-01,  8.9606e-02, -6.9823e-02,  1.9659e-01],\n",
      "         [ 2.8277e-01,  3.1650e-01, -2.6397e-01,  1.3101e-01,  1.8450e-01],\n",
      "         [ 4.7174e-01,  4.3976e-01,  2.6808e-01,  2.9374e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.57861342]\n",
      "[0.91600027]\n",
      "[0.25470186]\n",
      "[1.01955743]\n",
      "[0.22283571]\n",
      "[0.67522717]\n",
      "[0.0132134]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9437, 0.9495, 0.6459, 0.8545, 0.3448, 0.7684, 0.9483],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  3.7885e-01,  1.8725e-01,  1.5817e-01,  2.2860e-01],\n",
      "         [ 3.7544e-01,  2.6410e-01,  2.3618e-01,  1.8391e-01,  2.3139e-01],\n",
      "         [ 1.0124e-01,  3.6795e-01,  4.7080e-02,  1.0439e-01,  1.5891e-01],\n",
      "         [ 3.8276e-01,  3.7623e-01,  3.0460e-01,  2.7009e-01,  2.2826e-01],\n",
      "         [ 2.9787e-01,  3.1166e-01,  1.1607e-01, -6.9823e-02,  1.9659e-01],\n",
      "         [ 2.8277e-01,  3.1650e-01, -1.5410e-01,  1.3101e-01,  1.8450e-01],\n",
      "         [ 4.7174e-01,  4.3976e-01,  4.2270e-01,  2.9374e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.88341227]\n",
      "[0.99833322]\n",
      "[0.73622344]\n",
      "[0.95410813]\n",
      "[1.02881117]\n",
      "[1.0145263]\n",
      "[0.4934236]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8307, 0.1125, 0.6815, 0.8036, 0.3758, 0.7595, 0.9240],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  3.7885e-01,  1.8725e-01,  2.8369e-01,  2.2860e-01],\n",
      "         [ 3.7544e-01,  2.6410e-01,  2.3618e-01,  1.7121e-01,  2.3139e-01],\n",
      "         [ 1.0124e-01,  3.6795e-01,  4.7080e-02,  2.0329e-01,  1.5891e-01],\n",
      "         [ 3.8276e-01,  3.7623e-01,  3.0460e-01,  3.8475e-01,  2.2826e-01],\n",
      "         [ 2.9787e-01,  3.1166e-01,  1.1607e-01, -1.9391e-02,  1.9659e-01],\n",
      "         [ 2.8277e-01,  3.1650e-01, -1.5410e-01,  2.4041e-01,  1.8450e-01],\n",
      "         [ 4.7174e-01,  4.3976e-01,  4.2270e-01,  4.4218e-01,  3.0310e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.08075962]\n",
      "[0.04321774]\n",
      "[0.03718721]\n",
      "[1.5052973]\n",
      "[0.02310004]\n",
      "[0.22014848]\n",
      "[0.02568086]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6981, 0.6090, 0.9006, 0.8804, 0.7662, 0.6590, 0.8878],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0517e-01,  3.7885e-01,  1.8725e-01,  2.8369e-01,  3.2987e-01],\n",
      "         [ 3.7544e-01,  2.6410e-01,  2.3618e-01,  1.7121e-01,  3.2470e-01],\n",
      "         [ 1.0124e-01,  3.6795e-01,  4.7080e-02,  2.0329e-01,  2.9649e-01],\n",
      "         [ 3.8276e-01,  3.7623e-01,  3.0460e-01,  3.8475e-01,  3.6533e-01],\n",
      "         [ 2.9787e-01,  3.1166e-01,  1.1607e-01, -1.9391e-02,  3.1390e-01],\n",
      "         [ 2.8277e-01,  3.1650e-01, -1.5410e-01,  2.4041e-01,  2.7867e-01],\n",
      "         [ 4.7174e-01,  4.3976e-01,  4.2270e-01,  4.4218e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.82060382]\n",
      "[1.41383176]\n",
      "[0.8076784]\n",
      "[1.53998309]\n",
      "[0.17211368]\n",
      "[0.56394629]\n",
      "[0.02078767]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.1684,  0.6066, -0.7596,  0.5446,  0.6013, -0.7934,  0.9494],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  3.7885e-01,  1.8725e-01,  2.8369e-01,  3.2987e-01],\n",
      "         [ 4.4917e-01,  2.6410e-01,  2.3618e-01,  1.7121e-01,  3.2470e-01],\n",
      "         [-6.1993e-02,  3.6795e-01,  4.7080e-02,  2.0329e-01,  2.9649e-01],\n",
      "         [ 4.5284e-01,  3.7623e-01,  3.0460e-01,  3.8475e-01,  3.6533e-01],\n",
      "         [ 3.9020e-01,  3.1166e-01,  1.1607e-01, -1.9391e-02,  3.1390e-01],\n",
      "         [ 1.2561e-01,  3.1650e-01, -1.5410e-01,  2.4041e-01,  2.7867e-01],\n",
      "         [ 6.2812e-01,  4.3976e-01,  4.2270e-01,  4.4218e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.04141875]\n",
      "[0.10675017]\n",
      "[0.01571136]\n",
      "[0.24378251]\n",
      "[0.03251447]\n",
      "[0.02379043]\n",
      "[0.88771597]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.5111, 0.6064, 0.4446, 0.5019, 0.6149, 0.5240, 0.9459],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  4.4555e-01,  1.8725e-01,  2.8369e-01,  3.2987e-01],\n",
      "         [ 4.4917e-01,  3.4504e-01,  2.3618e-01,  1.7121e-01,  3.2470e-01],\n",
      "         [-6.1993e-02,  4.3197e-01,  4.7080e-02,  2.0329e-01,  2.9649e-01],\n",
      "         [ 4.5284e-01,  4.4103e-01,  3.0460e-01,  3.8475e-01,  3.6533e-01],\n",
      "         [ 3.9020e-01,  4.0261e-01,  1.1607e-01, -1.9391e-02,  3.1390e-01],\n",
      "         [ 1.2561e-01,  3.8834e-01, -1.5410e-01,  2.4041e-01,  2.7867e-01],\n",
      "         [ 6.2812e-01,  5.9563e-01,  4.2270e-01,  4.4218e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.05137508]\n",
      "[0.02211621]\n",
      "[0.05135512]\n",
      "[0.33266055]\n",
      "[0.00455116]\n",
      "[0.07750037]\n",
      "[0.13797808]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.2987, 0.8613, 0.4983, 0.7361, 0.5682, 0.2564, 0.9723],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  4.4555e-01,  2.2406e-01,  2.8369e-01,  3.2987e-01],\n",
      "         [ 4.4917e-01,  3.4504e-01,  3.6306e-01,  1.7121e-01,  3.2470e-01],\n",
      "         [-6.1993e-02,  4.3197e-01,  9.6470e-02,  2.0329e-01,  2.9649e-01],\n",
      "         [ 4.5284e-01,  4.4103e-01,  4.1571e-01,  3.8475e-01,  3.6533e-01],\n",
      "         [ 3.9020e-01,  4.0261e-01,  1.9738e-01, -1.9391e-02,  3.1390e-01],\n",
      "         [ 1.2561e-01,  3.8834e-01, -1.7137e-01,  2.4041e-01,  2.7867e-01],\n",
      "         [ 6.2812e-01,  5.9563e-01,  5.8261e-01,  4.4218e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.07082808]\n",
      "[0.20255007]\n",
      "[0.01985003]\n",
      "[0.45526603]\n",
      "[0.03528001]\n",
      "[0.00630587]\n",
      "[0.96372545]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6672, -0.0646,  0.5902,  0.7543,  0.8379,  0.7791,  0.8015],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  4.4555e-01,  2.2406e-01,  3.7162e-01,  3.2987e-01],\n",
      "         [ 4.4917e-01,  3.4504e-01,  3.6306e-01,  7.6302e-02,  3.2470e-01],\n",
      "         [-6.1993e-02,  4.3197e-01,  9.6470e-02,  2.8790e-01,  2.9649e-01],\n",
      "         [ 4.5284e-01,  4.4103e-01,  4.1571e-01,  5.0056e-01,  3.6533e-01],\n",
      "         [ 3.9020e-01,  4.0261e-01,  1.9738e-01,  1.0706e-01,  3.1390e-01],\n",
      "         [ 1.2561e-01,  3.8834e-01, -1.7137e-01,  3.5075e-01,  2.7867e-01],\n",
      "         [ 6.2812e-01,  5.9563e-01,  5.8261e-01,  5.6287e-01,  4.4730e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.12248166]\n",
      "[0.14522436]\n",
      "[0.05182291]\n",
      "[0.25815277]\n",
      "[0.0280159]\n",
      "[0.32991433]\n",
      "[0.05299037]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6561,  0.7694,  0.5410, -0.3339,  0.5072,  0.4655,  0.9534],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 4.0990e-01,  4.4555e-01,  2.2406e-01,  3.7162e-01,  4.2975e-01],\n",
      "         [ 4.4917e-01,  3.4504e-01,  3.6306e-01,  7.6302e-02,  4.4281e-01],\n",
      "         [-6.1993e-02,  4.3197e-01,  9.6470e-02,  2.8790e-01,  3.6956e-01],\n",
      "         [ 4.5284e-01,  4.4103e-01,  4.1571e-01,  5.0056e-01,  2.8579e-01],\n",
      "         [ 3.9020e-01,  4.0261e-01,  1.9738e-01,  1.0706e-01,  3.7616e-01],\n",
      "         [ 1.2561e-01,  3.8834e-01, -1.7137e-01,  3.5075e-01,  3.3729e-01],\n",
      "         [ 6.2812e-01,  5.9563e-01,  5.8261e-01,  5.6287e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.09488247]\n",
      "[0.01692163]\n",
      "[0.08421654]\n",
      "[0.41055438]\n",
      "[0.01690857]\n",
      "[0.20595028]\n",
      "[0.09394705]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7798, 0.8675, 0.5377, 0.6201, 0.6789, 0.7009, 0.9577],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  4.4555e-01,  2.2406e-01,  3.7162e-01,  4.2975e-01],\n",
      "         [ 5.8241e-01,  3.4504e-01,  3.6306e-01,  7.6302e-02,  4.4281e-01],\n",
      "         [-9.4632e-03,  4.3197e-01,  9.6470e-02,  2.8790e-01,  3.6956e-01],\n",
      "         [ 5.4544e-01,  4.4103e-01,  4.1571e-01,  5.0056e-01,  2.8579e-01],\n",
      "         [ 4.8687e-01,  4.0261e-01,  1.9738e-01,  1.0706e-01,  3.7616e-01],\n",
      "         [ 2.2615e-01,  3.8834e-01, -1.7137e-01,  3.5075e-01,  3.3729e-01],\n",
      "         [ 7.8415e-01,  5.9563e-01,  5.8261e-01,  5.6287e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.09812788]\n",
      "[0.25023948]\n",
      "[0.01823575]\n",
      "[0.3286931]\n",
      "[0.09067584]\n",
      "[0.10988243]\n",
      "[0.82384686]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8174, 0.7679, 0.0915, 0.7960, 0.6468, 0.5616, 0.8881],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  5.7165e-01,  2.2406e-01,  3.7162e-01,  4.2975e-01],\n",
      "         [ 5.8241e-01,  4.5116e-01,  3.6306e-01,  7.6302e-02,  4.4281e-01],\n",
      "         [-9.4632e-03,  3.9527e-01,  9.6470e-02,  2.8790e-01,  3.6956e-01],\n",
      "         [ 5.4544e-01,  5.6237e-01,  4.1571e-01,  5.0056e-01,  2.8579e-01],\n",
      "         [ 4.8687e-01,  4.9784e-01,  1.9738e-01,  1.0706e-01,  3.7616e-01],\n",
      "         [ 2.2615e-01,  4.6219e-01, -1.7137e-01,  3.5075e-01,  3.3729e-01],\n",
      "         [ 7.8415e-01,  7.3988e-01,  5.8261e-01,  5.6287e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.09028549]\n",
      "[0.12490922]\n",
      "[0.18185536]\n",
      "[0.56804216]\n",
      "[1.76755795]\n",
      "[1.18830307]\n",
      "[0.33671865]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6760,  0.8462, -1.7127,  0.8133,  0.8059, -0.0206,  0.9167],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  5.7165e-01,  3.2971e-01,  3.7162e-01,  4.2975e-01],\n",
      "         [ 5.8241e-01,  4.5116e-01,  4.8956e-01,  7.6302e-02,  4.4281e-01],\n",
      "         [-9.4632e-03,  3.9527e-01, -3.2967e-01,  2.8790e-01,  3.6956e-01],\n",
      "         [ 5.4544e-01,  5.6237e-01,  5.4350e-01,  5.0056e-01,  2.8579e-01],\n",
      "         [ 4.8687e-01,  4.9784e-01,  3.1981e-01,  1.0706e-01,  3.7616e-01],\n",
      "         [ 2.2615e-01,  4.6219e-01, -2.3961e-01,  3.5075e-01,  3.3729e-01],\n",
      "         [ 7.8415e-01,  7.3988e-01,  7.3260e-01,  5.6287e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.6926658]\n",
      "[0.03797917]\n",
      "[0.05775997]\n",
      "[0.5446889]\n",
      "[1.20985186]\n",
      "[-0.20198648]\n",
      "[0.16475993]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.3742,  0.4867, -0.3117,  0.5371,  0.7218,  0.5683,  0.8573],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  5.7165e-01,  3.2971e-01,  4.0932e-01,  4.2975e-01],\n",
      "         [ 5.8241e-01,  4.5116e-01,  4.8956e-01,  1.4858e-01,  4.4281e-01],\n",
      "         [-9.4632e-03,  3.9527e-01, -3.2967e-01,  2.0320e-01,  3.6956e-01],\n",
      "         [ 5.4544e-01,  5.6237e-01,  5.4350e-01,  5.7459e-01,  2.8579e-01],\n",
      "         [ 4.8687e-01,  4.9784e-01,  3.1981e-01,  2.1596e-01,  3.7616e-01],\n",
      "         [ 2.2615e-01,  4.6219e-01, -2.3961e-01,  4.3001e-01,  3.3729e-01],\n",
      "         [ 7.8415e-01,  7.3988e-01,  7.3260e-01,  7.0037e-01,  6.0314e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.44105107]\n",
      "[0.0960249]\n",
      "[0.24474813]\n",
      "[0.56819853]\n",
      "[0.25665673]\n",
      "[0.35245889]\n",
      "[0.05094847]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6629, 0.7896, 0.0682, 0.7391, 0.7131, 0.4323, 0.8616],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 5.2571e-01,  5.7165e-01,  3.2971e-01,  4.0932e-01,  5.2554e-01],\n",
      "         [ 5.8241e-01,  4.5116e-01,  4.8956e-01,  1.4858e-01,  5.6294e-01],\n",
      "         [-9.4632e-03,  3.9527e-01, -3.2967e-01,  2.0320e-01,  3.0674e-01],\n",
      "         [ 5.4544e-01,  5.6237e-01,  5.4350e-01,  5.7459e-01,  3.8597e-01],\n",
      "         [ 4.8687e-01,  4.9784e-01,  3.1981e-01,  2.1596e-01,  4.7452e-01],\n",
      "         [ 2.2615e-01,  4.6219e-01, -2.3961e-01,  4.3001e-01,  3.7914e-01],\n",
      "         [ 7.8415e-01,  7.3988e-01,  7.3260e-01,  7.0037e-01,  7.4204e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.91908208]\n",
      "[0.12446064]\n",
      "[0.16673474]\n",
      "[0.61202213]\n",
      "[0.93795764]\n",
      "[1.11684642]\n",
      "[0.29967482]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6715,  0.3942,  0.6644,  0.7423, -0.2034,  0.1459,  0.9222],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.5716,   0.3297,   0.4093,   0.5255],\n",
      "         [  0.6315,   0.4512,   0.4896,   0.1486,   0.5629],\n",
      "         [  0.0954,   0.3953,  -0.3297,   0.2032,   0.3067],\n",
      "         [  0.6531,   0.5624,   0.5435,   0.5746,   0.3860],\n",
      "         [  0.4433,   0.4978,   0.3198,   0.2160,   0.4745],\n",
      "         [  0.2356,   0.4622,  -0.2396,   0.4300,   0.3791],\n",
      "         [  0.9316,   0.7399,   0.7326,   0.7004,   0.7420]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.23511732]\n",
      "[0.17017639]\n",
      "[0.13213762]\n",
      "[0.51857252]\n",
      "[0.05148525]\n",
      "[0.71146455]\n",
      "[0.23378105]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9372, 0.8511, 0.6386, 0.9472, 0.7328, 0.8174, 0.9275],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.3297,   0.4093,   0.5255],\n",
      "         [  0.6315,   0.5897,   0.4896,   0.1486,   0.5629],\n",
      "         [  0.0954,   0.4934,  -0.3297,   0.2032,   0.3067],\n",
      "         [  0.6531,   0.7175,   0.5435,   0.5746,   0.3860],\n",
      "         [  0.4433,   0.5996,   0.3198,   0.2160,   0.4745],\n",
      "         [  0.2356,   0.5903,  -0.2396,   0.4300,   0.3791],\n",
      "         [  0.9316,   0.8929,   0.7326,   0.7004,   0.7420]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.30210599]\n",
      "[0.15081581]\n",
      "[0.34926467]\n",
      "[1.53539121]\n",
      "[0.22717496]\n",
      "[0.86608779]\n",
      "[0.09347999]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7161, 0.5245, 0.7043, 0.4398, 0.4714, 0.6571, 0.8375],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.4093,   0.5255],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.1486,   0.5629],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.2032,   0.3067],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.5746,   0.3860],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2160,   0.4745],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.4300,   0.3791],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.7004,   0.7420]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.09138154]\n",
      "[0.03540537]\n",
      "[0.08780202]\n",
      "[1.20354667]\n",
      "[0.02027068]\n",
      "[0.30542021]\n",
      "[0.13182698]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7606, 0.6734, 0.8773, 0.8804, 0.2730, 0.7063, 0.8900],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.5255],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.5629],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.3067],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.3860],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.4745],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.3791],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.7420]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.48618612]\n",
      "[1.9342872]\n",
      "[0.62206129]\n",
      "[1.58279976]\n",
      "[0.37614649]\n",
      "[1.09738457]\n",
      "[0.05123414]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8968, 0.8808, 0.8821, 0.9282, 0.8509, 0.7390, 0.9516],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.98745344]\n",
      "[0.78818604]\n",
      "[0.62929433]\n",
      "[0.72426986]\n",
      "[0.83692937]\n",
      "[0.98002925]\n",
      "[0.07136138]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8659, 0.7132, 0.8775, 0.9047, 0.8301, 0.8224, 0.9806],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.1400,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1076,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1423,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1454,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1329,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1208,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1623,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.51240692]\n",
      "[0.24831183]\n",
      "[0.37468364]\n",
      "[0.58364706]\n",
      "[0.10586649]\n",
      "[0.33473775]\n",
      "[0.02583257]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.4312, 0.6101, 0.5821, 0.7286, 0.5345, 0.0747, 0.9264],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 1.4004e-01,  6.5241e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0755e-01,  9.0513e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4231e-01,  8.8764e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4543e-01,  1.1111e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3288e-01,  8.4816e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.2082e-01, -9.2914e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.6235e-01,  1.5242e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.27885423]\n",
      "[0.30074886]\n",
      "[0.17229]\n",
      "[0.51758273]\n",
      "[-0.00279943]\n",
      "[0.1460552]\n",
      "[0.02331558]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6766, 0.4777, 0.5617, 0.8664, 0.7949, 0.6991, 0.9361],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 1.4004e-01,  6.5241e-02,  1.0238e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0755e-01,  9.0513e-02,  6.3331e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4231e-01,  8.8764e-02,  8.5693e-02,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.4543e-01,  1.1111e-01,  1.3813e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3288e-01,  8.4816e-02,  1.2298e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.2082e-01, -9.2914e-03,  1.0193e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.6235e-01,  1.5242e-01,  1.5237e-01,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.42902482]\n",
      "[0.37873339]\n",
      "[0.21442044]\n",
      "[0.53883205]\n",
      "[0.09856434]\n",
      "[0.27081597]\n",
      "[0.02164655]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8972, 0.9246, 0.8116, 0.8852, 0.8163, 0.5208, 0.9693],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 1.4004e-01,  6.5241e-02,  1.0238e-01,  1.4640e-01,  0.0000e+00],\n",
      "         [ 1.0755e-01,  9.0513e-02,  6.3331e-02,  1.5045e-01,  0.0000e+00],\n",
      "         [ 1.4231e-01,  8.8764e-02,  8.5693e-02,  1.2881e-01,  0.0000e+00],\n",
      "         [ 1.4543e-01,  1.1111e-01,  1.3813e-01,  1.4534e-01,  0.0000e+00],\n",
      "         [ 1.3288e-01,  8.4816e-02,  1.2298e-01,  1.2649e-01,  0.0000e+00],\n",
      "         [ 1.2082e-01, -9.2914e-03,  1.0193e-01,  7.3380e-02,  0.0000e+00],\n",
      "         [ 1.6235e-01,  1.5242e-01,  1.5237e-01,  1.5963e-01,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.77972167]\n",
      "[0.04185706]\n",
      "[0.05504222]\n",
      "[0.85006257]\n",
      "[0.45297707]\n",
      "[0.23292363]\n",
      "[0.17048108]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7586, 0.8113, 0.6494, 0.6613, 0.7570, 0.7395, 0.9407],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 1.4004e-01,  6.5241e-02,  1.0238e-01,  1.4640e-01,  1.1330e-01],\n",
      "         [ 1.0755e-01,  9.0513e-02,  6.3331e-02,  1.5045e-01,  1.2119e-01],\n",
      "         [ 1.4231e-01,  8.8764e-02,  8.5693e-02,  1.2881e-01,  9.0697e-02],\n",
      "         [ 1.4543e-01,  1.1111e-01,  1.3813e-01,  1.4534e-01,  6.9593e-02],\n",
      "         [ 1.3288e-01,  8.4816e-02,  1.2298e-01,  1.2649e-01,  1.1953e-01],\n",
      "         [ 1.2082e-01, -9.2914e-03,  1.0193e-01,  7.3380e-02,  1.1181e-01],\n",
      "         [ 1.6235e-01,  1.5242e-01,  1.5237e-01,  1.5963e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.08310563]\n",
      "[0.02224516]\n",
      "[0.11186942]\n",
      "[0.93537436]\n",
      "[0.02683095]\n",
      "[0.10724435]\n",
      "[0.2124614]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8818, 0.8916, 0.6957, 0.8023, 0.7004, 0.7234, 0.9560],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 2.6356e-01,  6.5241e-02,  1.0238e-01,  1.4640e-01,  1.1330e-01],\n",
      "         [ 2.5199e-01,  9.0513e-02,  6.3331e-02,  1.5045e-01,  1.2119e-01],\n",
      "         [ 1.7720e-01,  8.8764e-02,  8.5693e-02,  1.2881e-01,  9.0697e-02],\n",
      "         [ 2.7097e-01,  1.1111e-01,  1.3813e-01,  1.4534e-01,  6.9593e-02],\n",
      "         [ 1.9785e-01,  8.4816e-02,  1.2298e-01,  1.2649e-01,  1.1953e-01],\n",
      "         [ 1.3700e-01, -9.2914e-03,  1.0193e-01,  7.3380e-02,  1.1181e-01],\n",
      "         [ 3.1763e-01,  1.5242e-01,  1.5237e-01,  1.5963e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.96829841]\n",
      "[0.67973381]\n",
      "[0.86813464]\n",
      "[1.03659005]\n",
      "[0.79922714]\n",
      "[0.90091577]\n",
      "[0.36533729]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7631, 0.8514, 0.7868, 0.8971, 0.8165, 0.6077, 0.8484],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 2.6356e-01,  1.8651e-01,  1.0238e-01,  1.4640e-01,  1.1330e-01],\n",
      "         [ 2.5199e-01,  2.2641e-01,  6.3331e-02,  1.5045e-01,  1.2119e-01],\n",
      "         [ 1.7720e-01,  2.0662e-01,  8.5693e-02,  1.2881e-01,  9.0697e-02],\n",
      "         [ 2.7097e-01,  2.5579e-01,  1.3813e-01,  1.4534e-01,  6.9593e-02],\n",
      "         [ 1.9785e-01,  2.1325e-01,  1.2298e-01,  1.2649e-01,  1.1953e-01],\n",
      "         [ 1.3700e-01,  7.7273e-02,  1.0193e-01,  7.3380e-02,  1.1181e-01],\n",
      "         [ 3.1763e-01,  2.9208e-01,  1.5237e-01,  1.5963e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.99911411]\n",
      "[0.83352295]\n",
      "[0.68580816]\n",
      "[1.03452712]\n",
      "[0.62724668]\n",
      "[0.81040276]\n",
      "[0.45060351]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8535, 0.9623, 0.6337, 0.9282, 0.8549, 0.7184, 0.9483],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 2.6356e-01,  1.8651e-01,  2.3341e-01,  1.4640e-01,  1.1330e-01],\n",
      "         [ 2.5199e-01,  2.2641e-01,  2.1703e-01,  1.5045e-01,  1.2119e-01],\n",
      "         [ 1.7720e-01,  2.0662e-01,  1.4587e-01,  1.2881e-01,  9.0697e-02],\n",
      "         [ 2.7097e-01,  2.5579e-01,  2.8692e-01,  1.4534e-01,  6.9593e-02],\n",
      "         [ 1.9785e-01,  2.1325e-01,  2.5000e-01,  1.2649e-01,  1.1953e-01],\n",
      "         [ 1.3700e-01,  7.7273e-02,  1.4833e-01,  7.3380e-02,  1.1181e-01],\n",
      "         [ 3.1763e-01,  2.9208e-01,  3.0503e-01,  1.5963e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.99008054]\n",
      "[0.75955093]\n",
      "[0.90181191]\n",
      "[1.08290073]\n",
      "[0.66908586]\n",
      "[0.87377897]\n",
      "[0.85579921]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6256, 0.5669, 0.5602, 0.7934, 0.6893, 0.7779, 0.8514],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 2.6356e-01,  1.8651e-01,  2.3341e-01,  2.2532e-01,  1.1330e-01],\n",
      "         [ 2.5199e-01,  2.2641e-01,  2.1703e-01,  2.1723e-01,  1.2119e-01],\n",
      "         [ 1.7720e-01,  2.0662e-01,  1.4587e-01,  1.7655e-01,  9.0697e-02],\n",
      "         [ 2.7097e-01,  2.5579e-01,  2.8692e-01,  2.5699e-01,  6.9593e-02],\n",
      "         [ 1.9785e-01,  2.1325e-01,  2.5000e-01,  2.1505e-01,  1.1953e-01],\n",
      "         [ 1.3700e-01,  7.7273e-02,  1.4833e-01,  1.7990e-01,  1.1181e-01],\n",
      "         [ 3.1763e-01,  2.9208e-01,  3.0503e-01,  2.9089e-01,  1.4454e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.13538554]\n",
      "[0.05541719]\n",
      "[0.08774401]\n",
      "[0.63937453]\n",
      "[0.02304463]\n",
      "[0.25712527]\n",
      "[0.12494203]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7696, 0.8001, 0.7304, 0.8206, 0.6600, 0.6552, 0.8729],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.2636,   0.1865,   0.2334,   0.2253,   0.2177],\n",
      "         [  0.2520,   0.2264,   0.2170,   0.2172,   0.2298],\n",
      "         [  0.1772,   0.2066,   0.1459,   0.1765,   0.1744],\n",
      "         [  0.2710,   0.2558,   0.2869,   0.2570,   0.1867],\n",
      "         [  0.1978,   0.2132,   0.2500,   0.2150,   0.2054],\n",
      "         [  0.1370,   0.0773,   0.1483,   0.1799,   0.1856],\n",
      "         [  0.3176,   0.2921,   0.3050,   0.2909,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.17571526]\n",
      "[0.1534348]\n",
      "[0.0693115]\n",
      "[1.2772532]\n",
      "[0.01043046]\n",
      "[0.35402076]\n",
      "[0.09168679]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8970, 0.8681, 0.8340, 0.8871, 0.7740, 0.8329, 0.9772],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.1865,   0.2334,   0.2253,   0.2177],\n",
      "         [  0.3871,   0.2264,   0.2170,   0.2172,   0.2298],\n",
      "         [  0.3070,   0.2066,   0.1459,   0.1765,   0.1744],\n",
      "         [  0.4114,   0.2558,   0.2869,   0.2570,   0.1867],\n",
      "         [  0.3121,   0.2132,   0.2500,   0.2150,   0.2054],\n",
      "         [  0.2624,   0.0773,   0.1483,   0.1799,   0.1856],\n",
      "         [  0.4756,   0.2921,   0.3050,   0.2909,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.15982503]\n",
      "[0.13531295]\n",
      "[0.05424655]\n",
      "[0.72425222]\n",
      "[0.03033151]\n",
      "[0.47158145]\n",
      "[0.06093718]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9358, 0.9520, 0.8415, 0.9765, 0.8965, 0.8365, 0.9491],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.3402,   0.2334,   0.2253,   0.2177],\n",
      "         [  0.3871,   0.3832,   0.2170,   0.2172,   0.2298],\n",
      "         [  0.3070,   0.3420,   0.1459,   0.1765,   0.1744],\n",
      "         [  0.4114,   0.4168,   0.2869,   0.2570,   0.1867],\n",
      "         [  0.3121,   0.3558,   0.2500,   0.2150,   0.2054],\n",
      "         [  0.2624,   0.2041,   0.1483,   0.1799,   0.1856],\n",
      "         [  0.4756,   0.4486,   0.3050,   0.2909,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.85899958]\n",
      "[1.03022344]\n",
      "[0.87044259]\n",
      "[0.85259463]\n",
      "[0.87653842]\n",
      "[0.66081174]\n",
      "[0.30186609]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9388, 0.8896, 0.8950, 0.9436, 0.7592, 0.7619, 0.9562],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.3402,   0.3854,   0.2253,   0.2177],\n",
      "         [  0.3871,   0.3832,   0.3581,   0.2172,   0.2298],\n",
      "         [  0.3070,   0.3420,   0.2874,   0.1765,   0.1744],\n",
      "         [  0.4114,   0.4168,   0.4361,   0.2570,   0.1867],\n",
      "         [  0.3121,   0.3558,   0.3587,   0.2150,   0.2054],\n",
      "         [  0.2624,   0.2041,   0.2628,   0.1799,   0.1856],\n",
      "         [  0.4756,   0.4486,   0.4633,   0.2909,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.50231991]\n",
      "[0.24500941]\n",
      "[0.22639448]\n",
      "[1.32753334]\n",
      "[0.17693265]\n",
      "[0.53469883]\n",
      "[0.02546377]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9460, 0.8948, 0.9334, 0.9670, 0.8605, 0.7076, 0.9725],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.3402,   0.3854,   0.3807,   0.2177],\n",
      "         [  0.3871,   0.3832,   0.3581,   0.3640,   0.2298],\n",
      "         [  0.3070,   0.3420,   0.2874,   0.3293,   0.1744],\n",
      "         [  0.4114,   0.4168,   0.4361,   0.4156,   0.1867],\n",
      "         [  0.3121,   0.3558,   0.3587,   0.3488,   0.2054],\n",
      "         [  0.2624,   0.2041,   0.2628,   0.2903,   0.1856],\n",
      "         [  0.4756,   0.4486,   0.4633,   0.4507,   0.2809]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.14262296]\n",
      "[1.01019498]\n",
      "[1.08864671]\n",
      "[1.10042053]\n",
      "[0.96874237]\n",
      "[0.93047191]\n",
      "[0.13427213]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9035, 0.8833, 0.8340, 0.8959, 0.6427, 0.8438, 0.9515],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4043,   0.3402,   0.3854,   0.3807,   0.3661],\n",
      "         [  0.3871,   0.3832,   0.3581,   0.3640,   0.3671],\n",
      "         [  0.3070,   0.3420,   0.2874,   0.3293,   0.3059],\n",
      "         [  0.4114,   0.4168,   0.4361,   0.4156,   0.3315],\n",
      "         [  0.3121,   0.3558,   0.3587,   0.3488,   0.2858],\n",
      "         [  0.2624,   0.2041,   0.2628,   0.2903,   0.3121],\n",
      "         [  0.4756,   0.4486,   0.4633,   0.4507,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.35075494]\n",
      "[0.41606634]\n",
      "[0.5565349]\n",
      "[1.31332552]\n",
      "[0.34563085]\n",
      "[0.6074622]\n",
      "[0.04450679]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7846, 0.4582, 0.6865, 0.8027, 0.7330, 0.7809, 0.9141],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.3402,   0.3854,   0.3807,   0.3661],\n",
      "         [  0.3986,   0.3832,   0.3581,   0.3640,   0.3671],\n",
      "         [  0.3874,   0.3420,   0.2874,   0.3293,   0.3059],\n",
      "         [  0.5159,   0.4168,   0.4361,   0.4156,   0.3315],\n",
      "         [  0.4087,   0.3558,   0.3587,   0.3488,   0.2858],\n",
      "         [  0.3332,   0.2041,   0.2628,   0.2903,   0.3121],\n",
      "         [  0.6017,   0.4486,   0.4633,   0.4507,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.14152322]\n",
      "[0.06740237]\n",
      "[0.1364127]\n",
      "[0.56601328]\n",
      "[0.05883992]\n",
      "[0.19506315]\n",
      "[0.22509392]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8828, 0.7505, 0.7856, 0.7402, 0.7088, 0.7364, 0.9298],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.4837,   0.3854,   0.3807,   0.3661],\n",
      "         [  0.3986,   0.4947,   0.3581,   0.3640,   0.3671],\n",
      "         [  0.3874,   0.4637,   0.2874,   0.3293,   0.3059],\n",
      "         [  0.5159,   0.5298,   0.4361,   0.4156,   0.3315],\n",
      "         [  0.4087,   0.4644,   0.3587,   0.3488,   0.2858],\n",
      "         [  0.3332,   0.3144,   0.2628,   0.2903,   0.3121],\n",
      "         [  0.6017,   0.6014,   0.4633,   0.4507,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.0279155]\n",
      "[0.01489329]\n",
      "[0.04554824]\n",
      "[0.36788654]\n",
      "[0.0337967]\n",
      "[0.08192811]\n",
      "[0.13182323]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8284, 0.8763, 0.6891, 0.7373, 0.8632, 0.7571, 0.9387],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.4837,   0.5093,   0.3807,   0.3661],\n",
      "         [  0.3986,   0.4947,   0.4759,   0.3640,   0.3671],\n",
      "         [  0.3874,   0.4637,   0.3802,   0.3293,   0.3059],\n",
      "         [  0.5159,   0.5298,   0.5389,   0.4156,   0.3315],\n",
      "         [  0.4087,   0.4644,   0.4875,   0.3488,   0.2858],\n",
      "         [  0.3332,   0.3144,   0.3737,   0.2903,   0.3121],\n",
      "         [  0.6017,   0.6014,   0.6141,   0.4507,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06113793]\n",
      "[0.02529342]\n",
      "[0.08884058]\n",
      "[0.6074095]\n",
      "[0.02386796]\n",
      "[0.11954872]\n",
      "[0.16155103]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6010, 0.8621, 0.6451, 0.7577, 0.8380, 0.6135, 0.9496],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.4837,   0.5093,   0.4698,   0.3661],\n",
      "         [  0.3986,   0.4947,   0.4759,   0.5020,   0.3671],\n",
      "         [  0.3874,   0.4637,   0.3802,   0.4071,   0.3059],\n",
      "         [  0.5159,   0.5298,   0.5389,   0.5290,   0.3315],\n",
      "         [  0.4087,   0.4644,   0.4875,   0.4785,   0.2858],\n",
      "         [  0.3332,   0.3144,   0.3737,   0.3789,   0.3121],\n",
      "         [  0.6017,   0.6014,   0.6141,   0.6056,   0.4377]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.08160334]\n",
      "[0.17058998]\n",
      "[0.01659732]\n",
      "[0.28581403]\n",
      "[0.0656862]\n",
      "[0.08260006]\n",
      "[0.89464241]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8139, 0.6923, 0.6506, 0.7958, 0.6478, 0.7022, 0.8843],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.4987,   0.4837,   0.5093,   0.4698,   0.4879],\n",
      "         [  0.3986,   0.4947,   0.4759,   0.5020,   0.4544],\n",
      "         [  0.3874,   0.4637,   0.3802,   0.4071,   0.3650],\n",
      "         [  0.5159,   0.5298,   0.5389,   0.5290,   0.4427],\n",
      "         [  0.4087,   0.4644,   0.4875,   0.4785,   0.3643],\n",
      "         [  0.3332,   0.3144,   0.3737,   0.3789,   0.4093],\n",
      "         [  0.6017,   0.6014,   0.6141,   0.6056,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.13193817]\n",
      "[0.08498847]\n",
      "[0.14848625]\n",
      "[0.58358746]\n",
      "[0.05535254]\n",
      "[0.14810907]\n",
      "[0.27001837]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8899, 0.8646, 0.6525, 0.6838, 0.6684, 0.7922, 0.9455],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.4837,   0.5093,   0.4698,   0.4879],\n",
      "         [  0.5314,   0.4947,   0.4759,   0.5020,   0.4544],\n",
      "         [  0.4564,   0.4637,   0.3802,   0.4071,   0.3650],\n",
      "         [  0.5921,   0.5298,   0.5389,   0.5290,   0.4427],\n",
      "         [  0.4965,   0.4644,   0.4875,   0.4785,   0.3643],\n",
      "         [  0.4566,   0.3144,   0.3737,   0.3789,   0.4093],\n",
      "         [  0.7567,   0.6014,   0.6141,   0.6056,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.59411328]\n",
      "[0.18380329]\n",
      "[0.21927824]\n",
      "[0.63189389]\n",
      "[0.27842823]\n",
      "[0.42977049]\n",
      "[0.11110902]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7495, 0.7657, 0.4465, 0.6535, 0.7659, 0.7676, 0.9555],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.5851,   0.5093,   0.4698,   0.4879],\n",
      "         [  0.5314,   0.5837,   0.4759,   0.5020,   0.4544],\n",
      "         [  0.4564,   0.5093,   0.3802,   0.4071,   0.3650],\n",
      "         [  0.5921,   0.6076,   0.5389,   0.5290,   0.4427],\n",
      "         [  0.4965,   0.5800,   0.4875,   0.4785,   0.3643],\n",
      "         [  0.4566,   0.4230,   0.3737,   0.3789,   0.4093],\n",
      "         [  0.7567,   0.7546,   0.6141,   0.6056,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.07773117]\n",
      "[0.32215365]\n",
      "[0.00910057]\n",
      "[0.33443339]\n",
      "[0.08131734]\n",
      "[0.05286492]\n",
      "[0.88095022]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8875, 0.8644, 0.5530, 0.7559, 0.7293, 0.5365, 0.9534],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.5851,   0.6430,   0.4698,   0.4879],\n",
      "         [  0.5314,   0.5837,   0.6127,   0.5020,   0.4544],\n",
      "         [  0.4564,   0.5093,   0.4194,   0.4071,   0.3650],\n",
      "         [  0.5921,   0.6076,   0.6445,   0.5290,   0.4427],\n",
      "         [  0.4965,   0.5800,   0.5928,   0.4785,   0.3643],\n",
      "         [  0.4566,   0.4230,   0.4404,   0.3789,   0.4093],\n",
      "         [  0.7567,   0.7546,   0.7697,   0.6056,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.94458037]\n",
      "[0.10269996]\n",
      "[0.11034412]\n",
      "[0.57557063]\n",
      "[1.53330488]\n",
      "[0.82774625]\n",
      "[0.30612082]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8849, 0.7556, 0.2045, 0.7464, 0.8242, 0.6258, 0.9241],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.5851,   0.6430,   0.6042,   0.4879],\n",
      "         [  0.5314,   0.5837,   0.6127,   0.6179,   0.4544],\n",
      "         [  0.4564,   0.5093,   0.4194,   0.3929,   0.3650],\n",
      "         [  0.5921,   0.6076,   0.6445,   0.6342,   0.4427],\n",
      "         [  0.4965,   0.5800,   0.5928,   0.5974,   0.3643],\n",
      "         [  0.4566,   0.4230,   0.4404,   0.4544,   0.4093],\n",
      "         [  0.7567,   0.7546,   0.7697,   0.7547,   0.5703]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.94791438]\n",
      "[0.12064157]\n",
      "[0.10697415]\n",
      "[0.62146225]\n",
      "[1.34537451]\n",
      "[1.16806383]\n",
      "[0.2872286]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7472, 0.8157, 0.0197, 0.7700, 0.8376, 0.1727, 0.9324],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.6364,   0.5851,   0.6430,   0.6042,   0.5939],\n",
      "         [  0.5314,   0.5837,   0.6127,   0.6179,   0.5770],\n",
      "         [  0.4564,   0.5093,   0.4194,   0.3929,   0.3182],\n",
      "         [  0.5921,   0.6076,   0.6445,   0.6342,   0.5502],\n",
      "         [  0.4965,   0.5800,   0.5928,   0.5974,   0.4897],\n",
      "         [  0.4566,   0.4230,   0.4404,   0.4544,   0.3535],\n",
      "         [  0.7567,   0.7546,   0.7697,   0.7547,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.78617008]\n",
      "[0.07334242]\n",
      "[0.08993421]\n",
      "[0.72552967]\n",
      "[1.17685006]\n",
      "[0.51686392]\n",
      "[0.28053973]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8563, 0.8728, 0.8491, 0.9598, 0.7814, 0.5975, 0.9390],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.5851,   0.6430,   0.6042,   0.5939],\n",
      "         [  0.6716,   0.5837,   0.6127,   0.6179,   0.5770],\n",
      "         [  0.5927,   0.5093,   0.4194,   0.3929,   0.3182],\n",
      "         [  0.7462,   0.6076,   0.6445,   0.6342,   0.5502],\n",
      "         [  0.6062,   0.5800,   0.5928,   0.5974,   0.4897],\n",
      "         [  0.5458,   0.4230,   0.4404,   0.4544,   0.3535],\n",
      "         [  0.9115,   0.7546,   0.7697,   0.7547,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.8861914]\n",
      "[0.95326809]\n",
      "[0.76388804]\n",
      "[0.85634263]\n",
      "[0.85487643]\n",
      "[0.98031542]\n",
      "[0.26732501]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8451, 0.8123, 0.8038, 0.9498, 0.8026, 0.7412, 0.9417],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.6430,   0.6042,   0.5939],\n",
      "         [  0.6716,   0.7077,   0.6127,   0.6179,   0.5770],\n",
      "         [  0.5927,   0.6307,   0.4194,   0.3929,   0.3182],\n",
      "         [  0.7462,   0.7643,   0.6445,   0.6342,   0.5502],\n",
      "         [  0.6062,   0.6770,   0.5928,   0.5974,   0.4897],\n",
      "         [  0.5458,   0.5308,   0.4404,   0.4544,   0.3535],\n",
      "         [  0.9115,   0.9076,   0.7697,   0.7547,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.8783399]\n",
      "[0.99140417]\n",
      "[0.83878057]\n",
      "[0.79743333]\n",
      "[0.97712586]\n",
      "[0.84777373]\n",
      "[0.79835]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8919, 0.9105, 0.7759, 0.9650, 0.6966, 0.8110, 0.9378],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.6042,   0.5939],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.6179,   0.5770],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.3929,   0.3182],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.6342,   0.5502],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.5974,   0.4897],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.4544,   0.3535],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.7547,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.81581567]\n",
      "[0.9735367]\n",
      "[0.7339906]\n",
      "[0.81719201]\n",
      "[0.8130016]\n",
      "[0.81924256]\n",
      "[0.67790926]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9048, 0.8746, 0.8677, 0.9388, 0.6948, 0.7206, 0.9571],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.5939],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.5770],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.3182],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.5502],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.4897],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.3535],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.7219]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.93190794]\n",
      "[0.82929968]\n",
      "[0.78450425]\n",
      "[0.85229145]\n",
      "[0.85216777]\n",
      "[0.80603878]\n",
      "[0.20191619]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8881, 0.9362, 0.8678, 0.9634, 0.6328, 0.8068, 0.9374],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.7870766]\n",
      "[0.95296795]\n",
      "[0.73023332]\n",
      "[0.82154954]\n",
      "[0.999247]\n",
      "[0.78914169]\n",
      "[0.64860566]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8821, 0.8030, 0.7893, 0.8902, 0.8130, 0.6531, 0.9837],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1226,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1201,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1395,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1218,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0913,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1615,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.4025655]\n",
      "[0.41839952]\n",
      "[0.20842984]\n",
      "[0.47293733]\n",
      "[0.10643423]\n",
      "[0.3094505]\n",
      "[0.0188977]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8459, 0.6204, 0.6580, 0.7786, 0.7405, 0.6394, 0.9817],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.1326,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1226,   0.0910,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1201,   0.0961,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1395,   0.1245,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1218,   0.1124,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0913,   0.0748,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1615,   0.1607,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.37751738]\n",
      "[0.48833143]\n",
      "[0.18985925]\n",
      "[0.46711179]\n",
      "[0.12264433]\n",
      "[0.19449943]\n",
      "[0.03164604]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7762, 0.6460, 0.7654, 0.8369, 0.7645, 0.6749, 0.9659],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.1326,   0.1262,   0.0000,   0.0000],\n",
      "         [  0.1226,   0.0910,   0.1030,   0.0000,   0.0000],\n",
      "         [  0.1201,   0.0961,   0.1242,   0.0000,   0.0000],\n",
      "         [  0.1395,   0.1245,   0.1351,   0.0000,   0.0000],\n",
      "         [  0.1218,   0.1124,   0.1225,   0.0000,   0.0000],\n",
      "         [  0.0913,   0.0748,   0.1040,   0.0000,   0.0000],\n",
      "         [  0.1615,   0.1607,   0.1596,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.39621596]\n",
      "[0.21981562]\n",
      "[0.14872993]\n",
      "[0.36641334]\n",
      "[0.11514083]\n",
      "[0.19831455]\n",
      "[0.01291161]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8754, 0.6701, 0.8301, 0.7386, 0.6955, 0.8707, 0.9025],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.1326,   0.1262,   0.1375,   0.0000],\n",
      "         [  0.1226,   0.0910,   0.1030,   0.0989,   0.0000],\n",
      "         [  0.1201,   0.0961,   0.1242,   0.1195,   0.0000],\n",
      "         [  0.1395,   0.1245,   0.1351,   0.1056,   0.0000],\n",
      "         [  0.1218,   0.1124,   0.1225,   0.1119,   0.0000],\n",
      "         [  0.0913,   0.0748,   0.1040,   0.1378,   0.0000],\n",
      "         [  0.1615,   0.1607,   0.1596,   0.1471,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.03862543]\n",
      "[0.00611077]\n",
      "[0.08499306]\n",
      "[1.06451755]\n",
      "[0.01702779]\n",
      "[0.07331281]\n",
      "[0.14247851]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8272, 0.8212, 0.8554, 0.9076, 0.8491, 0.6894, 0.9647],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.1408,   0.1326,   0.1262,   0.1375,   0.1295],\n",
      "         [  0.1226,   0.0910,   0.1030,   0.0989,   0.1281],\n",
      "         [  0.1201,   0.0961,   0.1242,   0.1195,   0.1223],\n",
      "         [  0.1395,   0.1245,   0.1351,   0.1056,   0.1409],\n",
      "         [  0.1218,   0.1124,   0.1225,   0.1119,   0.1354],\n",
      "         [  0.0913,   0.0748,   0.1040,   0.1378,   0.1043],\n",
      "         [  0.1615,   0.1607,   0.1596,   0.1471,   0.1589]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.52540168]\n",
      "[0.33536528]\n",
      "[0.21605337]\n",
      "[0.49510689]\n",
      "[0.11825651]\n",
      "[0.37694103]\n",
      "[0.01493503]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7471, 0.8150, 0.5597, 0.8766, 0.7443, 0.4945, 0.9504],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  1.3260e-01,  1.2625e-01,  1.3749e-01,  1.2953e-01],\n",
      "         [ 2.2652e-01,  9.0950e-02,  1.0301e-01,  9.8855e-02,  1.2813e-01],\n",
      "         [ 1.3508e-01,  9.6077e-02,  1.2418e-01,  1.1954e-01,  1.2226e-01],\n",
      "         [ 2.7342e-01,  1.2454e-01,  1.3511e-01,  1.0561e-01,  1.4094e-01],\n",
      "         [ 2.0362e-01,  1.1243e-01,  1.2249e-01,  1.1186e-01,  1.3537e-01],\n",
      "         [ 5.8697e-02,  7.4835e-02,  1.0402e-01,  1.3777e-01,  1.0431e-01],\n",
      "         [ 3.1520e-01,  1.6075e-01,  1.5963e-01,  1.4710e-01,  1.5893e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.78797321]\n",
      "[1.58159381]\n",
      "[0.54156923]\n",
      "[1.29190636]\n",
      "[0.27921859]\n",
      "[1.13572488]\n",
      "[0.03734971]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7208, 0.7975, 0.6325, 0.9152, 0.6481, 0.7954, 0.8773],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  2.4327e-01,  1.2625e-01,  1.3749e-01,  1.2953e-01],\n",
      "         [ 2.2652e-01,  2.1621e-01,  1.0301e-01,  9.8855e-02,  1.2813e-01],\n",
      "         [ 1.3508e-01,  1.7564e-01,  1.2418e-01,  1.1954e-01,  1.2226e-01],\n",
      "         [ 2.7342e-01,  2.7112e-01,  1.3511e-01,  1.0561e-01,  1.4094e-01],\n",
      "         [ 2.0362e-01,  1.8706e-01,  1.2249e-01,  1.1186e-01,  1.3537e-01],\n",
      "         [ 5.8697e-02,  1.9826e-01,  1.0402e-01,  1.3777e-01,  1.0431e-01],\n",
      "         [ 3.1520e-01,  3.0397e-01,  1.5963e-01,  1.4710e-01,  1.5893e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.06885]\n",
      "[0.0342411]\n",
      "[0.02742624]\n",
      "[0.49028546]\n",
      "[0.01490691]\n",
      "[0.10109079]\n",
      "[0.05404896]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8336, 0.6900, 0.2576, 0.9205, 0.6379, 0.4932, 0.9465],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  2.4327e-01,  2.4994e-01,  1.3749e-01,  1.2953e-01],\n",
      "         [ 2.2652e-01,  2.1621e-01,  2.0844e-01,  9.8855e-02,  1.2813e-01],\n",
      "         [ 1.3508e-01,  1.7564e-01,  1.1928e-01,  1.1954e-01,  1.2226e-01],\n",
      "         [ 2.7342e-01,  2.7112e-01,  2.8106e-01,  1.0561e-01,  1.4094e-01],\n",
      "         [ 2.0362e-01,  1.8706e-01,  2.1261e-01,  1.1186e-01,  1.3537e-01],\n",
      "         [ 5.8697e-02,  1.9826e-01,  1.4535e-01,  1.3777e-01,  1.0431e-01],\n",
      "         [ 3.1520e-01,  3.0397e-01,  3.1227e-01,  1.4710e-01,  1.5893e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.47502161]\n",
      "[0.34652985]\n",
      "[0.88062191]\n",
      "[1.72665617]\n",
      "[0.31810187]\n",
      "[1.30042099]\n",
      "[0.0986125]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6729,  0.7127,  0.5581,  0.8255, -0.0858,  0.4825,  0.9104],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  2.4327e-01,  2.4994e-01,  2.0888e-01,  1.2953e-01],\n",
      "         [ 2.2652e-01,  2.1621e-01,  2.0844e-01,  1.8845e-01,  1.2813e-01],\n",
      "         [ 1.3508e-01,  1.7564e-01,  1.1928e-01,  1.5983e-01,  1.2226e-01],\n",
      "         [ 2.7342e-01,  2.7112e-01,  2.8106e-01,  2.2314e-01,  1.4094e-01],\n",
      "         [ 2.0362e-01,  1.8706e-01,  2.1261e-01,  5.2315e-02,  1.3537e-01],\n",
      "         [ 5.8697e-02,  1.9826e-01,  1.4535e-01,  1.7769e-01,  1.0431e-01],\n",
      "         [ 3.1520e-01,  3.0397e-01,  3.1227e-01,  2.9160e-01,  1.5893e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[0.13936189]\n",
      "[0.07423184]\n",
      "[0.07061949]\n",
      "[0.60648384]\n",
      "[0.0345412]\n",
      "[0.3974506]\n",
      "[0.111263]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7899, 0.9357, 0.6255, 0.9289, 0.8528, 0.6551, 0.9258],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 2.4455e-01,  2.4327e-01,  2.4994e-01,  2.0888e-01,  2.5343e-01],\n",
      "         [ 2.2652e-01,  2.1621e-01,  2.0844e-01,  1.8845e-01,  2.7910e-01],\n",
      "         [ 1.3508e-01,  1.7564e-01,  1.1928e-01,  1.5983e-01,  2.0154e-01],\n",
      "         [ 2.7342e-01,  2.7112e-01,  2.8106e-01,  2.2314e-01,  2.8976e-01],\n",
      "         [ 2.0362e-01,  1.8706e-01,  2.1261e-01,  5.2315e-02,  2.4773e-01],\n",
      "         [ 5.8697e-02,  1.9826e-01,  1.4535e-01,  1.7769e-01,  1.9764e-01],\n",
      "         [ 3.1520e-01,  3.0397e-01,  3.1227e-01,  2.9160e-01,  3.1089e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.11507387]\n",
      "[0.74907694]\n",
      "[0.89721121]\n",
      "[1.08657089]\n",
      "[0.88807824]\n",
      "[0.8769918]\n",
      "[0.45562104]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9215, 0.9048, 0.8395, 0.8882, 0.6720, 0.7227, 0.9669],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 3.9419e-01,  2.4327e-01,  2.4994e-01,  2.0888e-01,  2.5343e-01],\n",
      "         [ 3.7376e-01,  2.1621e-01,  2.0844e-01,  1.8845e-01,  2.7910e-01],\n",
      "         [ 2.6971e-01,  1.7564e-01,  1.1928e-01,  1.5983e-01,  2.0154e-01],\n",
      "         [ 4.1861e-01,  2.7112e-01,  2.8106e-01,  2.2314e-01,  2.8976e-01],\n",
      "         [ 3.0349e-01,  1.8706e-01,  2.1261e-01,  5.2315e-02,  2.4773e-01],\n",
      "         [ 1.6630e-01,  1.9826e-01,  1.4535e-01,  1.7769e-01,  1.9764e-01],\n",
      "         [ 4.7481e-01,  3.0397e-01,  3.1227e-01,  2.9160e-01,  3.1089e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.27408052]\n",
      "[0.22716031]\n",
      "[0.492585]\n",
      "[1.27860918]\n",
      "[0.25546186]\n",
      "[0.76098909]\n",
      "[0.03847826]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8966, 0.8777, 0.8338, 0.9410, 0.7592, 0.5569, 0.9796],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 3.9419e-01,  3.8885e-01,  2.4994e-01,  2.0888e-01,  2.5343e-01],\n",
      "         [ 3.7376e-01,  3.5322e-01,  2.0844e-01,  1.8845e-01,  2.7910e-01],\n",
      "         [ 2.6971e-01,  3.0595e-01,  1.1928e-01,  1.5983e-01,  2.0154e-01],\n",
      "         [ 4.1861e-01,  4.2161e-01,  2.8106e-01,  2.2314e-01,  2.8976e-01],\n",
      "         [ 3.0349e-01,  3.0521e-01,  2.1261e-01,  5.2315e-02,  2.4773e-01],\n",
      "         [ 1.6630e-01,  2.6635e-01,  1.4535e-01,  1.7769e-01,  1.9764e-01],\n",
      "         [ 4.7481e-01,  4.6455e-01,  3.1227e-01,  2.9160e-01,  3.1089e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.5204221]\n",
      "[0.27528202]\n",
      "[0.33225551]\n",
      "[1.45120215]\n",
      "[0.24109393]\n",
      "[0.89574072]\n",
      "[0.03431782]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9388, 0.9548, 0.9482, 0.9563, 0.8474, 0.8698, 0.9786],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[-1.7148e+00, -1.1841e+00, -2.0996e+01, -3.0956e+00, -1.5254e+00],\n",
      "         [-1.1192e+00, -3.2352e+00, -1.0864e+01, -1.6272e+00, -1.4525e+00],\n",
      "         [-2.9225e+00, -2.0566e+00, -7.3825e+01, -1.4582e+01, -3.2874e+00],\n",
      "         [-1.1343e+00, -9.8539e-01, -2.0169e+00, -1.6740e+01, -3.1631e+00],\n",
      "         [-3.1955e+00, -2.6328e+00, -1.0915e+00, -4.2357e+00, -2.9870e+00],\n",
      "         [-5.5799e+00, -9.3941e+00, -1.2787e+01, -1.7994e+01, -1.6477e+00],\n",
      "         [ 3.3806e-01, -1.4310e-01, -9.6709e+00, -3.2836e+00, -1.4765e+00]],\n",
      "\n",
      "        [[ 6.2678e-01,  7.2484e-01,  4.3697e-01,  5.2936e-01,  6.7263e-01],\n",
      "         [ 6.3152e-01,  5.8972e-01,  5.6745e-01,  2.5268e-01,  7.0835e-01],\n",
      "         [ 9.5386e-02,  4.9341e-01, -2.2128e-01,  3.4601e-01,  4.5045e-01],\n",
      "         [ 6.5308e-01,  7.1745e-01,  5.9535e-01,  7.1745e-01,  5.3870e-01],\n",
      "         [ 4.4327e-01,  5.9961e-01,  3.8306e-01,  2.4207e-01,  6.0800e-01],\n",
      "         [ 2.3560e-01,  5.9028e-01, -1.4246e-01,  5.3229e-01,  4.9588e-01],\n",
      "         [ 9.3157e-01,  8.9289e-01,  8.6789e-01,  8.4747e-01,  8.9972e-01]],\n",
      "\n",
      "        [[ 7.7589e-01,  7.1792e-01,  7.8510e-01,  7.5089e-01,  7.3983e-01],\n",
      "         [ 6.7163e-01,  7.0767e-01,  7.5905e-01,  7.5979e-01,  7.2974e-01],\n",
      "         [ 5.9267e-01,  6.3069e-01,  5.3405e-01,  5.2817e-01,  4.5284e-01],\n",
      "         [ 7.4621e-01,  7.6427e-01,  8.0237e-01,  7.8709e-01,  7.0862e-01],\n",
      "         [ 6.0618e-01,  6.7702e-01,  6.7475e-01,  6.9709e-01,  5.7688e-01],\n",
      "         [ 5.4582e-01,  5.3085e-01,  5.5479e-01,  5.6769e-01,  4.8230e-01],\n",
      "         [ 9.1146e-01,  9.0756e-01,  9.2361e-01,  9.1238e-01,  8.7680e-01]],\n",
      "\n",
      "        [[ 3.9419e-01,  3.8885e-01,  4.0460e-01,  2.0888e-01,  2.5343e-01],\n",
      "         [ 3.7376e-01,  3.5322e-01,  3.6379e-01,  1.8845e-01,  2.7910e-01],\n",
      "         [ 2.6971e-01,  3.0595e-01,  2.7452e-01,  1.5983e-01,  2.0154e-01],\n",
      "         [ 4.1861e-01,  4.2161e-01,  4.3764e-01,  2.2314e-01,  2.8976e-01],\n",
      "         [ 3.0349e-01,  3.0521e-01,  3.4598e-01,  5.2315e-02,  2.4773e-01],\n",
      "         [ 1.6630e-01,  2.6635e-01,  2.7688e-01,  1.7769e-01,  1.9764e-01],\n",
      "         [ 4.7481e-01,  4.6455e-01,  4.7397e-01,  2.9160e-01,  3.1089e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "[1.03418695]\n",
      "[1.07442789]\n",
      "[1.17692364]\n",
      "[1.1634885]\n",
      "[1.01883643]\n",
      "[0.74738135]\n",
      "[0.20465383]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9408, 0.8868, 0.8796, 0.9344, 0.7678, 0.9228, 0.9739],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.3942,   0.3888,   0.4046,   0.3568,   0.2534],\n",
      "         [  0.3738,   0.3532,   0.3638,   0.3215,   0.2791],\n",
      "         [  0.2697,   0.3060,   0.2745,   0.2985,   0.2015],\n",
      "         [  0.4186,   0.4216,   0.4376,   0.3708,   0.2898],\n",
      "         [  0.3035,   0.3052,   0.3460,   0.1645,   0.2477],\n",
      "         [  0.1663,   0.2664,   0.2769,   0.3190,   0.1976],\n",
      "         [  0.4748,   0.4645,   0.4740,   0.4507,   0.3109]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.10219966]\n",
      "[0.08976463]\n",
      "[0.04933195]\n",
      "[0.54626462]\n",
      "[0.02527084]\n",
      "[0.31891236]\n",
      "[0.03352556]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9277, 0.9086, 0.8626, 0.9608, 0.8255, 0.5757, 0.9123],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.3942,   0.3888,   0.4046,   0.3568,   0.3967],\n",
      "         [  0.3738,   0.3532,   0.3638,   0.3215,   0.4194],\n",
      "         [  0.2697,   0.3060,   0.2745,   0.2985,   0.3328],\n",
      "         [  0.4186,   0.4216,   0.4376,   0.3708,   0.4449],\n",
      "         [  0.3035,   0.3052,   0.3460,   0.1645,   0.3750],\n",
      "         [  0.1663,   0.2664,   0.2769,   0.3190,   0.2565],\n",
      "         [  0.4748,   0.4645,   0.4740,   0.4507,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.75333622]\n",
      "[1.01883183]\n",
      "[0.99882255]\n",
      "[1.56357613]\n",
      "[0.23387719]\n",
      "[0.7879611]\n",
      "[0.02040617]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6574, 0.5335, 0.5840, 0.5033, 0.7611, 0.7750, 0.8505],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.3888,   0.4046,   0.3568,   0.3967],\n",
      "         [  0.4113,   0.3532,   0.3638,   0.3215,   0.4194],\n",
      "         [  0.3366,   0.3060,   0.2745,   0.2985,   0.3328],\n",
      "         [  0.4842,   0.4216,   0.4376,   0.3708,   0.4449],\n",
      "         [  0.3997,   0.3052,   0.3460,   0.1645,   0.3750],\n",
      "         [  0.2694,   0.2664,   0.2769,   0.3190,   0.2565],\n",
      "         [  0.5943,   0.4645,   0.4740,   0.4507,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.1433275]\n",
      "[0.11314362]\n",
      "[0.0706263]\n",
      "[0.34069418]\n",
      "[0.02413885]\n",
      "[0.30660861]\n",
      "[0.11323082]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7612, 0.8426, 0.6719, 0.6566, 0.8428, 0.7043, 0.9269],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.4986,   0.4046,   0.3568,   0.3967],\n",
      "         [  0.4113,   0.4787,   0.3638,   0.3215,   0.4194],\n",
      "         [  0.3366,   0.3939,   0.2745,   0.2985,   0.3328],\n",
      "         [  0.4842,   0.5141,   0.4376,   0.3708,   0.4449],\n",
      "         [  0.3997,   0.4288,   0.3460,   0.1645,   0.3750],\n",
      "         [  0.2694,   0.3666,   0.2769,   0.3190,   0.2565],\n",
      "         [  0.5943,   0.6168,   0.4740,   0.4507,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.07957271]\n",
      "[0.04009429]\n",
      "[0.07313421]\n",
      "[0.22992675]\n",
      "[0.01924314]\n",
      "[0.2087582]\n",
      "[0.03854116]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8437, 0.7370, 0.4112, 0.6966, 0.8271, 0.8030, 0.7944],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.4986,   0.5324,   0.3568,   0.3967],\n",
      "         [  0.4113,   0.4787,   0.4672,   0.3215,   0.4194],\n",
      "         [  0.3366,   0.3939,   0.3141,   0.2985,   0.3328],\n",
      "         [  0.4842,   0.5141,   0.5412,   0.3708,   0.4449],\n",
      "         [  0.3997,   0.4288,   0.4745,   0.1645,   0.3750],\n",
      "         [  0.2694,   0.3666,   0.3874,   0.3190,   0.2565],\n",
      "         [  0.5943,   0.6168,   0.6006,   0.4507,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.09019388]\n",
      "[0.03250095]\n",
      "[0.10476985]\n",
      "[0.49920191]\n",
      "[0.03723115]\n",
      "[0.13111947]\n",
      "[0.22958012]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8433, 0.7009, 0.7140, 0.7563, 0.8496, 0.7949, 0.8810],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.4986,   0.5324,   0.4595,   0.3967],\n",
      "         [  0.4113,   0.4787,   0.4672,   0.4196,   0.4194],\n",
      "         [  0.3366,   0.3939,   0.3141,   0.3789,   0.3328],\n",
      "         [  0.4842,   0.5141,   0.5412,   0.4835,   0.4449],\n",
      "         [  0.3997,   0.4288,   0.4745,   0.2785,   0.3750],\n",
      "         [  0.2694,   0.3666,   0.3874,   0.4169,   0.2565],\n",
      "         [  0.5943,   0.6168,   0.6006,   0.5887,   0.4591]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.18254372]\n",
      "[0.11733496]\n",
      "[0.09810731]\n",
      "[0.36946404]\n",
      "[0.02092464]\n",
      "[0.35707803]\n",
      "[0.10645892]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8703, 0.8498, 0.7000, 0.8475, 0.8297, 0.7976, 0.9102],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.4778,   0.4986,   0.5324,   0.4595,   0.5193],\n",
      "         [  0.4113,   0.4787,   0.4672,   0.4196,   0.5441],\n",
      "         [  0.3366,   0.3939,   0.3141,   0.3789,   0.4166],\n",
      "         [  0.4842,   0.5141,   0.5412,   0.4835,   0.5693],\n",
      "         [  0.3997,   0.4288,   0.4745,   0.2785,   0.4883],\n",
      "         [  0.2694,   0.3666,   0.3874,   0.4169,   0.3582],\n",
      "         [  0.5943,   0.6168,   0.6006,   0.5887,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.13196666]\n",
      "[0.08080704]\n",
      "[0.13361424]\n",
      "[0.53108224]\n",
      "[0.04892763]\n",
      "[0.20205287]\n",
      "[0.2474573]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9121, 0.8876, 0.0917, 0.7827, 0.7452, 0.7784, 0.9531],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.4986,   0.5324,   0.4595,   0.5193],\n",
      "         [  0.5392,   0.4787,   0.4672,   0.4196,   0.5441],\n",
      "         [  0.2996,   0.3939,   0.3141,   0.3789,   0.4166],\n",
      "         [  0.5892,   0.5141,   0.5412,   0.4835,   0.5693],\n",
      "         [  0.4983,   0.4288,   0.4745,   0.2785,   0.4883],\n",
      "         [  0.3814,   0.3666,   0.3874,   0.4169,   0.3582],\n",
      "         [  0.7483,   0.6168,   0.6006,   0.5887,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.45102182]\n",
      "[0.20080987]\n",
      "[0.2886548]\n",
      "[0.46862557]\n",
      "[0.0802635]\n",
      "[0.76499848]\n",
      "[0.08789203]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8221, 0.6911, 0.4338, 0.8052, 0.8685, 0.8728, 0.9755],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.6252,   0.5324,   0.4595,   0.5193],\n",
      "         [  0.5392,   0.5884,   0.4672,   0.4196,   0.5441],\n",
      "         [  0.2996,   0.4266,   0.3141,   0.3789,   0.4166],\n",
      "         [  0.5892,   0.6258,   0.5412,   0.4835,   0.5693],\n",
      "         [  0.4983,   0.5657,   0.4745,   0.2785,   0.4883],\n",
      "         [  0.3814,   0.4974,   0.3874,   0.4169,   0.3582],\n",
      "         [  0.7483,   0.7771,   0.6006,   0.5887,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.49606515]\n",
      "[0.08889655]\n",
      "[0.28214582]\n",
      "[0.59155366]\n",
      "[0.21673222]\n",
      "[0.26958858]\n",
      "[0.038617]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.6437,  0.8770, -0.6170,  0.5033,  0.7834, -0.0225,  0.8938],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.6252,   0.6247,   0.4595,   0.5193],\n",
      "         [  0.5392,   0.5884,   0.6002,   0.4196,   0.5441],\n",
      "         [  0.2996,   0.4266,   0.1515,   0.3789,   0.4166],\n",
      "         [  0.5892,   0.6258,   0.5989,   0.4835,   0.5693],\n",
      "         [  0.4983,   0.5657,   0.5929,   0.2785,   0.4883],\n",
      "         [  0.3814,   0.4974,   0.3399,   0.4169,   0.3582],\n",
      "         [  0.7483,   0.7771,   0.7452,   0.5887,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.80670248]\n",
      "[0.09909374]\n",
      "[0.07644913]\n",
      "[0.73872699]\n",
      "[1.18274359]\n",
      "[0.5124512]\n",
      "[0.22888795]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8380, 0.8962, 0.6583, 0.8188, 0.8839, 0.7864, 0.9550],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.6252,   0.6247,   0.5864,   0.5193],\n",
      "         [  0.5392,   0.5884,   0.6002,   0.5544,   0.5441],\n",
      "         [  0.2996,   0.4266,   0.1515,   0.4278,   0.4166],\n",
      "         [  0.5892,   0.6258,   0.5989,   0.6010,   0.5693],\n",
      "         [  0.4983,   0.5657,   0.5929,   0.4114,   0.4883],\n",
      "         [  0.3814,   0.4974,   0.3399,   0.5296,   0.3582],\n",
      "         [  0.7483,   0.7771,   0.7452,   0.7372,   0.6037]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.48818046]\n",
      "[0.14379967]\n",
      "[0.2814378]\n",
      "[0.57570997]\n",
      "[0.17296336]\n",
      "[0.79535505]\n",
      "[0.08201899]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8967, 0.8894, 0.3246, 0.7778, 0.8211, 0.8017, 0.9692],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.6114,   0.6252,   0.6247,   0.5864,   0.6609],\n",
      "         [  0.5392,   0.5884,   0.6002,   0.5544,   0.6845],\n",
      "         [  0.2996,   0.4266,   0.1515,   0.4278,   0.4151],\n",
      "         [  0.5892,   0.6258,   0.5989,   0.6010,   0.6813],\n",
      "         [  0.4983,   0.5657,   0.5929,   0.4114,   0.6041],\n",
      "         [  0.3814,   0.4974,   0.3399,   0.5296,   0.4830],\n",
      "         [  0.7483,   0.7771,   0.7452,   0.7372,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.40986762]\n",
      "[0.1208521]\n",
      "[0.36449529]\n",
      "[0.50642732]\n",
      "[0.12903694]\n",
      "[0.59969355]\n",
      "[0.07970008]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9136, 0.8877, 0.8599, 0.9594, 0.8850, 0.6797, 0.9576],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.6252,   0.6247,   0.5864,   0.6609],\n",
      "         [  0.6832,   0.5884,   0.6002,   0.5544,   0.6845],\n",
      "         [  0.4343,   0.4266,   0.1515,   0.4278,   0.4151],\n",
      "         [  0.7432,   0.6258,   0.5989,   0.6010,   0.6813],\n",
      "         [  0.6222,   0.5657,   0.5929,   0.4114,   0.6041],\n",
      "         [  0.4786,   0.4974,   0.3399,   0.5296,   0.4830],\n",
      "         [  0.9055,   0.7771,   0.7452,   0.7372,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.94592704]\n",
      "[0.91268345]\n",
      "[0.90836605]\n",
      "[0.87492432]\n",
      "[0.9768584]\n",
      "[1.01386982]\n",
      "[0.27732182]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9211, 0.8084, 0.8836, 0.9404, 0.7674, 0.7024, 0.9636],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.6247,   0.5864,   0.6609],\n",
      "         [  0.6832,   0.7200,   0.6002,   0.5544,   0.6845],\n",
      "         [  0.4343,   0.5658,   0.1515,   0.4278,   0.4151],\n",
      "         [  0.7432,   0.7796,   0.5989,   0.6010,   0.6813],\n",
      "         [  0.6222,   0.6793,   0.5929,   0.4114,   0.6041],\n",
      "         [  0.4786,   0.5967,   0.3399,   0.5296,   0.4830],\n",
      "         [  0.9055,   0.9360,   0.7452,   0.7372,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.89748698]\n",
      "[0.91593859]\n",
      "[0.85573162]\n",
      "[0.86460127]\n",
      "[0.82331666]\n",
      "[0.93160402]\n",
      "[0.24383163]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8800, 0.9101, 0.8664, 0.9538, 0.7922, 0.6765, 0.9406],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.5864,   0.6609],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.5544,   0.6845],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.4278,   0.4151],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.6010,   0.6813],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.4114,   0.6041],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.5296,   0.4830],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.7372,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.89410571]\n",
      "[0.89814751]\n",
      "[0.78394586]\n",
      "[0.85447891]\n",
      "[0.87399294]\n",
      "[0.90643076]\n",
      "[0.24117444]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9129, 0.9043, 0.8113, 0.9452, 0.8733, 0.7227, 0.9422],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.6609],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.6845],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.4151],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.6813],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.6041],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.4830],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.7612]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.91895603]\n",
      "[0.85211251]\n",
      "[0.77004353]\n",
      "[0.79465433]\n",
      "[0.77024881]\n",
      "[0.82088678]\n",
      "[0.28755121]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8535, 0.8436, 0.9108, 0.9591, 0.8956, 0.7707, 0.9156],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.8311049]\n",
      "[0.75573502]\n",
      "[0.66964394]\n",
      "[0.88131951]\n",
      "[0.92329692]\n",
      "[0.63215707]\n",
      "[0.19856577]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8855, 0.8176, 0.8288, 0.8050, 0.6423, 0.8823, 0.9229],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1278,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1238,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1245,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1023,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1286,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1506,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06109698]\n",
      "[0.03249756]\n",
      "[0.07210588]\n",
      "[0.71612455]\n",
      "[0.01973256]\n",
      "[0.10045489]\n",
      "[0.16244369]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7940, 0.6968, 0.8296, 0.9011, 0.7206, 0.6797, 0.9681],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.1267,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1278,   0.1122,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1238,   0.1340,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1245,   0.1386,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1023,   0.1145,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1286,   0.1026,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1506,   0.1602,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.43338537]\n",
      "[0.20248918]\n",
      "[0.19951279]\n",
      "[0.4821078]\n",
      "[0.04624547]\n",
      "[0.33629289]\n",
      "[0.01154209]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9276, 0.9355, 0.8680, 0.8689, 0.8303, 0.7225, 0.9730],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.1267,   0.1482,   0.0000,   0.0000],\n",
      "         [  0.1278,   0.1122,   0.1493,   0.0000,   0.0000],\n",
      "         [  0.1238,   0.1340,   0.1336,   0.0000,   0.0000],\n",
      "         [  0.1245,   0.1386,   0.1343,   0.0000,   0.0000],\n",
      "         [  0.1023,   0.1145,   0.1308,   0.0000,   0.0000],\n",
      "         [  0.1286,   0.1026,   0.1023,   0.0000,   0.0000],\n",
      "         [  0.1506,   0.1602,   0.1611,   0.0000,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.77064]\n",
      "[0.06968173]\n",
      "[0.06717952]\n",
      "[0.75538816]\n",
      "[0.44534484]\n",
      "[0.16845062]\n",
      "[0.19959654]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8648, 0.8771, 0.8692, 0.8907, 0.8513, 0.7876, 0.9735],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.1267,   0.1482,   0.1375,   0.0000],\n",
      "         [  0.1278,   0.1122,   0.1493,   0.1382,   0.0000],\n",
      "         [  0.1238,   0.1340,   0.1336,   0.1292,   0.0000],\n",
      "         [  0.1245,   0.1386,   0.1343,   0.1410,   0.0000],\n",
      "         [  0.1023,   0.1145,   0.1308,   0.1373,   0.0000],\n",
      "         [  0.1286,   0.1026,   0.1023,   0.1193,   0.0000],\n",
      "         [  0.1506,   0.1602,   0.1611,   0.1599,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.53876239]\n",
      "[0.32490396]\n",
      "[0.29096393]\n",
      "[0.52497387]\n",
      "[0.19511542]\n",
      "[0.40010748]\n",
      "[0.02165219]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8742, 0.7305, 0.8052, 0.7648, 0.7940, 0.8244, 0.9455],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.1375,   0.1267,   0.1482,   0.1375,   0.1365],\n",
      "         [  0.1278,   0.1122,   0.1493,   0.1382,   0.1061],\n",
      "         [  0.1238,   0.1340,   0.1336,   0.1292,   0.1129],\n",
      "         [  0.1245,   0.1386,   0.1343,   0.1410,   0.1103],\n",
      "         [  0.1023,   0.1145,   0.1308,   0.1373,   0.1257],\n",
      "         [  0.1286,   0.1026,   0.1023,   0.1193,   0.1259],\n",
      "         [  0.1506,   0.1602,   0.1611,   0.1599,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.07054089]\n",
      "[0.03595638]\n",
      "[0.08798174]\n",
      "[0.91533029]\n",
      "[0.02185151]\n",
      "[0.09396567]\n",
      "[0.15750508]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8037, 0.7728, 0.1496, 0.9090, 0.4923, 0.6627, 0.9439],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1267,   0.1482,   0.1375,   0.1365],\n",
      "         [  0.2383,   0.1122,   0.1493,   0.1382,   0.1061],\n",
      "         [  0.0973,   0.1340,   0.1336,   0.1292,   0.1129],\n",
      "         [  0.2622,   0.1386,   0.1343,   0.1410,   0.1103],\n",
      "         [  0.1494,   0.1145,   0.1308,   0.1373,   0.1257],\n",
      "         [  0.2062,   0.1026,   0.1023,   0.1193,   0.1259],\n",
      "         [  0.3043,   0.1602,   0.1611,   0.1599,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.57845891]\n",
      "[0.30467334]\n",
      "[0.82690642]\n",
      "[1.67363467]\n",
      "[0.41769488]\n",
      "[1.20873051]\n",
      "[0.09954467]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.4785, 0.8069, 0.6953, 0.8843, 0.5938, 0.6016, 0.8683],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1878,   0.1482,   0.1375,   0.1365],\n",
      "         [  0.2383,   0.2331,   0.1493,   0.1382,   0.1061],\n",
      "         [  0.0973,   0.2213,   0.1336,   0.1292,   0.1129],\n",
      "         [  0.2622,   0.2669,   0.1343,   0.1410,   0.1103],\n",
      "         [  0.1494,   0.1751,   0.1308,   0.1373,   0.1257],\n",
      "         [  0.2062,   0.1891,   0.1023,   0.1193,   0.1259],\n",
      "         [  0.3043,   0.2950,   0.1611,   0.1599,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.0943231]\n",
      "[0.03650244]\n",
      "[0.04944443]\n",
      "[1.15165954]\n",
      "[0.0208738]\n",
      "[0.1067268]\n",
      "[0.08451934]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9217, 0.8838, 0.7838, 0.9449, 0.8898, 0.7927, 0.9630],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1878,   0.2931,   0.1375,   0.1365],\n",
      "         [  0.2383,   0.2331,   0.2917,   0.1382,   0.1061],\n",
      "         [  0.0973,   0.2213,   0.2471,   0.1292,   0.1129],\n",
      "         [  0.2622,   0.2669,   0.2885,   0.1410,   0.1103],\n",
      "         [  0.1494,   0.1751,   0.2566,   0.1373,   0.1257],\n",
      "         [  0.2062,   0.1891,   0.2072,   0.1193,   0.1259],\n",
      "         [  0.3043,   0.2950,   0.3192,   0.1599,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.91278799]\n",
      "[0.7420375]\n",
      "[0.72661676]\n",
      "[1.05405731]\n",
      "[0.73038282]\n",
      "[0.86796806]\n",
      "[0.36025236]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9130, 0.9132, 0.6214, 0.9190, 0.6309, 0.8741, 0.9564],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1878,   0.2931,   0.2764,   0.1365],\n",
      "         [  0.2383,   0.2331,   0.2917,   0.2739,   0.1061],\n",
      "         [  0.0973,   0.2213,   0.2471,   0.2043,   0.1129],\n",
      "         [  0.2622,   0.2669,   0.2885,   0.2872,   0.1103],\n",
      "         [  0.1494,   0.1751,   0.2566,   0.1913,   0.1257],\n",
      "         [  0.2062,   0.1891,   0.2072,   0.2335,   0.1259],\n",
      "         [  0.3043,   0.2950,   0.3192,   0.3170,   0.1528]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.51225246]\n",
      "[1.42277892]\n",
      "[0.36483744]\n",
      "[1.39348488]\n",
      "[0.36397643]\n",
      "[0.91276405]\n",
      "[0.02776382]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6476, 0.7283, 0.6222, 0.9455, 0.5774, 0.6756, 0.9303],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.2532,   0.1878,   0.2931,   0.2764,   0.2170],\n",
      "         [  0.2383,   0.2331,   0.2917,   0.2739,   0.2108],\n",
      "         [  0.0973,   0.2213,   0.2471,   0.2043,   0.1724],\n",
      "         [  0.2622,   0.2669,   0.2885,   0.2872,   0.2516],\n",
      "         [  0.1494,   0.1751,   0.2566,   0.1913,   0.1880],\n",
      "         [  0.2062,   0.1891,   0.2072,   0.2335,   0.2030],\n",
      "         [  0.3043,   0.2950,   0.3192,   0.3170,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.11450942]\n",
      "[0.07947797]\n",
      "[0.06017903]\n",
      "[0.65113077]\n",
      "[0.01723155]\n",
      "[0.27060145]\n",
      "[0.0790013]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9321, 0.9617, 0.9561, 0.9701, 0.8813, 0.7059, 0.9747],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.1878,   0.2931,   0.2764,   0.2170],\n",
      "         [  0.3947,   0.2331,   0.2917,   0.2739,   0.2108],\n",
      "         [  0.2534,   0.2213,   0.2471,   0.2043,   0.1724],\n",
      "         [  0.4213,   0.2669,   0.2885,   0.2872,   0.2516],\n",
      "         [  0.2896,   0.1751,   0.2566,   0.1913,   0.1880],\n",
      "         [  0.3080,   0.1891,   0.2072,   0.2335,   0.2030],\n",
      "         [  0.4652,   0.2950,   0.3192,   0.3170,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.05714516]\n",
      "[1.02423697]\n",
      "[1.0989669]\n",
      "[1.09541577]\n",
      "[0.98453904]\n",
      "[0.89476073]\n",
      "[0.27002026]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8618, 0.8806, 0.8387, 0.9127, 0.7860, 0.7471, 0.9628],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.3187,   0.2931,   0.2764,   0.2170],\n",
      "         [  0.3947,   0.3686,   0.2917,   0.2739,   0.2108],\n",
      "         [  0.2534,   0.3401,   0.2471,   0.2043,   0.1724],\n",
      "         [  0.4213,   0.4079,   0.2885,   0.2872,   0.2516],\n",
      "         [  0.2896,   0.2945,   0.2566,   0.1913,   0.1880],\n",
      "         [  0.3080,   0.2918,   0.2072,   0.2335,   0.2030],\n",
      "         [  0.4652,   0.4520,   0.3192,   0.3170,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.59807575]\n",
      "[0.35624244]\n",
      "[0.49243636]\n",
      "[1.49774852]\n",
      "[0.26465588]\n",
      "[1.03364875]\n",
      "[0.04829712]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9635, 0.9356, 0.9254, 0.9050, 0.8506, 0.7500, 0.9738],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.3187,   0.4495,   0.2764,   0.2170],\n",
      "         [  0.3947,   0.3686,   0.4423,   0.2739,   0.2108],\n",
      "         [  0.2534,   0.3401,   0.3928,   0.2043,   0.1724],\n",
      "         [  0.4213,   0.4079,   0.4347,   0.2872,   0.2516],\n",
      "         [  0.2896,   0.2945,   0.3889,   0.1913,   0.1880],\n",
      "         [  0.3080,   0.2918,   0.3054,   0.2335,   0.2030],\n",
      "         [  0.4652,   0.4520,   0.4790,   0.3170,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.76046191]\n",
      "[1.08920332]\n",
      "[0.47813609]\n",
      "[1.43483443]\n",
      "[0.29155655]\n",
      "[0.83558342]\n",
      "[0.01923997]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9322, 0.9020, 0.9179, 0.9221, 0.8237, 0.8816, 0.9723],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.3187,   0.4495,   0.4251,   0.2170],\n",
      "         [  0.3947,   0.3686,   0.4423,   0.4143,   0.2108],\n",
      "         [  0.2534,   0.3401,   0.3928,   0.3443,   0.1724],\n",
      "         [  0.4213,   0.4079,   0.4347,   0.4169,   0.2516],\n",
      "         [  0.2896,   0.2945,   0.3889,   0.3172,   0.1880],\n",
      "         [  0.3080,   0.2918,   0.3054,   0.3700,   0.2030],\n",
      "         [  0.4652,   0.4520,   0.4790,   0.4751,   0.3043]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.14116884]\n",
      "[0.06735008]\n",
      "[0.07715621]\n",
      "[1.20971928]\n",
      "[0.02503762]\n",
      "[0.39716672]\n",
      "[0.04960227]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9231, 0.9594, 0.9430, 0.9554, 0.8807, 0.6535, 0.9687],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.4049,   0.3187,   0.4495,   0.4251,   0.3683],\n",
      "         [  0.3947,   0.3686,   0.4423,   0.4143,   0.3675],\n",
      "         [  0.2534,   0.3401,   0.3928,   0.3443,   0.3254],\n",
      "         [  0.4213,   0.4079,   0.4347,   0.4169,   0.4057],\n",
      "         [  0.2896,   0.2945,   0.3889,   0.3172,   0.3284],\n",
      "         [  0.3080,   0.2918,   0.3054,   0.3700,   0.2962],\n",
      "         [  0.4652,   0.4520,   0.4790,   0.4751,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.08872456]\n",
      "[1.03074754]\n",
      "[1.10560763]\n",
      "[1.13605651]\n",
      "[0.99023606]\n",
      "[0.9451829]\n",
      "[0.2468443]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9058, 0.8791, 0.8254, 0.8439, 0.7407, 0.8363, 0.9590],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.3187,   0.4495,   0.4251,   0.3683],\n",
      "         [  0.5316,   0.3686,   0.4423,   0.4143,   0.3675],\n",
      "         [  0.3751,   0.3401,   0.3928,   0.3443,   0.3254],\n",
      "         [  0.5546,   0.4079,   0.4347,   0.4169,   0.4057],\n",
      "         [  0.3868,   0.2945,   0.3889,   0.3172,   0.3284],\n",
      "         [  0.4345,   0.2918,   0.3054,   0.3700,   0.2962],\n",
      "         [  0.6230,   0.4520,   0.4790,   0.4751,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06766953]\n",
      "[0.02789246]\n",
      "[0.06157395]\n",
      "[0.31369373]\n",
      "[0.0197897]\n",
      "[0.16433146]\n",
      "[0.09844738]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8708, 0.8667, 0.6422, 0.7566, 0.7731, 0.7915, 0.9556],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.4509,   0.4495,   0.4251,   0.3683],\n",
      "         [  0.5316,   0.4955,   0.4423,   0.4143,   0.3675],\n",
      "         [  0.3751,   0.4250,   0.3928,   0.3443,   0.3254],\n",
      "         [  0.5546,   0.5161,   0.4347,   0.4169,   0.4057],\n",
      "         [  0.3868,   0.4071,   0.3889,   0.3172,   0.3284],\n",
      "         [  0.4345,   0.4093,   0.3054,   0.3700,   0.2962],\n",
      "         [  0.6230,   0.6088,   0.4790,   0.4751,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.11295395]\n",
      "[0.07223752]\n",
      "[0.09668045]\n",
      "[0.38457901]\n",
      "[0.01916273]\n",
      "[0.2150111]\n",
      "[0.13864319]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7790, 0.7896, 0.6752, 0.8307, 0.7662, 0.7288, 0.8985],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.4509,   0.5699,   0.4251,   0.3683],\n",
      "         [  0.5316,   0.4955,   0.5600,   0.4143,   0.3675],\n",
      "         [  0.3751,   0.4250,   0.4846,   0.3443,   0.3254],\n",
      "         [  0.5546,   0.5161,   0.5571,   0.4169,   0.4057],\n",
      "         [  0.3868,   0.4071,   0.5039,   0.3172,   0.3284],\n",
      "         [  0.4345,   0.4093,   0.4133,   0.3700,   0.2962],\n",
      "         [  0.6230,   0.6088,   0.6229,   0.4751,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.09917383]\n",
      "[0.04415978]\n",
      "[0.11471508]\n",
      "[0.49132754]\n",
      "[0.04528883]\n",
      "[0.16106811]\n",
      "[0.23465642]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8484, 0.8473, 0.7571, 0.7663, 0.8260, 0.7962, 0.9284],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.4509,   0.5699,   0.5469,   0.3683],\n",
      "         [  0.5316,   0.4955,   0.5600,   0.5245,   0.3675],\n",
      "         [  0.3751,   0.4250,   0.4846,   0.4595,   0.3254],\n",
      "         [  0.5546,   0.5161,   0.5571,   0.5247,   0.4057],\n",
      "         [  0.3868,   0.4071,   0.5039,   0.4333,   0.3284],\n",
      "         [  0.4345,   0.4093,   0.4133,   0.4849,   0.2962],\n",
      "         [  0.6230,   0.6088,   0.6229,   0.6221,   0.4635]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.09705949]\n",
      "[0.08313929]\n",
      "[0.0505674]\n",
      "[0.26697959]\n",
      "[0.02317965]\n",
      "[0.21895643]\n",
      "[0.09202839]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.4437, 0.6643, 0.5929, 0.5677, 0.8554, 0.8594, 0.9172],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.5465,   0.4509,   0.5699,   0.5469,   0.4151],\n",
      "         [  0.5316,   0.4955,   0.5600,   0.5245,   0.4412],\n",
      "         [  0.3751,   0.4250,   0.4846,   0.4595,   0.3941],\n",
      "         [  0.5546,   0.5161,   0.5571,   0.5247,   0.4841],\n",
      "         [  0.3868,   0.4071,   0.5039,   0.4333,   0.4507],\n",
      "         [  0.4345,   0.4093,   0.4133,   0.4849,   0.4227],\n",
      "         [  0.6230,   0.6088,   0.6229,   0.6221,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.12595742]\n",
      "[0.08858596]\n",
      "[0.07008049]\n",
      "[0.30176959]\n",
      "[0.01816264]\n",
      "[0.24020995]\n",
      "[0.10040068]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8436, 0.9141, 0.3575, 0.8357, 0.8574, 0.7864, 0.9619],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.4509,   0.5699,   0.5469,   0.4151],\n",
      "         [  0.6726,   0.4955,   0.5600,   0.5245,   0.4412],\n",
      "         [  0.3554,   0.4250,   0.4846,   0.4595,   0.3941],\n",
      "         [  0.6777,   0.5161,   0.5571,   0.5247,   0.4841],\n",
      "         [  0.5096,   0.4071,   0.5039,   0.4333,   0.4507],\n",
      "         [  0.5353,   0.4093,   0.4133,   0.4849,   0.4227],\n",
      "         [  0.7768,   0.6088,   0.6229,   0.6221,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.46522427]\n",
      "[0.19874998]\n",
      "[0.43916757]\n",
      "[0.50766556]\n",
      "[0.2337302]\n",
      "[0.7423122]\n",
      "[0.09637242]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8263, 0.8331, 0.3773, 0.8068, 0.7855, 0.5587, 0.9378],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.5755,   0.5699,   0.5469,   0.4151],\n",
      "         [  0.6726,   0.6252,   0.5600,   0.5245,   0.4412],\n",
      "         [  0.3554,   0.4470,   0.4846,   0.4595,   0.3941],\n",
      "         [  0.6777,   0.6325,   0.5571,   0.5247,   0.4841],\n",
      "         [  0.5096,   0.5219,   0.5039,   0.4333,   0.4507],\n",
      "         [  0.5353,   0.4815,   0.4133,   0.4849,   0.4227],\n",
      "         [  0.7768,   0.7612,   0.6229,   0.6221,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.94164199]\n",
      "[0.11094469]\n",
      "[0.1330915]\n",
      "[0.730654]\n",
      "[1.31137936]\n",
      "[0.93303862]\n",
      "[0.34458214]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9011, 0.8841, 0.6477, 0.8548, 0.9106, 0.5827, 0.9700],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.5755,   0.7079,   0.5469,   0.4151],\n",
      "         [  0.6726,   0.6252,   0.6952,   0.5245,   0.4412],\n",
      "         [  0.3554,   0.4470,   0.5583,   0.4595,   0.3941],\n",
      "         [  0.6777,   0.6325,   0.6847,   0.5247,   0.4841],\n",
      "         [  0.5096,   0.5219,   0.6406,   0.4333,   0.4507],\n",
      "         [  0.5353,   0.4815,   0.4876,   0.4849,   0.4227],\n",
      "         [  0.7768,   0.7612,   0.7815,   0.6221,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.90670781]\n",
      "[0.12535583]\n",
      "[0.10238077]\n",
      "[0.7645274]\n",
      "[1.32277114]\n",
      "[0.90311065]\n",
      "[0.26918812]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9122, 0.8611, 0.6632, 0.7556, 0.9065, 0.7588, 0.9548],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.5755,   0.7079,   0.6848,   0.4151],\n",
      "         [  0.6726,   0.6252,   0.6952,   0.6560,   0.4412],\n",
      "         [  0.3554,   0.4470,   0.5583,   0.5197,   0.3941],\n",
      "         [  0.6777,   0.6325,   0.6847,   0.6299,   0.4841],\n",
      "         [  0.5096,   0.5219,   0.6406,   0.5743,   0.4507],\n",
      "         [  0.5353,   0.4815,   0.4876,   0.5934,   0.4227],\n",
      "         [  0.7768,   0.7612,   0.7815,   0.7774,   0.6052]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.06179815]\n",
      "[0.18134512]\n",
      "[0.01716977]\n",
      "[0.32521496]\n",
      "[0.0584179]\n",
      "[0.06362197]\n",
      "[0.92884933]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([ 0.7919,  0.8241, -0.1020,  0.8204,  0.8519,  0.2925,  0.9574],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.6709,   0.5755,   0.7079,   0.6848,   0.5323],\n",
      "         [  0.6726,   0.6252,   0.6952,   0.6560,   0.5692],\n",
      "         [  0.3554,   0.4470,   0.5583,   0.5197,   0.2959],\n",
      "         [  0.6777,   0.6325,   0.6847,   0.6299,   0.5911],\n",
      "         [  0.5096,   0.5219,   0.6406,   0.5743,   0.5757],\n",
      "         [  0.5353,   0.4815,   0.4876,   0.5934,   0.4421],\n",
      "         [  0.7768,   0.7612,   0.7815,   0.7774,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.49957275]\n",
      "[0.17931039]\n",
      "[0.34040875]\n",
      "[0.50374347]\n",
      "[0.1832406]\n",
      "[0.42031754]\n",
      "[0.04481311]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8939, 0.9550, 0.8510, 0.9681, 0.7083, 0.8007, 0.9484],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.5755,   0.7079,   0.6848,   0.5323],\n",
      "         [  0.8285,   0.6252,   0.6952,   0.6560,   0.5692],\n",
      "         [  0.4826,   0.4470,   0.5583,   0.5197,   0.2959],\n",
      "         [  0.8369,   0.6325,   0.6847,   0.6299,   0.5911],\n",
      "         [  0.5904,   0.5219,   0.6406,   0.5743,   0.5757],\n",
      "         [  0.6566,   0.4815,   0.4876,   0.5934,   0.4421],\n",
      "         [  0.9327,   0.7612,   0.7815,   0.7774,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.81740645]\n",
      "[0.93862671]\n",
      "[0.77398058]\n",
      "[0.83613675]\n",
      "[0.85721097]\n",
      "[0.92248914]\n",
      "[0.75284765]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9335, 0.8380, 0.7493, 0.9330, 0.8402, 0.8886, 0.9687],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.7079,   0.6848,   0.5323],\n",
      "         [  0.8285,   0.7598,   0.6952,   0.6560,   0.5692],\n",
      "         [  0.4826,   0.5615,   0.5583,   0.5197,   0.2959],\n",
      "         [  0.8369,   0.7837,   0.6847,   0.6299,   0.5911],\n",
      "         [  0.5904,   0.6486,   0.6406,   0.5743,   0.5757],\n",
      "         [  0.6566,   0.6234,   0.4876,   0.5934,   0.4421],\n",
      "         [  0.9327,   0.9207,   0.7815,   0.7774,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.42263334]\n",
      "[0.19500685]\n",
      "[0.43096873]\n",
      "[1.46442041]\n",
      "[0.23713499]\n",
      "[0.80175201]\n",
      "[0.1418373]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8906, 0.8447, 0.8426, 0.9067, 0.7858, 0.7290, 0.9437],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.6848,   0.5323],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.6560,   0.5692],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.5197,   0.2959],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.6299,   0.5911],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.5743,   0.5757],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.5934,   0.4421],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.7774,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[1.45581942]\n",
      "[1.66812749]\n",
      "[0.68534491]\n",
      "[1.58238494]\n",
      "[0.30958706]\n",
      "[1.09153821]\n",
      "[0.04756957]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9060, 0.9369, 0.8868, 0.9657, 0.7350, 0.8217, 0.9528],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.5323],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.5692],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.2959],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.5911],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.5757],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.4421],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.7579]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.79924622]\n",
      "[0.89633752]\n",
      "[0.78642654]\n",
      "[0.81024923]\n",
      "[0.81014892]\n",
      "[0.78112398]\n",
      "[0.70173311]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8844, 0.9433, 0.9041, 0.9735, 0.7028, 0.8086, 0.9340],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.87303306]\n",
      "[1.00781679]\n",
      "[0.78768069]\n",
      "[0.8269091]\n",
      "[0.99795601]\n",
      "[0.89195585]\n",
      "[0.77137153]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8728, 0.9153, 0.8630, 0.8319, 0.8264, 0.7043, 0.9771],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1437,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1259,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1236,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1279,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0993,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1609,   0.0000,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.50637662]\n",
      "[0.30555928]\n",
      "[0.24734409]\n",
      "[0.49529023]\n",
      "[0.15384236]\n",
      "[0.33767109]\n",
      "[0.01641765]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8723, 0.9115, 0.8815, 0.9195, 0.8450, 0.7514, 0.9730],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.1406,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1437,   0.1468,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1259,   0.1361,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1236,   0.1430,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1279,   0.1359,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0993,   0.1149,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.1609,   0.1607,   0.0000,   0.0000,   0.0000]]])\n",
      "[0.49545859]\n",
      "[0.2689647]\n",
      "[0.30225554]\n",
      "[0.59279098]\n",
      "[0.12215343]\n",
      "[0.24007558]\n",
      "[0.01664211]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9379, 0.8965, 0.7646, 0.9062, 0.8170, 0.7045, 0.9836],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.1406,   0.1488,   0.0000,   0.0000],\n",
      "         [  0.1437,   0.1468,   0.1396,   0.0000,   0.0000],\n",
      "         [  0.1259,   0.1361,   0.1151,   0.0000,   0.0000],\n",
      "         [  0.1236,   0.1430,   0.1415,   0.0000,   0.0000],\n",
      "         [  0.1279,   0.1359,   0.1288,   0.0000,   0.0000],\n",
      "         [  0.0993,   0.1149,   0.1014,   0.0000,   0.0000],\n",
      "         [  0.1609,   0.1607,   0.1616,   0.0000,   0.0000]]])\n",
      "[0.38466799]\n",
      "[0.42059928]\n",
      "[0.28467721]\n",
      "[0.57698693]\n",
      "[0.09971113]\n",
      "[0.19600564]\n",
      "[0.03748663]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8580, 0.6717, 0.7962, 0.9009, 0.8173, 0.8519, 0.9441],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.1406,   0.1488,   0.1388,   0.0000],\n",
      "         [  0.1437,   0.1468,   0.1396,   0.1033,   0.0000],\n",
      "         [  0.1259,   0.1361,   0.1151,   0.1240,   0.0000],\n",
      "         [  0.1236,   0.1430,   0.1415,   0.1413,   0.0000],\n",
      "         [  0.1279,   0.1359,   0.1288,   0.1301,   0.0000],\n",
      "         [  0.0993,   0.1149,   0.1014,   0.1320,   0.0000],\n",
      "         [  0.1609,   0.1607,   0.1616,   0.1551,   0.0000]]])\n",
      "[0.04683058]\n",
      "[0.01455749]\n",
      "[0.07316558]\n",
      "[0.74371817]\n",
      "[0.02326372]\n",
      "[0.08666937]\n",
      "[0.12609362]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8404, 0.7868, 0.8903, 0.8487, 0.8615, 0.7610, 0.9646],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.1353,   0.1406,   0.1488,   0.1388,   0.1323],\n",
      "         [  0.1437,   0.1468,   0.1396,   0.1033,   0.1243],\n",
      "         [  0.1259,   0.1361,   0.1151,   0.1240,   0.1277],\n",
      "         [  0.1236,   0.1430,   0.1415,   0.1413,   0.1268],\n",
      "         [  0.1279,   0.1359,   0.1288,   0.1301,   0.1390],\n",
      "         [  0.0993,   0.1149,   0.1014,   0.1320,   0.1152],\n",
      "         [  0.1609,   0.1607,   0.1616,   0.1551,   0.1571]]])\n",
      "[0.45878588]\n",
      "[0.26513487]\n",
      "[0.32251921]\n",
      "[0.57054374]\n",
      "[0.16828287]\n",
      "[0.28662616]\n",
      "[0.02139747]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8692, 0.9458, 0.8122, 0.9507, 0.8837, 0.8627, 0.9186],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.1406,   0.1488,   0.1388,   0.1323],\n",
      "         [  0.2961,   0.1468,   0.1396,   0.1033,   0.1243],\n",
      "         [  0.2418,   0.1361,   0.1151,   0.1240,   0.1277],\n",
      "         [  0.2782,   0.1430,   0.1415,   0.1413,   0.1268],\n",
      "         [  0.2498,   0.1359,   0.1288,   0.1301,   0.1390],\n",
      "         [  0.2281,   0.1149,   0.1014,   0.1320,   0.1152],\n",
      "         [  0.3099,   0.1607,   0.1616,   0.1551,   0.1571]]])\n",
      "[1.03923018]\n",
      "[0.9028543]\n",
      "[0.74707417]\n",
      "[1.13382905]\n",
      "[0.77296035]\n",
      "[0.80932582]\n",
      "[0.79818587]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8472, 0.8876, 0.6595, 0.9373, 0.6208, 0.8549, 0.9770],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.2702,   0.1488,   0.1388,   0.1323],\n",
      "         [  0.2961,   0.2822,   0.1396,   0.1033,   0.1243],\n",
      "         [  0.2418,   0.2257,   0.1151,   0.1240,   0.1277],\n",
      "         [  0.2782,   0.2918,   0.1415,   0.1413,   0.1268],\n",
      "         [  0.2498,   0.1963,   0.1288,   0.1301,   0.1390],\n",
      "         [  0.2281,   0.2246,   0.1014,   0.1320,   0.1152],\n",
      "         [  0.3099,   0.3195,   0.1616,   0.1551,   0.1571]]])\n",
      "[1.62870839]\n",
      "[1.5875119]\n",
      "[0.3298151]\n",
      "[1.40661871]\n",
      "[0.32525665]\n",
      "[0.84154561]\n",
      "[0.02968318]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7600, 0.8732, 0.7608, 0.9008, 0.6590, 0.7201, 0.9243],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.2702,   0.2532,   0.1388,   0.1323],\n",
      "         [  0.2961,   0.2822,   0.2754,   0.1033,   0.1243],\n",
      "         [  0.2418,   0.2257,   0.2052,   0.1240,   0.1277],\n",
      "         [  0.2782,   0.2918,   0.2747,   0.1413,   0.1268],\n",
      "         [  0.2498,   0.1963,   0.2137,   0.1301,   0.1390],\n",
      "         [  0.2281,   0.2246,   0.1926,   0.1320,   0.1152],\n",
      "         [  0.3099,   0.3195,   0.3062,   0.1551,   0.1571]]])\n",
      "[0.15075074]\n",
      "[0.08817591]\n",
      "[0.06964356]\n",
      "[1.01092688]\n",
      "[0.01894608]\n",
      "[0.35232043]\n",
      "[0.1039875]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7981, 0.9232, 0.5931, 0.9096, 0.6309, 0.6734, 0.9682],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.2702,   0.2532,   0.2627,   0.1323],\n",
      "         [  0.2961,   0.2822,   0.2754,   0.2478,   0.1243],\n",
      "         [  0.2418,   0.2257,   0.2052,   0.1959,   0.1277],\n",
      "         [  0.2782,   0.2918,   0.2747,   0.2853,   0.1268],\n",
      "         [  0.2498,   0.1963,   0.2137,   0.1955,   0.1390],\n",
      "         [  0.2281,   0.2246,   0.1926,   0.2142,   0.1152],\n",
      "         [  0.3099,   0.3195,   0.3062,   0.3131,   0.1571]]])\n",
      "[1.73425348]\n",
      "[1.57018927]\n",
      "[0.4615416]\n",
      "[1.35533556]\n",
      "[0.4581574]\n",
      "[1.18314176]\n",
      "[0.03481445]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9245, 0.9515, 0.8180, 0.9361, 0.8820, 0.8545, 0.9695],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.2724,   0.2702,   0.2532,   0.2627,   0.2784],\n",
      "         [  0.2961,   0.2822,   0.2754,   0.2478,   0.2789],\n",
      "         [  0.2418,   0.2257,   0.2052,   0.1959,   0.2411],\n",
      "         [  0.2782,   0.2918,   0.2747,   0.2853,   0.2774],\n",
      "         [  0.2498,   0.1963,   0.2137,   0.1955,   0.2759],\n",
      "         [  0.2281,   0.2246,   0.1926,   0.2142,   0.2381],\n",
      "         [  0.3099,   0.3195,   0.3062,   0.3131,   0.3153]]])\n",
      "[0.91670077]\n",
      "[0.73888932]\n",
      "[0.79302539]\n",
      "[1.04479031]\n",
      "[0.73228374]\n",
      "[0.85586826]\n",
      "[0.41792453]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8923, 0.9210, 0.9267, 0.9175, 0.8643, 0.7922, 0.9437],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.2702,   0.2532,   0.2627,   0.2784],\n",
      "         [  0.4447,   0.2822,   0.2754,   0.2478,   0.2789],\n",
      "         [  0.3849,   0.2257,   0.2052,   0.1959,   0.2411],\n",
      "         [  0.4215,   0.2918,   0.2747,   0.2853,   0.2774],\n",
      "         [  0.3855,   0.1963,   0.2137,   0.1955,   0.2759],\n",
      "         [  0.3385,   0.2246,   0.1926,   0.2142,   0.2381],\n",
      "         [  0.4645,   0.3195,   0.3062,   0.3131,   0.3153]]])\n",
      "[1.86145487]\n",
      "[1.51861001]\n",
      "[0.84629884]\n",
      "[1.56513154]\n",
      "[0.25156617]\n",
      "[1.07416911]\n",
      "[0.02086842]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9649, 0.9521, 0.9092, 0.9262, 0.7820, 0.8836, 0.9738],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.4235,   0.2532,   0.2627,   0.2784],\n",
      "         [  0.4447,   0.4320,   0.2754,   0.2478,   0.2789],\n",
      "         [  0.3849,   0.3695,   0.2052,   0.1959,   0.2411],\n",
      "         [  0.4215,   0.4380,   0.2747,   0.2853,   0.2774],\n",
      "         [  0.3855,   0.3178,   0.2137,   0.1955,   0.2759],\n",
      "         [  0.3385,   0.3591,   0.1926,   0.2142,   0.2381],\n",
      "         [  0.4645,   0.4789,   0.3062,   0.3131,   0.3153]]])\n",
      "[0.15147468]\n",
      "[0.11778679]\n",
      "[0.05467035]\n",
      "[0.53493621]\n",
      "[0.03015393]\n",
      "[0.52362401]\n",
      "[0.05105766]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9358, 0.9560, 0.8056, 0.9536, 0.7708, 0.8939, 0.9574],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.4235,   0.4053,   0.2627,   0.2784],\n",
      "         [  0.4447,   0.4320,   0.4306,   0.2478,   0.2789],\n",
      "         [  0.3849,   0.3695,   0.3336,   0.1959,   0.2411],\n",
      "         [  0.4215,   0.4380,   0.4294,   0.2853,   0.2774],\n",
      "         [  0.3855,   0.3178,   0.3296,   0.1955,   0.2759],\n",
      "         [  0.3385,   0.3591,   0.3312,   0.2142,   0.2381],\n",
      "         [  0.4645,   0.4789,   0.4636,   0.3131,   0.3153]]])\n",
      "[1.32432995]\n",
      "[0.21870187]\n",
      "[0.41661458]\n",
      "[1.50749058]\n",
      "[0.18137813]\n",
      "[0.63030705]\n",
      "[0.03422566]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9473, 0.9542, 0.9306, 0.9561, 0.8789, 0.8096, 0.9719],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.4235,   0.4053,   0.4191,   0.2784],\n",
      "         [  0.4447,   0.4320,   0.4306,   0.4048,   0.2789],\n",
      "         [  0.3849,   0.3695,   0.3336,   0.3486,   0.2411],\n",
      "         [  0.4215,   0.4380,   0.4294,   0.4426,   0.2774],\n",
      "         [  0.3855,   0.3178,   0.3296,   0.3351,   0.2759],\n",
      "         [  0.3385,   0.3591,   0.3312,   0.3389,   0.2381],\n",
      "         [  0.4645,   0.4789,   0.4636,   0.4740,   0.3153]]])\n",
      "[1.00447246]\n",
      "[1.04848191]\n",
      "[1.03643248]\n",
      "[1.02706106]\n",
      "[0.9797846]\n",
      "[0.86779999]\n",
      "[0.16226901]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9376, 0.9421, 0.9056, 0.9346, 0.8130, 0.9006, 0.9777],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.4134,   0.4235,   0.4053,   0.4191,   0.4279],\n",
      "         [  0.4447,   0.4320,   0.4306,   0.4048,   0.4294],\n",
      "         [  0.3849,   0.3695,   0.3336,   0.3486,   0.3850],\n",
      "         [  0.4215,   0.4380,   0.4294,   0.4426,   0.4199],\n",
      "         [  0.3855,   0.3178,   0.3296,   0.3351,   0.4005],\n",
      "         [  0.3385,   0.3591,   0.3312,   0.3389,   0.3802],\n",
      "         [  0.4645,   0.4789,   0.4636,   0.4740,   0.4743]]])\n",
      "[0.08763258]\n",
      "[0.05735593]\n",
      "[0.04264209]\n",
      "[1.05918316]\n",
      "[0.02122396]\n",
      "[0.28438902]\n",
      "[0.04140599]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8202, 0.8390, 0.8717, 0.8473, 0.8760, 0.7509, 0.9501],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.4235,   0.4053,   0.4191,   0.4279],\n",
      "         [  0.5568,   0.4320,   0.4306,   0.4048,   0.4294],\n",
      "         [  0.5126,   0.3695,   0.3336,   0.3486,   0.3850],\n",
      "         [  0.5456,   0.4380,   0.4294,   0.4426,   0.4199],\n",
      "         [  0.5217,   0.3178,   0.3296,   0.3351,   0.4005],\n",
      "         [  0.4413,   0.3591,   0.3312,   0.3389,   0.3802],\n",
      "         [  0.6164,   0.4789,   0.4636,   0.4740,   0.4743]]])\n",
      "[0.09461352]\n",
      "[0.0442874]\n",
      "[0.09818073]\n",
      "[0.50338873]\n",
      "[0.04037575]\n",
      "[0.15212996]\n",
      "[0.27100685]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8714, 0.9032, 0.8018, 0.8298, 0.8164, 0.6976, 0.9675],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.5579,   0.4053,   0.4191,   0.4279],\n",
      "         [  0.5568,   0.5689,   0.4306,   0.4048,   0.4294],\n",
      "         [  0.5126,   0.4912,   0.3336,   0.3486,   0.3850],\n",
      "         [  0.5456,   0.5670,   0.4294,   0.4426,   0.4199],\n",
      "         [  0.5217,   0.4305,   0.3296,   0.3351,   0.4005],\n",
      "         [  0.4413,   0.4599,   0.3312,   0.3389,   0.3802],\n",
      "         [  0.6164,   0.6357,   0.4636,   0.4740,   0.4743]]])\n",
      "[0.15080419]\n",
      "[0.08909571]\n",
      "[0.09755479]\n",
      "[0.36079898]\n",
      "[0.02271132]\n",
      "[0.35420229]\n",
      "[0.14253775]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8421, 0.7956, 0.7134, 0.7725, 0.7844, 0.5895, 0.9612],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.5579,   0.5333,   0.4191,   0.4279],\n",
      "         [  0.5568,   0.5689,   0.5524,   0.4048,   0.4294],\n",
      "         [  0.5126,   0.4912,   0.4386,   0.3486,   0.3850],\n",
      "         [  0.5456,   0.5670,   0.5420,   0.4426,   0.4199],\n",
      "         [  0.5217,   0.4305,   0.4344,   0.3351,   0.4005],\n",
      "         [  0.4413,   0.4599,   0.4127,   0.3389,   0.3802],\n",
      "         [  0.6164,   0.6357,   0.6197,   0.4740,   0.4743]]])\n",
      "[0.12940099]\n",
      "[0.06675622]\n",
      "[0.09235374]\n",
      "[0.35689947]\n",
      "[0.0253621]\n",
      "[0.30688407]\n",
      "[0.16710192]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.6986, 0.8622, 0.7454, 0.8669, 0.8773, 0.7800, 0.9783],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.5579,   0.5333,   0.5214,   0.4279],\n",
      "         [  0.5568,   0.5689,   0.5524,   0.5410,   0.4294],\n",
      "         [  0.5126,   0.4912,   0.4386,   0.4439,   0.3850],\n",
      "         [  0.5456,   0.5670,   0.5420,   0.5721,   0.4199],\n",
      "         [  0.5217,   0.4305,   0.4344,   0.4709,   0.4005],\n",
      "         [  0.4413,   0.4599,   0.4127,   0.4493,   0.3802],\n",
      "         [  0.6164,   0.6357,   0.6197,   0.6337,   0.4743]]])\n",
      "[0.07029603]\n",
      "[0.24464717]\n",
      "[0.0163489]\n",
      "[0.3198633]\n",
      "[0.05215768]\n",
      "[0.08488725]\n",
      "[0.96870946]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8634, 0.8190, 0.8260, 0.8389, 0.7849, 0.8943, 0.9487],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.5362,   0.5579,   0.5333,   0.5214,   0.5524],\n",
      "         [  0.5568,   0.5689,   0.5524,   0.5410,   0.5540],\n",
      "         [  0.5126,   0.4912,   0.4386,   0.4439,   0.4982],\n",
      "         [  0.5456,   0.5670,   0.5420,   0.5721,   0.5305],\n",
      "         [  0.5217,   0.4305,   0.4344,   0.4709,   0.5012],\n",
      "         [  0.4413,   0.4599,   0.4127,   0.4493,   0.5159],\n",
      "         [  0.6164,   0.6357,   0.6197,   0.6337,   0.6264]]])\n",
      "[0.13091599]\n",
      "[0.07334752]\n",
      "[0.07823339]\n",
      "[0.32334844]\n",
      "[0.01952754]\n",
      "[0.26788532]\n",
      "[0.10005937]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8961, 0.8332, 0.5096, 0.7289, 0.8864, 0.8770, 0.9349],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.5579,   0.5333,   0.5214,   0.5524],\n",
      "         [  0.6799,   0.5689,   0.5524,   0.5410,   0.5540],\n",
      "         [  0.5423,   0.4912,   0.4386,   0.4439,   0.4982],\n",
      "         [  0.6478,   0.5670,   0.5420,   0.5721,   0.5305],\n",
      "         [  0.6570,   0.4305,   0.4344,   0.4709,   0.5012],\n",
      "         [  0.5634,   0.4599,   0.4127,   0.4493,   0.5159],\n",
      "         [  0.7651,   0.6357,   0.6197,   0.6337,   0.6264]]])\n",
      "[0.08522691]\n",
      "[0.15789888]\n",
      "[0.02588735]\n",
      "[0.39712844]\n",
      "[0.07439873]\n",
      "[0.07620783]\n",
      "[0.98786029]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.7922, 0.8592, 0.4868, 0.8406, 0.8783, 0.6149, 0.9601],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.6797,   0.5333,   0.5214,   0.5524],\n",
      "         [  0.6799,   0.7017,   0.5524,   0.5410,   0.5540],\n",
      "         [  0.5423,   0.5245,   0.4386,   0.4439,   0.4982],\n",
      "         [  0.6478,   0.6911,   0.5420,   0.5721,   0.5305],\n",
      "         [  0.6570,   0.5642,   0.4344,   0.4709,   0.5012],\n",
      "         [  0.5634,   0.5354,   0.4127,   0.4493,   0.5159],\n",
      "         [  0.7651,   0.7884,   0.6197,   0.6337,   0.6264]]])\n",
      "[0.58977323]\n",
      "[0.21970921]\n",
      "[0.27124005]\n",
      "[0.43793328]\n",
      "[0.27141379]\n",
      "[0.57146329]\n",
      "[0.04819385]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9145, 0.8583, 0.6892, 0.8871, 0.8743, 0.8618, 0.9765],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.6797,   0.6758,   0.5214,   0.5524],\n",
      "         [  0.6799,   0.7017,   0.6889,   0.5410,   0.5540],\n",
      "         [  0.5423,   0.5245,   0.5219,   0.4439,   0.4982],\n",
      "         [  0.6478,   0.6911,   0.6755,   0.5721,   0.5305],\n",
      "         [  0.6570,   0.5642,   0.5725,   0.4709,   0.5012],\n",
      "         [  0.5634,   0.5354,   0.5366,   0.4493,   0.5159],\n",
      "         [  0.7651,   0.7884,   0.7794,   0.6337,   0.6264]]])\n",
      "[0.48612046]\n",
      "[0.1472635]\n",
      "[0.23670529]\n",
      "[0.48414001]\n",
      "[0.22888138]\n",
      "[0.44050232]\n",
      "[0.03540915]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8992, 0.8766, 0.5400, 0.9115, 0.8632, 0.7303, 0.9723],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.6797,   0.6758,   0.6622,   0.5524],\n",
      "         [  0.6799,   0.7017,   0.6889,   0.6784,   0.5540],\n",
      "         [  0.5423,   0.5245,   0.5219,   0.5015,   0.4982],\n",
      "         [  0.6478,   0.6911,   0.6755,   0.7137,   0.5305],\n",
      "         [  0.6570,   0.5642,   0.5725,   0.6012,   0.5012],\n",
      "         [  0.5634,   0.5354,   0.5366,   0.5512,   0.5159],\n",
      "         [  0.7651,   0.7884,   0.7794,   0.7917,   0.6264]]])\n",
      "[0.57643014]\n",
      "[0.21156527]\n",
      "[0.23331184]\n",
      "[0.59615867]\n",
      "[0.25297331]\n",
      "[0.46097798]\n",
      "[0.1156767]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8760, 0.8772, 0.0378, 0.6912, 0.8610, 0.7559, 0.9728],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.6663,   0.6797,   0.6758,   0.6622,   0.6846],\n",
      "         [  0.6799,   0.7017,   0.6889,   0.6784,   0.6856],\n",
      "         [  0.5423,   0.5245,   0.5219,   0.5015,   0.4673],\n",
      "         [  0.6478,   0.6911,   0.6755,   0.7137,   0.6237],\n",
      "         [  0.6570,   0.5642,   0.5725,   0.6012,   0.6336],\n",
      "         [  0.5634,   0.5354,   0.5366,   0.5512,   0.6211],\n",
      "         [  0.7651,   0.7884,   0.7794,   0.7917,   0.7833]]])\n",
      "[0.10715771]\n",
      "[0.33159249]\n",
      "[0.0197274]\n",
      "[0.382551]\n",
      "[0.0900454]\n",
      "[0.11822554]\n",
      "[0.92871586]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9567, 0.8587, 0.8067, 0.9281, 0.5900, 0.8749, 0.9704],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.6797,   0.6758,   0.6622,   0.6846],\n",
      "         [  0.8157,   0.7017,   0.6889,   0.6784,   0.6856],\n",
      "         [  0.6619,   0.5245,   0.5219,   0.5015,   0.4673],\n",
      "         [  0.7941,   0.6911,   0.6755,   0.7137,   0.6237],\n",
      "         [  0.7122,   0.5642,   0.5725,   0.6012,   0.6336],\n",
      "         [  0.6945,   0.5354,   0.5366,   0.5512,   0.6211],\n",
      "         [  0.9247,   0.7884,   0.7794,   0.7917,   0.7833]]])\n",
      "[1.41675444]\n",
      "[0.24371413]\n",
      "[0.44155784]\n",
      "[1.45378398]\n",
      "[0.26089476]\n",
      "[0.74008987]\n",
      "[0.14834033]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9067, 0.8935, 0.8809, 0.8907, 0.7977, 0.7969, 0.9539],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.8234,   0.6758,   0.6622,   0.6846],\n",
      "         [  0.8157,   0.8459,   0.6889,   0.6784,   0.6856],\n",
      "         [  0.6619,   0.6625,   0.5219,   0.5015,   0.4673],\n",
      "         [  0.7941,   0.8301,   0.6755,   0.7137,   0.6237],\n",
      "         [  0.7122,   0.6747,   0.5725,   0.6012,   0.6336],\n",
      "         [  0.6945,   0.6588,   0.5366,   0.5512,   0.6211],\n",
      "         [  0.9247,   0.9435,   0.7794,   0.7917,   0.7833]]])\n",
      "[0.10853332]\n",
      "[0.03323938]\n",
      "[0.10343785]\n",
      "[1.12715287]\n",
      "[0.02280728]\n",
      "[0.19340472]\n",
      "[0.14759011]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8891, 0.9157, 0.8950, 0.9405, 0.8916, 0.8023, 0.9625],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.8234,   0.8209,   0.6622,   0.6846],\n",
      "         [  0.8157,   0.8459,   0.8386,   0.6784,   0.6856],\n",
      "         [  0.6619,   0.6625,   0.6668,   0.5015,   0.4673],\n",
      "         [  0.7941,   0.8301,   0.8298,   0.7137,   0.6237],\n",
      "         [  0.7122,   0.6747,   0.6974,   0.6012,   0.6336],\n",
      "         [  0.6945,   0.6588,   0.6621,   0.5512,   0.6211],\n",
      "         [  0.9247,   0.9435,   0.9380,   0.7917,   0.7833]]])\n",
      "[0.92758152]\n",
      "[0.86330696]\n",
      "[0.68388785]\n",
      "[0.89206254]\n",
      "[0.91026449]\n",
      "[0.72562835]\n",
      "[0.19317769]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.8721, 0.7618, 0.9061, 0.9153, 0.5831, 0.6935, 0.9306],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.8234,   0.8209,   0.8019,   0.6846],\n",
      "         [  0.8157,   0.8459,   0.8386,   0.7986,   0.6856],\n",
      "         [  0.6619,   0.6625,   0.6668,   0.6450,   0.4673],\n",
      "         [  0.7941,   0.8301,   0.8298,   0.8624,   0.6237],\n",
      "         [  0.7122,   0.6747,   0.6974,   0.6730,   0.6336],\n",
      "         [  0.6945,   0.6588,   0.6621,   0.6533,   0.6211],\n",
      "         [  0.9247,   0.9435,   0.9380,   0.9437,   0.7833]]])\n",
      "[1.79186475]\n",
      "[1.85555563]\n",
      "[0.95505196]\n",
      "[1.41362753]\n",
      "[0.33102971]\n",
      "[1.36017382]\n",
      "[0.05831114]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "tensor([0.9207, 0.8904, 0.9067, 0.9356, 0.8385, 0.7727, 0.9440],\n",
      "       dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor([[[ -1.7148,  -1.1841, -20.9964,  -3.0956,  -1.5254],\n",
      "         [ -1.1192,  -3.2352, -10.8636,  -1.6272,  -1.4525],\n",
      "         [ -2.9225,  -2.0566, -73.8253, -14.5816,  -3.2874],\n",
      "         [ -1.1343,  -0.9854,  -2.0169, -16.7396,  -3.1631],\n",
      "         [ -3.1955,  -2.6328,  -1.0915,  -4.2357,  -2.9870],\n",
      "         [ -5.5799,  -9.3941, -12.7869, -17.9943,  -1.6477],\n",
      "         [  0.3381,  -0.1431,  -9.6709,  -3.2836,  -1.4765]],\n",
      "\n",
      "        [[  0.6268,   0.7248,   0.4370,   0.5294,   0.6726],\n",
      "         [  0.6315,   0.5897,   0.5674,   0.2527,   0.7083],\n",
      "         [  0.0954,   0.4934,  -0.2213,   0.3460,   0.4504],\n",
      "         [  0.6531,   0.7175,   0.5954,   0.7175,   0.5387],\n",
      "         [  0.4433,   0.5996,   0.3831,   0.2421,   0.6080],\n",
      "         [  0.2356,   0.5903,  -0.1425,   0.5323,   0.4959],\n",
      "         [  0.9316,   0.8929,   0.8679,   0.8475,   0.8997]],\n",
      "\n",
      "        [[  0.7759,   0.7179,   0.7851,   0.7509,   0.7398],\n",
      "         [  0.6716,   0.7077,   0.7590,   0.7598,   0.7297],\n",
      "         [  0.5927,   0.6307,   0.5341,   0.5282,   0.4528],\n",
      "         [  0.7462,   0.7643,   0.8024,   0.7871,   0.7086],\n",
      "         [  0.6062,   0.6770,   0.6747,   0.6971,   0.5769],\n",
      "         [  0.5458,   0.5308,   0.5548,   0.5677,   0.4823],\n",
      "         [  0.9115,   0.9076,   0.9236,   0.9124,   0.8768]],\n",
      "\n",
      "        [[  0.7599,   0.7748,   0.7682,   0.7339,   0.7985],\n",
      "         [  0.6832,   0.7200,   0.7480,   0.6997,   0.8210],\n",
      "         [  0.4343,   0.5658,   0.2892,   0.5553,   0.5600],\n",
      "         [  0.7432,   0.7796,   0.7531,   0.7550,   0.8365],\n",
      "         [  0.6222,   0.6793,   0.7085,   0.5369,   0.7334],\n",
      "         [  0.4786,   0.5967,   0.4421,   0.6346,   0.6028],\n",
      "         [  0.9055,   0.9360,   0.9006,   0.8910,   0.9115]],\n",
      "\n",
      "        [[  0.8138,   0.7265,   0.8510,   0.8324,   0.6764],\n",
      "         [  0.8285,   0.7598,   0.8276,   0.8099,   0.7236],\n",
      "         [  0.4826,   0.5615,   0.6888,   0.6575,   0.4296],\n",
      "         [  0.8369,   0.7837,   0.8300,   0.7883,   0.7506],\n",
      "         [  0.5904,   0.6486,   0.7523,   0.6774,   0.6660],\n",
      "         [  0.6566,   0.6234,   0.5971,   0.7226,   0.5693],\n",
      "         [  0.9327,   0.9207,   0.9364,   0.9339,   0.9118]],\n",
      "\n",
      "        [[  0.8180,   0.8234,   0.8209,   0.8019,   0.8323],\n",
      "         [  0.8157,   0.8459,   0.8386,   0.7986,   0.8284],\n",
      "         [  0.6619,   0.6625,   0.6668,   0.6450,   0.6114],\n",
      "         [  0.7941,   0.8301,   0.8298,   0.8624,   0.7737],\n",
      "         [  0.7122,   0.6747,   0.6974,   0.6730,   0.7603],\n",
      "         [  0.6945,   0.6588,   0.6621,   0.6533,   0.7391],\n",
      "         [  0.9247,   0.9435,   0.9380,   0.9437,   0.9383]]])\n",
      "[1.53329401]\n",
      "[1.61639087]\n",
      "[0.81647532]\n",
      "[1.36927962]\n",
      "[0.2425125]\n",
      "[0.91654456]\n",
      "[0.04919384]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:237: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_mean = torch.tensor(R2_score.mean(axis=0))\n",
      "/Users/pmzcwl/Documents/GitHub/Calibration/GPE_ensemble.py:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R2_std = torch.tensor(R2_score.std(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "reps=5\n",
    "nn=[10,20,30,40,50,60]\n",
    "R2=torch.zeros(7,len(nn),7,reps)\n",
    "ISE=torch.zeros(7,len(nn),7,reps)\n",
    "Ti=torch.zeros(7,len(nn),reps)\n",
    "\n",
    "for num, n in enumerate(nn):\n",
    "    for k in range(len(emulators)):\n",
    "        emulators2=emulators.copy()\n",
    "        emulators2.pop(k)\n",
    "        print(len(emulators2))\n",
    "\n",
    "        X_train = train_input[k]\n",
    "        y_train = train_output[k]\n",
    "        X_test = test_input[k]\n",
    "        y_test = test_output[k]\n",
    "        \n",
    "        for i in range(reps):\n",
    "\n",
    "            b=np.random.choice(range(X_train.shape[0]),n,replace=False)\n",
    "\n",
    "            start = time.time()\n",
    "            model_f=GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"linear\",training_iter=500)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_f.R2_sample(X_test,y_test,1000)\n",
    "            R2[0,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[0,num,:,i]+=model_f.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[0,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "\n",
    "            em=np.random.randint(len(emulators2))\n",
    "            start = time.time()\n",
    "            model_dc_1 = GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]],a=torch.tensor([[1],[1],[1],[1],[1],[1],[1]]))\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_1.R2_sample(X_test,y_test,1000)\n",
    "            R2[1,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[1,num,:,i]+=model_dc_1.ISE(X_test,y_test)/(len(emulators))\n",
    "            print(model_dc_1.R2(X_test,y_test))\n",
    "            print(R2[1])\n",
    "\n",
    "            Ti[1,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            m0 = emulators2[em].predict(X_train[b,:])\n",
    "            a_d=np.zeros((y_train.shape[1],1))\n",
    "            for l in range(y_train.shape[1]):\n",
    "                result = scipy.optimize.minimize(proxy, 1, args=(y_train[b,:],m0,l), method='Nelder-Mead', tol=1e-8)\n",
    "                print(result.x)\n",
    "                a_d[l]=result.x\n",
    "            a_d=torch.tensor(a_d)\n",
    "            model_dc_reg = GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]],a=a_d)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_reg.R2_sample(X_test,y_test,1000)\n",
    "            R2[2,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[2,num,:,i]+=model_dc_reg.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[2,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_learned = GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=[emulators2[em]])\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[3,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[3,num,:,i]+=model_dc_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[3,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_all = GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_all.R2_sample(X_test,y_test,1000)\n",
    "            R2[4,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[4,num,:,i]+=model_dc_all.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[4,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            a_d=torch.zeros((y_train.shape[1],len(emulators2)))\n",
    "            for j in range(y_train.shape[1]):\n",
    "                m0=m0_mat(y_train[b],emulators2,X_train[b],j)\n",
    "                # fit to an order-3 polynomial data\n",
    "                y_t=(y_train[b,j]-y_train.mean(axis=0)[j])/y_train.std(axis=0)[j]\n",
    "                model = model.fit(m0.detach().numpy(), y_t.detach().numpy())\n",
    "                a_d[j]=torch.tensor(model.named_steps['lasso'].coef_)\n",
    "\n",
    "\n",
    "            model_dc_lasso=GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_lasso.R2_sample(X_test,y_test,1000)\n",
    "            R2[5,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[5,num,:,i]+=model_dc_lasso.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[5,num,i]+=(end-start)/(len(emulators))\n",
    "\n",
    "            start = time.time()\n",
    "            model_dc_lasso_learned=GPE.ensemble(X_train[b,:],y_train[b,:],mean_func=\"discrepancy_cohort\",training_iter=500,ref_emulator=emulators2,a=a_d,a_indicator=True)\n",
    "            end = time.time()\n",
    "            R2temp,R2std=model_dc_lasso_learned.R2_sample(X_test,y_test,1000)\n",
    "            R2[6,num ,:,i]+=R2temp/(len(emulators))\n",
    "            ISE[6,num,:,i]+=model_dc_lasso_learned.ISE(X_test,y_test)/(len(emulators))\n",
    "\n",
    "            Ti[6,num,i]+=(end-start)/(len(emulators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bdc8cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGeCAYAAABGlgGHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAADDq0lEQVR4nOz9eZAk6X3fB3+ePOq++u6ee7Cz92B3sbsgsIBAmSINGaJxWTKhly9AQ6QiDMkWA1ybEcBrkxGAaK6kVVBg0FwYMEmJAGQHZFKk8FogqWUoYIoApVdYYQgsFtxzZufqmb7rrsrjed4/Mivr6Oqjuqu7urqfT2xtZWVlZWX3VFd+83d8f0IppdBoNBqNRqMZEcaoD0Cj0Wg0Gs3JRosRjUaj0Wg0I0WLEY1Go9FoNCNFixGNRqPRaDQjRYsRjUaj0Wg0I0WLEY1Go9FoNCNFixGNRqPRaDQjRYsRjUaj0Wg0I0WLEY1Go9FoNCPFGvUB7AYpJbdv3yabzSKEGPXhaDQajUaj2QVKKcrlMqdOncIwtol/qD3w67/+6+rChQsqHo+rxx9/XP3Jn/zJttt/5StfUY888ohKJpNqfn5effzjH1crKyu7fr8bN24oQN/0Td/0Td/0Td/G8Hbjxo1tz/NCqcFm03z1q1/lYx/7GM899xzvfve7+cIXvsBv/MZv8NJLL3Hu3LlN2//pn/4pf/kv/2X+yT/5J7z//e/n1q1bfOITn+Dee+/l937v93b1nsVikUKhwI0bN8jlcoMcrkaj0Wg0mhFRKpU4e/YsGxsb5PP5LbcbWIy84x3v4PHHH+fzn/98tO7BBx/kQx/6EM8888ym7f/xP/7HfP7zn+f111+P1v3ar/0a/+gf/SNu3Lixq/cslUrk83mKxaIWIxqNRqPRjAm7PX8PVMDqOA4vvPAC733ve7vWv/e97+Vb3/pW39e8613v4ubNm3z9619HKcXdu3f5nd/5HX78x398y/dpNpuUSqWum0aj0Wg0muPJQGJkZWUF3/eZm5vrWj83N8edO3f6vuZd73oX//yf/3M+8pGPEIvFmJ+fp1Ao8Gu/9mtbvs8zzzxDPp+PbmfPnh3kMDUajUaj0YwRe2rt7e1oUUpt2eXy0ksv8bM/+7P84i/+Ii+88AJ/+Id/yNWrV/nEJz6x5f4//elPUywWo9tu0zkajUaj0WjGj4Fae6enpzFNc1MUZGlpaVO0pMUzzzzDu9/9bn7+538egEceeYR0Os173vMefumXfomFhYVNr4nH48Tj8UEOTaPRaDQazZgyUGQkFovxxBNP8Pzzz3etf/7553nXu97V9zW1Wm1Tb7FpmkAQUdFoNBqNRnOyGThN8/TTT/Mbv/Eb/NZv/RY/+MEP+Lmf+zmuX78epV0+/elP81M/9VPR9u9///v5l//yX/L5z3+eN954g29+85v87M/+LD/0Qz/EqVOnhveTaDQajUajGUsGdmD9yEc+wurqKp/97GdZXFzk8uXLfP3rX+f8+fMALC4ucv369Wj7j3/845TLZf7X//V/5X/4H/4HCoUCf+Wv/BX+4T/8h8P7KTQajUaj0YwtA/uMjALtM6LRaDQazfhxID4jGo1Go9FoNMNGixGNRqPRaDQjRYsRjUaj0Wg0I0WLEY1Go9FoNCNFixGNRqPRaDQjZeDWXo1Go9FoNKNHSQVSgQpNRFvLUoFSIAGlwucAGS6r1jLhaxTWVBIjOTpJoMWIRqPRaDTb0HlyD07mHSd62Xly77NeKVS4bpNwkD0iomN5O+EQvW6IGGlbixGNRqPRaLpP4O1lZJ+Te3hi7jpJK/qIg62iBR37aImF1om+9zg0B44WIxqNRnPC6LyC771y7zpJt5a3TAWwKTqw3f66REFruTOVoM/7JxYtRjQajeaIozyJrLn4VQ/l+B3RAjaf5HvrBHpEhT7pa44iWoxoNBrNEUI6PrLmIasusuYG901/1Iel0RwoWoxoNBrNiJBNv0t0+LUw8tGD70o8x8dzJbKjhkF0btTzQIju50TnBv0Xgweivbb3uf7v1/NeQOeKjt11Lfdu27u6fSzdz4lNBxLso+NwNGOIFiMajUZzwCilUA0/FBxuFPlQnmxvIxWuI/FD0dESH54rh945cdLoFTD9BVgfCda72Pm4V4FF++izvo9+2n4fvccEvYpPdKqv3t1s2m3Hzrb4PRgNf6SCQIsRjUajGSJKKmTdC6IdrXRL1Y26MjqjHJ4jcV0f35H4vtxhz5q9ouiuk1HdT27a+iSSqLukRvj+WoxoNBrNHlG+QtbDuo5qhwDxpY5yaDQDoMWIRqPR7IKgo8ULu1pcZMXFLTt4jh8ID9fHdQMB4vtacGiOIEqBrxCeRLiy677xx9dx/ppJ7Ex2JIemxYhGo9H0oFwZ1HZUXbySQ3O9iVdx2mmVMMWiDimkr6SP5zTwnDrS94IaAMMIayGCQgBhiOhx8Hy4Pnrc2h6EMNrbiWCbzVWomrFBKfD6i4xNy1t8ZL3VBu5yXYsRjUajGQWy6eNXHJobTZyNJu5GE6/mRtGOUUQ5pOfiNet4Th3XaSA951DeNxIlPUIlWG8Qdc4IIyq+7CdshGhVbRrh9gKBsf3+tTDajNpCYIT3eKq9bpDdmgJlGSjbiO7TlwrETmcO7EfZCS1GNBrNiUAphVdxaaw1cFrCo9TEq3v4rjq0KEefA8Nzm/ih8PCcOsofja9INHeFEZdxtlqRO8QMoZgRXWKGSBhFUaIOcSNaokkQvT4SQJvET6/w4uCEkdxFFMOVQUpll7tUAFYoMnqERte9ZYRRs25ib53Gnh1dCasWIxqN5lihpKLZ8GiuNWiuN3GLoegou0hv9B0r7ZRLIDx8p6GLWntRrQ4Yf/S9LS0Pk460mOiN7rTSYBJMKTDCm+kbGBIMv30TPhgDfAwVbC8ubCMSIeMcUdJiRKPRjCWe6+PUfRoVB2fDwdlo4Bab+FUPmt6oDy9Cei6eU2+LD/dwUi6aISEVKIHpqUhomFJgSgNDGe1laWAMkCxRKHxD4hsKKWR72ZD4or0shWp7jrQiNUogHAGuQDRaAqkzmhMeSWv9LtJonjs17N/cQGgxotFojixKKpyGR7Pu4dQ9GhUXN0yvyJqHcPwgnH1UUArfbeI5DVynju/UkSNKuWh2QIGhBIYMBYUy2ss9QmOTkdk2SEJBsZXQMCS+UKgOkbHrQ95jGk0p8CV4UuD7Ak+KjmXwfMEbRcU7Ewlmz+cGO6ghocWIRqMZOZ7r06wFgqNZ93BqHs2Kg1dywfEQjsRwfOhIsxgjPN4WSkr8UHjolMsRIRQZpjTCiMXWQmMgkSECEeEbMhQV4bLoFhrqADMlUoEvBZ4f3neKC19EgiNYDu49GazfUfmUqjxwt6bFiEajOd5EUY6aF923lv1GIDiE42M4PsKR4EvMUR90D0HKpRGmXXTK5VBRYT2GMkKhEUY1VIfokAaGGkxk+KJDXIj+QsM35FBn3shWlCIUFltGLHrEhtyn0jGEwjIUphncWyaYRrA8ef8cU2d0N41Gozkm9I1y1D3chh9EDVyJcNuiw3B8DHkEowlK4btOJDw8p4H0j04tyrFB0RYXvUJDGpiqLTp2v0uF7I1ihOkS2SM09ioylAoiFVEUoidi0S860Vq3X1HREhCRqIiWw+fM3ufBNFXURKMAfD/o2vJ98D3OvGWeqVNajGg0mjGiM8rRKTicuoffSqUoBZ7ECCMeZivicUTTGN0plwa+U9cpl30gQpGxVR2G0RIaA4qM7jqMUFxsEhq7Fxm7qafYJDAk+L5g9423/X+alkiwesWFqTANOpY7BMYATTMKFYoOCa6P9L2ghqlPHZNyRyu0tRjRaDRb4jl+JDKaHYLDqfd8canQfMnxMUPxIVz/SM8ck56L5zaiqIfvNo/08R4JFAgldiz4HLSzpF30uVXBZyA0thMZUT2FB740+oiIfdRTbIOgUzAQCobe6ASbxIYxRAuTtugIhYYXCA/kESru3gEtRjSaE46Sqi04topydCIVwvWjGg/hBGmXI01nysVt4DXrOuXSSUfRZ7+Cz1Ytxl46S7Yq+IzW93SWbKqn8AS+bxxOPUVLVPTUU2wXsRimqNgJhQLPR7WExr5Eh8LHw8PHx4PYaD1KtBjRaE4IfaMcYQHplvgt4eH37Wg5qrRTLkG6xTupKZctOks6Cz73UvQZCYotCj49IfGExJU99RReZ6TCxPOtI1dPcRRQSoHvBZEOzwfpB+mVPUY6fLxQeLTvJT0XEJYWIxqNZkhIqaI0SmeUoxmOtd8WX/btaBkHpO/1GIsd85RLqx6jV2iEhZ576SzpLvps12G4KFwUjlI0laKhwnTHpnoKIxAYQ6un2H10Yi/1FEcBpWSQXvH8KM2ifB/U/kRHEOtwo+VxQIsRjWYMcR0/inB0Co9toxydeC3R0Uq1+EHSfRxQCt9zusSH9I5hyiUUHLZvYvkmtm9g+SZWWJ+x+92oyB/DQ+EKiaOgqRQOioaEhlTUJbib6ilMxr2e4iigpOyq6div6JBRgqU74rFXXOlR9+t7fv0w0GJEozmidEU5Ouo4dhXlaNHT0RLVd4yL8KAn5eIGAkSNUWHejiiwpBEKjm7hsV1UQ6ICcUEgKhwUTamoS0VdQd2DmoSa15v6GNwubhzqKY4CwxcdEh93k/AYBlJJmr5D02/iK5+6p8WIRnOicR2/q2i0Fe3YdZSjRUdHS0t0COdod7T0Q/leNL3Wd+p4rnNk24EHQoHltyMdlmyJj62LQn2lqClFWSpKvqLoQlUGqRK3768knDmyBcehnmLUKAhqN3wvaJkNazsC0bG3z6lCdtVztITHQUySdqRL02/iygaGkAjDxzYkdmK0nsZajGg0h0ArytHPDGzXUY6uHXYKjzHpaOlHlHJpt9hKzx31Ue0PRSQyOqMd1jadKJ5SlH1FWRLc+4EAqfX9aARCIWH1CIhjVk8xagLR4bdrOqSP9FoeHXsXHb2FpAclOiD497YMQHg4qkrDryINB9OUWKL7PS1dwKrRjAYlFUopgtlT4X2YvoiWw1HmSraGVPXbtv28UkTbuE0/TK+4uI19CAWp2oIj9O84UsPhBkBJGQ2S85wantNEyTEUUQSmXpZvYnomhmdieSYxaRJj66JRtyU6fChLFQmPenheEEIRMxUxS5K0FDkrWA7WKWxLMuaT4o8cm91I/Q5jsL2KDtW3g0VxAH+3oeAwQ0HainQZpqTh1am4FRqtFIxxNGY69UOLEc2W7Ppk3bFN78k6iFr2P1lveo0M++g7Xrtpn53HEfy383GE+4bun+FI0tHRElimy7Fopd2KVsrFb0U+3OZYpVykBNcTCNfG9AxivklcmSQQJMXWX+uOVB1iIxAeVSnxTUXMlIGwiCsSliQXCo2YJXX04gDpZ4Eufbkv0dH26ugWHvIARIdpdggOg3ZdTs/H0JMuFbdCtV7FV+Mj9LUY2QfH7WTd8mE40ifr44TXEh0ybKX1wR/v331gLNaKehztlIsfCg3HN3A8ge8KbN8ipgziyiAlDDKGINVbMNHxsBEKjopU1JE0hMQ1fISliMUktqVImIqcFhqHRuBGKntEhx+2qe/976u3nqOvV8c+MYzOuh4i4WHuMENHKUnNrVFxqzT9xlCP6bA48WJk5WaZ0kpDn6w1B4dS4Kmu4XBj1Uq7FVKFduqNyNlU9Zl5MQp6hYbjCVwvWFa+IK5M0sIga8K0IciagoQh2GpMcEMqakgaSJqGxDN9fMvHtCUxU2IbgYFl/nB/zBPNJgt0P6zp2Gfarze9MmyvDkO0ohwyuBdBd5JhqIHFquM3qThVal4VuceOnaPCiRcjTt2jXtZjwE8ErfRASwOojgd91onedV2PO/bXtW17f8KVkW36OKUmtkL5fkeh6ehSLr4kFBgGri+iZccT4WMDXwriArKmIGsIpk3IGIKcLYjHt/7GbyJphtEN3/KRlodvSTDbP2csvGkOh+FaoLcZtldHJ0LQlUqJIhzm4IKjF6UkVbdKxa3g+Mfn3HXixcixZoCTb1dhtaL7JLPLk2/X+3Y8FludzLdYJ1Sf/W3xc+woGOg+Fs3u8V03aq91nQbSO/gvvi6h4Qkcv0NohJEOX3Z/mydC0RFEOCAbCwRIbJt+VFdIXNPHN308y8c1fTzTZ59O5Jp9MGwL9BYy9CPtrevYNy3B0VE42nq8TTnRnml6DSpuhZpXO5ajDU68GFGOj2j6ez/59p4EGezkKw7wal2j2TVS4XnNSHh4Tn3oKZdOodEZxehMo/jbqIGkgClTkLUFORNypiBjCKwtLjWDcfMyEhquGaRXtOgYLZEFeig69msMFu33gLw6WqmUlthoebAY21u6DAVfeVSdGlW3giuPbv3VMDjxYoTVBtbd6qiPQqM5VJT0u7w9fKex56stpYK6W7dDWPRLoex2AFrWVBRsyFuB2EgbgiQG5hbf/AqFF9ZxtISHZwYi5KBPFpqtGbYbabTfA/DqMDpaY60BCkcPBkXda1BxKtS92vZb9lxEq+6F8JpVdW7S/bjjQrm0vEyjWiGRzuz3B9gTWoxoNCcA6bl4zaDWY5CUi1JhRMMP0yZbRDZ2KzRMI2hhtU1FzoKcCRnDIIUgjkFMmhjbiY5QZHiG3yE+pBYde0SF7XvRPWHkIngS1fncFvcyFB3S9yLRIaUHSraL/oOOgNY7tLsI+z1WrSWFVEHxqI8fpFuUT+TVEe2Prn20f67u9QIQhkIgESKYm4NojfNrv071vG7TffTefe63e52ifbyq9bvufN/gdyqDtkk6tu5+fEDB77tf+hLZiXkeeNcPH8wb7MCexMhzzz3Hs88+y+LiIg8//DCf+9zneM973tN3249//OP89m//9qb1Dz30EN///vf38vYajWY7lMJzd065RELDM3D8sCajFd3w290ngwqNToOumKnImIK0ECRCsWH7VvfclZ4L5UB0tNMqUbTDOLmio9Ess1Z8k7XidZpOpeMkKDtOgpvFQpcI6Dph9hZmaU4KQVYz/EMSrSWBlUhgWqOLTwz8zl/96lf55Cc/yXPPPce73/1uvvCFL/C+972Pl156iXPnzm3a/ld/9Vf5B//gH0SPPc/j0Ucf5b/+r//r/R25RqMBOlMurXkuDaRUXULD8azuNEoY6dit0LCMUFxYgUNotGwFJl62qYirzrkrVrDsGQiv/3vIUHT0Cg//BIuOFkopKrVl1jbeZLX4JrX62qgPKRSPIvwvdJkVbbfZ3setZREut/aggo0jlFDtfdN5shRdJ04R1QaJaACfEALDIHpsiNYxEB1n9DjaX/A60T4Ttx93vM+m+46fKXrpLl8nROfPQzSYruHV2/GPrvfp+K13/i2IHm/fzseRvhB0biS6drAZ4UvSK0Xekp7i4qUHtt32IBFqwETxO97xDh5//HE+//nPR+sefPBBPvShD/HMM8/s+Prf//3f57/6r/4rrl69yvnz53f1nqVSiXw+T7FYJJfLDXK4O3LrT25SvVEe6j41moPEdx0a9TrVaoN6vUmj6be7T/YrNKwOh1CrnVIxW90BavC5KzKKdHTXc2jR0Y0vPTZKN1nbCCIgbtcUVUEuM89k/hyZ1DRCGB0nuM77HlHQEgSiJSTa64NhbxKkRPgSGd53ncg3CYFd/ixD8OowWp0qHYWjpgg/i2P4uTlKLbl2rUn2zhq5O+tkljcwwvlYp/7hPyD/wQ8O9b12e/4eKDLiOA4vvPACn/rUp7rWv/e97+Vb3/rWrvbxm7/5m/zYj/3YtkKk2WzSbDajx6VSaZDD1GjGFqUUritpOJJG06fZ9KnXHep1N3jsysC4KxIadnjrj2W0RYbdlUJpp1R67aShPXfF8m1sty08zO1Eh1AdKZW2+PBHUgQ4HjSdKuvF66wW36RYuoXssO82DZuJ/Fkm8+eZyJ/FthID719Bn7krXl8LdAMBxhaub1vQ6dXRKTx2y9EqHD0YAmOyClWv1q7HOWykIrVWIndnneydNZKl7sJYNxHDfsfbiV28OJrjY0AxsrKygu/7zM3Nda2fm5vjzp07O75+cXGRP/iDP+D/+D/+j223e+aZZ/jMZz4zyKFpNEeefkKjEd6aHeu2N2ZtKwfL7KnPsNpD1uxwuZ/Q6EQosLx2hCOKesitT0pSyKhwtN026yPF8TmBHBRKKar11Sj6Uaktdz0fj2WZzJ9jqnCeXGYBY5fi4KAs0Fvsx6ujNTnW7DQAC4XHcbXHl8oPoiDO6FpyzYZD9u56EP1YWsfqmOqtgNpkltL8JOX5CRr5NI99+P9N8pFHRnKssMcC1t6QnVJqV2G8f/bP/hmFQoEPfehD22736U9/mqeffjp6XCqVOHv27F4OVaM5NJRSNB1JteZRq3uR0Gg0JU0nWN5tUrRTaETioieFso2n1yaEFNibUitBemUrfCG7OlZawkOLjsGQ0qdYvh0UoG68SdPtthLIpmeZzJ9nsnCeVGJi2+/SINLhDd0Cvb1/GUoNN+xhCR7tKGhE/8mxw3AcHR8UDa8ZGJO5NQ69QFgpkhsVsnfWyd1ZI7le6foz9WIW5bkJyvMTlGcn8ONbR1RHwUBiZHp6GtM0N0VBlpaWNkVLelFK8Vu/9Vt87GMfIxbb3kw5Ho8Tj8cHOTSN5tDwfUm17lOreVTrwS1Y9pG7mDcTsyBmg2342KbfTpvsUWh0YkgRpleMrmiHqbYXHZ0RDi8sJpWG7rbYK65bZ610g7WNa2yUbuF3XB0bhkUhe5rJwnkm8+eI2alt96UAXAflOEjH3bdPR7BPST8r9J28OnY7OfYk4UsvmJLrVvHkcOzkd4vheGSXNsjeWSN7dx272R2FqefTUfSjNpk90pMaBxIjsViMJ554gueff54Pf/jD0frnn3+eD+5Q9PL//D//D6+99ho/8zM/s7cj1WgOEaUUjaakVveiSEc1XG46W58MhIBkwiSdtEjEDWKmwjJcTOFiqQaWKfcsNNoHB4YSXWmVlvDYTnS0jME66zlcU6K06Ng3SinqjQ3Wim+yuvEm5erdrudjdiqKfuSzpzCN7b96uwWIs2dnZUXniHs3inWobUbcbyocPYZ1HPtHUffqoTFZfefNh/a2ini5FtV+pFdLXaM8fMukMlugND9BeW4CLzk+F/UDp2mefvppPvaxj/Hkk0/y1FNP8cUvfpHr16/ziU98AghSLLdu3eJLX/pS1+t+8zd/k3e84x1cvnx5OEeu0QwBz5fU6n5bcITRjlrd23YkhmVCKi5IxCEZg0RMkYxJYrZC4KKkRPru/iK1oejoreewfRNjW9GxuZ5DW6APH6kkpcod1jausVa8TqPZXWifTk4xWTjPVP486dT0jqlshQLXRTUdpDuYAGmJjs5Ix3aiI5gc267fsLTg2BWedIMoiFPFV8Mdl7AVwvPJLBfJ3Vkje2edWL3Z9Xwjm6Q8P0lpboLadA41pmGqgcXIRz7yEVZXV/nsZz/L4uIily9f5utf/3rUHbO4uMj169e7XlMsFvnd3/1dfvVXf3U4R63RSIVSsuOmQAb30WMVtCsq5dNwJPW6pNaEelNRb0LdCcbMb4VAEbcVCVsGt5iMlq1+dYUK5F469hSYsiPSIU1sL7g3tlAQeu7KaPC8JuulG0EBaukGfkeLphBGV/olHtvZVluhUI4Lzm4FiIr6V/yO2g7Zp222VThqnaDC0YNAKRlEQdwKDa9xKO8ZqzaC1Eur9bYj/SsNg8pMPqj9mJ/ESQ/eZdWJUoqmJ6k2Rzv7ZmCfkVGgfUbGFyVl4Py4pWgInpdSAuE2si0mou2k7HrcL+LgS2i4Bg3HCO5dQcM1aLrGtp4bliFJxDpER3iL2Xuv3ej/ywBTGn19Ora1QNdzV0ZKvVmKoh/F8iKdHz7bSjCRPxe03+bOYJo7FwV2CRDHoffDrMLelVYBaeejTZEOsXlybKuQ9CAmx54kgpbcKjWvGli0HyBCStIrpUiAJCrdqR8nFQ9qP+YmqMzkUX2vhnaPUuD4EseTOKHHyGMf+n/xrh/5L/a1334ciM+I5viylWiQqr0+iDS0REKPuECFIiIUDeG6YReUKxVMfg3ERnhzgseuv/W3b1eUI9YtOvb5d93nIMGSRlhIakZdLDuKDj135UiglKRcXYrcT+uNja7nU4mJKPqRTc8idnHW7xUgPm6XyJAd4gNCR1Ej+Kc3QqMvmyCiYXROjtWCY6gcpjGZVW8GtR9318ksbWB6Ha23QlCdylGen6A0P0kzm9x38alS4PqKpudHAgQCU8Lr+SL/fvm3uey8i1xsuBf8u0WLkTGjN0LQLRp6Ig39IhL9RMMRDI5tFeVouEaH4ddmLFOSsHuiHDFJ3BpyaDoqIg08OayOaMd2bqR67srRxPdd1ks3o/kvXkc4XiDIZReYzJ9nqnCeRHznL2slFNKQSL+B36ziO1WU8pDCRcQkwgz+uU0RRDIMEQhmwzjSDQ/HlqbfpOKUqXm1g/s+VIrUWjlyPk0Wu1u83bgdpV7KswWkPZzTcyRAPNl1bViMN/jB1CovT65Ri7lQh6+/8XX+5gN/cyjvOyhajBwESnWnGVpRgkFEQ8djpWSU2jhObIpyOG3RsXOUIxQdhxHl6BAcdofg2K6IVM9dOfo0nUoU/SiWb3e5Y5pmjMn8uaAAtXAW244jDCOYh2KAMATKIvgGtUDFQNgKZSlkvYQsrUGxhJASc3uTXM2I8JVPzQmiIAdlTGY2XbJ316Ob5bRbfxVQm8hGAqReSA9NiXoyqANpun6XAPGE5I2JdX4wtcrtbCVan3At3jP5Lt658M6hvP9eOPFiREmJ8r22aGiJhJYAoC0oekVDO0qxuR5C08aXdEQ4OlIr3oBRjlB4HFyUo1NsBJGO7ezPW0WkUcus2W6d1RboRwgRiAcI3E9X16+xuv4mlepK12apZJ7ZmYvMzr6FyclTmJaJskBZKrg3VfQYk+jfV/k+frGIu7qGLJaC7wXNEUVR9xodLbnDzyMnitWo9Ta1Vu42HrNNKnNB6qU8W8BPbO+5NQh+S4B4EtlzDlpO1vjB9AqvTqzjWGE6SMG5Uo4HVqe4UMzzxAffz4X8haEdz6CceDGycfcm1TsrO2+o2ZbOKEfdCYpGj1qUoz1vpVtwbFfLAcHMlVYdR7uuIxAeWnAcEuGEVmG07oPlYEprIDaMcD0CDCMYDucrj7XVG9xdfIO7S1dpNqtdO52YXGB24SKzpy6SmpgAOxAbvgm+uXXrpvJ9/I0NvPV1ZKmI2oXZnWZ0tIzJKm4Ff0hutS0M1yOztBHVf9iN7lqTej5FeW6S0vwEtckcw6yK95XC8SRNV+L3CJCm6fHqxDo/mF5hJdUuiM02YzywOsUDq1Nk3OGJof1y4sWIZjC8nihH0zWoO4LmDlEO25TEbUXSloH4OMAohymNjtSKsSsX0u5WWRl5dWj78+EiDBGmOkSPuOgQGf3ExQAfkkazyuKdq9xdeoPllevIDldM07KZPnWOmXMXmTp/nlgqGT0nd7hKVp6HXyxqATImKKWoe7Xht+QqRbxSJ9syHlspYXQIAd80qMwWgtqPuQnc1HCNx2RLgHgSr+czqFDczlT4i6lVXp9YDyK0BM7Mb9ko8ODqFKfL2S2jvaNEixHNJpSCZquWoyu9IvC2i3IIRcLqbpNtCQ9ryFX/QgqsnjbZlvjY7g8tmLciuyIduoB0MCLxYBCOsu8RGa3lfiLjAH7JUihKtWXuLr3B0p2rFNe63U8T6Qwz5y8yfe4Ck6fOYJi7D7kFAmQDb20Dv1TcsxPqvlAKvCZ4NXDqoPpZjovue9G5rmebTcJObPHZF32e69mwa1/bvXfn+2xxXF37FH2e2t0+XelS8epUvXp3S67YtNB/f/3W+j6ZlXJY+7FBvNotbprpROB6Oj9JdSqPMo2hfp+0WnGbnsT1N6cBq7bDy5Nr/MXUKsVE2xRtsp7gwZVp7lubJOEf7dP90T46zYHi+fTUcQSCo+kaqG3+kmyzlUrpTq3EDiDK0W6R7Y507BTl6BUcrdSKtj4P6RAQRliYSZj22DFqcdiqTXTXayhT4Quf1eWbLN++yvL1azQq3V5BuZlZZs5dZOb8RTKTU7sa5NlCeR7exgb++jp+qTQCAaLAc8Ctg1cP7nUd2rYoFFXpUvGbOH3F2uDYdZfccoXsSpXMaq3beEwIqpNJSjMZytNpnHQr3eFBc3WbvfaIxp7FzhWKoA7ElRLfD6YGCQStd5Iork3UeHGmxLWJWmR2aHuC+1ezPLSUY66aCP5eRX/LetV5AE6p7zaHhRYjx5woytFbQLqbKIfdITo6ikjNYUY5OopHOwWH7ZvbFo9C51TZduGoZ8qT07HSJSgEiK3rKCJB0RIXR+wXpIy20GiLju5iUadRZ+XGmyy/eZXVm9fx3Y7hc6bJ5OmzzJy/yMy5C8RT6YHeX3oe/igFiO8GoqN1OySr8XHHUR4V36EqnR2H/O2IVKQ36mSXK+SWqySq3bUfTsKiPJ2mPJOhMplC7incq3ruuxchECCer3Cl6nqy9Re7nnD5/myFl2aq1GLtz8mpUpyHlzLct5rCjqZxu33fox/CO1hflZ3QYuSY0D/KYdB0xe6iHD0OpAcS5QhbYu0eT47dtchuTq0cF9vzLrHQEhdCgNEWFH2LNMfMkKJTYET34TJbZE6qG+ssvxlEPzbuLnaJhFgyxcz5C8ycu8jk6TOY1mD9s9Jz8TeK+Otr+KXy4QoQ6bWFh1cH/3CnvY4zEknVd6hIB3efos1qemRXqmSXK2RXa5heOwWiBFQLyVCApGlk4gdmAuNL8KTEl2pTJwyAa0hem6zx4myFW/l2GibpGjy0nOHhuxkmG+PdP67FyBixZZTDEXhywChHmF4ZdpTDlKKvCdiuWmR7CkfdcS0eFWDFzLCjY3MdhWF0i4vjghJhJCNqg+1uid3Nv6OUko27iyy/eZWVN69SKxW7ns9MTgcC5PxFctOzA6VfoCVANvDX1vHLhyhAlOyIfNSCSIhmIBrSpSIdansaABWiFMlig1woQFKl7qFznm1SnklTmk5TmU7j28M2Lmrjy6AbxvM3t+K2WEo3eXG2wl9MV3GsMKmi4PxGkstLGS6uJzGPyVWZFiNHkK4oR1fnyi6iHLHNqZVhRznaLbKbazm2b5GVHXNWulMrYyc4ejBMgR03sWImpm0cK5HRiTL7C40onbIHXKfJ6o3rLF+/ysqNN/Ga7ROEMAwmF05HBajJ7OBW1dJz8dc32hGQw0BJ8BptAeI3h25pcRLwoyhIE2+P82FM1yezUg0EyEoVy+mOptRyiUiA1POJA7XAVQpcub0AaZg+L0/XeHGuzHK6LVpzDZOHlzI8tJwh64Sn7iA72z7ksJ6363FPjW/n4877RHbnwY4HiRYjI0IqcNzeGSvBvSe3/mMwRMuXY3MB6fCjHO0W2c7Uyo7Fo31MwDxTjmeUYysEWLaJFTewYybmUH/5I6RPsWhnDQdD+jFrpSIr16+x/OZV1he73U/teILpc+eD9tsz57Big3shSNcN6j821g9HgCgVCI4o9dLQRaf7oC4dKr5DXe0hgqQUiUqT7HKV3HKV1Ea962vHtwzKUynKYfGpFz/Y02CrENXzJX7rctIItXsoBJRQ3Mw2+e50hVcnanhhob0p4b5iikdXM1wIi1FF6Og7aFRwJ4a9v0HRYuSA8XyouwbNntTKnqIcMUnMHG6Uw5CDz1eBdvFob2rlOLfIClNgxwIBYtnm2EY/+haLhlGOTmfRob6nlBSX77IcCpDq+lrX8+nCBDPnLjJ9/gKF2XnEHibASdcJIyBhCuag8ZtBq61XB7cRREM0e8ZTPhXpUPUd/N7pxDtgeJLMWpXschD9iDW6a3AamRilsPi0WkhuazwWRBZE1GHcijRE7epdj9tRiNZrgscK1w8m4rq+xBQiDBx2v2/Z8vjuRIU/n6iwHm8f82zd5rH1LA9vpEn5YcjxgK536vk45Zk0D8xMHswb7BItRoZAK8pR7zICC+53E+UIjMCCKEfLFGzYUY6uFtlOu/MdW2QDgeH2eHKclBZZK2ZixcLox7DNUg6KVnTDpCedEtZuHNKP4bsuq7duBPUfN67h1NvthUIICvOnmDl3genzF0nnC3t6j0CArOOvbeBXDliA+G671datw5CdPE8iCkVdulR8h8aAUZBY1SG3UiW3XCG1Vu8yHpOGoDadpjqXoTafxk/FIqGQJYwCdAoK2mJj7z8LOL6P4yocv2MmTM8+fRSvZetcmSzzerYeFeLHfMHDG2keW8+yUI8daMebtAwqU0nKMyn8VmRIR0bGB9enp1PFoO4aODtEOWJbRDnsYUY5elpk7V3OVwGCtEpnp8pJa5HtQBgiSr1YsaMb/WgVi/amU2ilU0Z02I1KJYh+XL/K+u2bSL99wrZiMabOng8EyNnz2PHEnt5Duk5QgLq+jl+p7PyCvSL9Dq+Pmu542SOtE70hRHgDF5+KbFKWDspQCAOSQc/5JnEQPZaSxEqN5J0KicUKdqW7kNVL2zQWsjROZWjOpGk5LR6k4bkCvNCMrOnJbduL12IuVybKfHeiQtVuR37OVuM8tpblgWKK2DYXh8PASduUZ1JUJ7aPDo0CLUZ6kAqabu8k2eDm7xDl6B7opiIH0mFGObrmq3QUju5YPNoxRbY3tXJMirH3jGkbYfGpgWltL9wOkyiN0qeGY6/FosNGKUV5dTlqvy2vLHc9n8zmQu+PixQWFjCMvR24dMIIyEEKECWDdEtLfIzYd2EUBKKhUzgEfw0tEdEpKETHstFnuVOAQNA5subVWPGr1MJuonTL5XULzJpLfLFM4naF+FIVo6f1tjmTpnkqQ2Mhi5eNHdrVvSsVjufTdOW2YwRcIflBvsafT5S5nmkXZqddg7duZHh0PcN082DnwyghqE0mKM+kOszZjh4nWowsvVli8bZHcTXWVcux3R9HzJLdbbJhxGPYUQ5TGv1TK9u08Pa2yHamVo5V8eg+EYYIUi/x0UY/lCCKZHR6bgy7WHTY+J7H2u2bQQHq9as0q9Wu5/Oz85H5WHpics+hb9lshkZka/iV6s4vGJRem3W/OVZFp+0Tf38B0Fov+gkG+q8/iCLGiu+w4lVZ9+tbdpBESEVstUZisULidhm72N166ycsGgsZGqeyNOfSqANsve3Fk+FMGN/H32YukUJxJ+FwZbLC9wsVmma7JfeecpJH1zPcW0phHvAXshcP2pSrU8k9GrQdLidajPzHf32Nay979AbyoihHT2pl6FEOSXfhqAxG2Ju7aJF19XyVgTBtgRUzsePmoUY/omJRq6eGw1KB2BiTf69mrcbKjWssv3mN1VvXkV47ZWFYFtNnzjF97gIz5y4QS6b2/D6BAFkP0jDVYQuQ8bVZNwRk4zb5uE3CNqMiy6OKq3zWvDorXpWG3D69ZTQ84ncC8ZG4U8FwO6IfgDOVDMTHQga3cLCtt734StH0JI7nbxpK10vd8HlxosqViTJLyXb9S8GxeHQtwyPrGXLewZ9y6/k4lZkU9dzuTNqEkpyqr7Lwnd+Csw9BajSFrCdajCzck6d4cw3ba3S5kB5ElMPey3yVrVpkT0jx6H4QIoh+WHETOzQgOwxkTCETChlXKFsd2ejGTiilqK6vsXz9KstvXqW41D18Lp5OR7NfJhZOY1p7/yo5UAEy5jbrKdskn7DJxu0jW78UoaAkmyx7VYp+fWudpxT2eoNEmH6x17pbb2XMpDGfobGQobmQQR5w620vEoXjShqexJPbd/UoFG+mG1yZrPAXuSqtCRumhAdKaR5by3C+NR/mII/ZNKhMJ6nMpHbVqmxLlwvVu1yq3OKe6iIpP4xAPfRfwCM/caDHuhUnWow8/lfPE9u4RfXNfQ4IiopH998i20qtnMTi0f1iWK3WWxPLPpzohzJAxmUgQBJ7N/46CkjfZ/3O7aD+482rm4bPZadnIgGSnZre15W5bDbw1zfw1teQ1dp+D71jx+Nvs26bgnw8Rj5hY4+Bf40jfVb8KqtuDWcLsSccn/jdConbFRJ3Kpg9rbdOIREUni5kcSYPv7hS0YqABK24O13ulTpacjf6tORe3kiT9A/+y8BJ2ZRnU9QmkqgdfmcZt8Y91dtcqtzmfO0uVkcresOwqZ19N5OF8wd9yFtyosXIwCg2CQ57T/NVZFRMetKLR/eFIOp6sWLGoRmPSVtF4kPFxrsWx200WLkZDp+7cR3PbRdtGqbJ5KkzkftpIr0/h0bZbOCvreOtryNrQxIgXTbrdfDHs+g0SsMkbJKWeaRTMBBktzb8IA1T8pt9N7BKzaD2Y7FMbLmG6DjDS8ugOZcOul8WMsjU4c9VUSgcX9J0FW5nK+4W+Chey9W4MlHpasmN+4KHNzI8tpZhvnGwLbkwQEGqUsw0N7hUuc2l6i0WGutdT2/YaV7LnOa1zCluJmf4sSc+zuS5dxzosW+HFiO9dLTI2r2eHLuZr9InteIb433COkpE0Y+YiRkztq2tGRZKgEoo/IRExtXY/9VUixuB98f1q2zcWUR1DZ9LMn02mP0ydfospr2/k4RsNPDX1/DWNpD1IQgQpbq9PsbcZn2s0jBAXXqselVWvdome3bhSWJL1aD2Y7GCVev2DXGzMRoLWZqnMjSnUwzXTGl3tFpxG2EUZDeTfldjLn8+Wea7hf4tuQ8WU9gH3JIL4MVMKjMpKtNbTww2lM/Z2jKXKre4VLlN3mv/zSngdmKK1zKneC1zmtVYbuTeIp2M+dfq/nDvVrHXJdl6oiu1sn3xqOoqHHXDwlHP9LXgOAhEj/HYIX2BKSuIfPgJhYqPt5iUUlK8eyeq/6gVN7qez0xMhtGPi+Rn5/Z9VS4bdfz19eEIkGNosx4zBblEjHx8PNIwUinWwihItSfyZFacoPZjMWi9FX7730YZguZsOup+8TOjayt1ZSA+dmrFjbYPW3KvTJa5ke5uyX1kI8Oja1mmnMOJ5jRyccqzWxekxn2Ht1QXuVS5xVuqd4jLtgh0hcm19ByvpU/zRmaBqpU8lGPeCydajBT/4Brpawro/gdqRTncPqkV3SJ78BhmR+fLIQ2dUwJUXCETEj8x/tEPz3FYvXk9cj91O4fPCYOJU6eZOXeBmXMXSeYGHz7Xi2zU8UIjMtnhtLonjqHNuiEgF7fJJWxS9nh8uKrSYcWtsebX2i25viS2UgtqPxbL2OUe47GUHRSensrSnE2jRthS6klF0/NxfLltK24LhWIx6XBlosxLheqmltzH1rJcKicPvCUXdi5IzTuVIPpRvc3Z2jJGh8CqmnFeD6Mf11JzeMZ4fN7G4ygPiNjZLJUbazTdpm6RHSUiHDoXen8cdvRDJhQyNr6dLy3q5VI0+2V98RaqoxPAiseZOXuB6XMXmDp7DjsW3/f7yXodb30df30NWW/sfUfH2GY9HTPJx2Nk4tZYpGF8JVn1aqx4NerhFbZRd0mFvh/xu5uNx5zpVOR86u2ynfSgGKQVt0Xd9Hmx0Kclt2nx2HqGtx5SSy6EBakzKWqTPQWpSrHQWAvTL7eYcbqbLpZjuaj+YzExdaTSL7vlRIuR3I+e4/rKTapvDrGaX7MrgqFzRpiCOSTjMdFqvQ26X9Th18wNFaUUpeW7LL8ZmI9V1la7nk/lC0H04/xF8nMLGHsYPteLrNVCAbKObOxRgBxzm/WYaZBP2OTGJA2DgrJssuJV2fDrSF8RW6uTDVtvYxvd/85+3Gzbrs9lULHRtpBJFE03sGPfqRW3xXYtuQ8W0zy6fjgtuQAIQXUiQXm2uyDVkh7na3e5VLnNPZXbZPz2v4NEcCM1w2vpU7yeOc1GbH/F5UlskubexjMMixMtRjSHixWKj8McOqfMjuhHfPyjH77nsnrrZliAeg2nsyZDCApzC8ycD9Iv6cLEUN5z3wLkBNism53dMGOShnGVH0ZBqrj1JvE7FfJh663htKNTCnAnk1HthztxuMZj/ZAEbqiOF0zG3S1RS+5khY1Yd0vu29aCKblJeTjiyo+ZlHsKUlNeg3vC7pcL1bvYHa3STcPianqBVzOneSO9QNPcXw2OgWBGZJkXefIiSc7O7mt/+2U8/mo0Y8lIhs61oh9h/Yc6uqMYdk2jWgms19+8ylrP8DnTtpk+cz7ofjl7jlhiOAVqfq2Kvx5YsctGn9bN7Rhzm/XdIoBUzCSfiJGJjUcaRiko+Q1WvArVlQ3iixWyt8vE1urdrbe2ERqPBc6nMjH6U8WgrbgtfBSvhi25b4ywJbdFIxenPJOing9SpVNOiUvFoPvlVGO16yiKVirqfrmRmkGK/QulDAkWjDyzIos1hP0Ni9F/wjTHCtM22rUfh2S7rgyi1ItMjH/0QylFZW2FpTevsvLmNUorS13PJzLZaPbLxMJpDHM4Xyh+rRpNw5XNQQTI+Nqs74VWGiafsLGGkPo6DJrSY6VeonxrCft2icRihXS9Oz3m5uNR+sWZSh2Jqa4KcMOpuLttxW2xGnO5Mlnmez0tuecqcR5bD6bkHkZLLgQFqdWpJOWZFH7c4Ex9hUvLgQCZcLsHPy4mJngtHdR/LMcLQ4lCWZjMiiwLRp6MGG06Ziu0GNHsCyF6oh+HbbuekEHtx+i/N/eF9H3Wbt9k+fo1Vt68SqPa/QWVn51j+txFZs5fIDMxNTRTrD0LkFbRqTOeNuuDYhqt2TAxkoc4nG0/+FKysb5G+fpduL1BfLlGoaOoU5qC5lzbdt0/QhNdXRkKkF224rZwhOQH+Sp/Plnpack1eXQ9mJI7eUgtuQBuMnBIdfMWF+p3eNfaS7ylukhStlOVnjB4MzXHa5lTvJ4+RcXe+2ynXiZEijmRZ0ZkMMTRFs5ajGgGxrSC1lttu74/nHo9HD53ldVbN/DddiW/YVpMnTnLzLmLTJ87TzyVHtr7+tVqMAl3bQPp7FKAHAOb9UERQDpmkUvY45OG8XzKt5cp37iLurmOVXXovA72MrH21NuZ0RiPbUXUiutJ/AEia50tud8vVHE6WnIvlZM8eogtucEbBwWpTMA5tcxfqrzIudVlTNrRmZoZ4/V02H6bnsM1hieQ4ljMizzzRp6EGGS/o/18azGi2RHRMh477KFzx8h2HcLhcxvrweyX61cp3r3T9Xw8lQ4m356/yOSpM/saPteLX620IyDOLgpIj4nN+l6IWwb50BNkHNIwfqVO/cYy1Rt3kYsbCF9FWl0ZguZMKnI+9bL7b+keJp4M60A8f1deIJ1s1ZI70bR4dD2Ykps9pJZcAD9mEJ/wOGWvck99kbm7G13Pr8ayUfrldnIKNcRIhUAwLTLMizwTIjVQ5FSacVYKj3I6d2Zox7MXtBjR9KXTdt2KHVL0QxClXo5L9ENKn43FxcD99Po16qVi1/PZqRlmzgf+H7np2aGlX5RSyGoVf32XAkSpwN3UrR0Lm/VBMQ1BLuyGSVhH+4OnpMS9u0Hz5gr1G0vIjXZHlQD8pNVuvZ1No45YWkmGXiDNAbxAWigU18KW3Jc7WnItKXigmOKx9QznDqslFzCQTGSrzCfWueDcJVttG/5JBLeSU7yWOc3rmVOsxfZvLthLmjjzRp45kcUWg5/OS5mLrOYfoeqb/JuX7vKTP3SOxIg+L1qMaALC6Id9yEPnlB10vhwH23UIRECtVKS0dJeVG9dYufEmXocQEIYRDZ+bOXeBRGZ47XRKKWSlgr/REiDudlsHHS/HyGZ9UISAtG2RD9MwR3k4nV9r0Ly5SvPmCs7tVZTTTpMpAc5UKki/LGTxCqM1HutH1IrrSpxdeoF0UrI8/nwymJJb7GjJnavHeGwtc6gtuTFcFmLrzCfXOS1XiCkfQg3iCIur6Xley5zijfQCdWv4xaImBrMiy7yRJyf21j3n2Hn+IvEIV1YtfvD9m1xbrSIVnJ9M8aMPzg35iHeHFiMnGMMU2PFw6Jy2XR8YJSXV4gbl1WVKy0uUV5cpr6x0Tb4FsBMJZs5dYPpcMHzOig2vUDASIOvr+Bs7CJAxt1nv7qQIHqn2w65tVNfq8P8qWI5ZBpm4Hbqigo/HRlgD0/n61lLrdZ2/LdX1/gql1Kb3VKq1H9X1mq73UR3bd/5cvsRaqWAtlrFul7D6GI8154Paj8ZcGtXHMnzUqFCAND2J6w9ShhrgC8Wr2RpXJiu8keluyb28keHRtQwLjcNJO2VFjdPWKguxNWZEMWjYC2u2y1aS18L6j+upWXzjYERRniTzRp4ZkcXcQ4pHKnitluZPG+e5smZzt7Tc9fzF6TSuP7oLkqP3CT5k/OIGsloBBBhGcEUhAGEEFxdCgGitD25H65pjAFq26/HRDJ0bZ9t1KSW1jXVKK0uUVpYpryxTXl3B9zaf/A3TJDM5zeSp04H76cwcwjDCE5bCb3WeKJDhKU4RnJVUeGKL1kH0us7HslxBbmwgN4oo14WO51oo6aLcZiA+vCZIr+d03nnf7yTesz86lzteoeg50fQIhZ59t7boDMT0kxnD/FoUQhCPBTfPFNSApSNWBmNWHeJ3KsHQuV7bdTqMx+YzuJPJI9F624sCHN/H8dTArbgtVuIOfz5R4bsTFWpW+3dwvpLg0fXMobTkChRTRonT1iqnrVVyRve8pbvxQmS/fjc+cWCRqBgWcyLHvJEjJQYXXo4UfK+U4oVilm8XcxRdA5BAE0PAhak0Dy7keGA+y4cfP82ZieF18gzKiRcjqlYJB3u1RMhuPlShMDFEGNoVCCNcd8RETWC7bkbup4dmPBZXUffLuNmuS+lTXV+jtLIcCo8lyquryD4dJIZlkiwUiE/ksSeyGLkUIhPHw6eiJGW1iFq+PYSjUhjVBlaphlmqIbw+rbRKBYWmvhvcH/N2290QswXxmIFtcfTSML4kvlwjvhi4ntql7s6mKPqxENiuHwXjsa1oeYE09yhAWi25VyYr3BxRS66Fz7y5HkRArDUSon2h4WNwPTUT1X+U7OF1t/UiEEyKFPMiz6TIDPydXXRN/lMxwwvFDN8tpWnKtnCLWwb3zWV5cCHL/XM5kiO28u/k6H66D4tGKSja6yIUFdAWJy2h0RItAK1ZiUKEIcTe13Vu2w/RLU5Er6jpfL5T1HQLm963aBWdatv1nZG+T2VtNYp4lFaWqKyvofzNJ3JhmVj5DGY+hZlPYxXSGJlkdJKLvrrkgI6lW6EkRq25tQBRCqTbFiDy+Lfb7gbTDKMgtji0zq/dYlacIPJxJxw61xEWVwKcySTNsPbjKNiub4crFc4eWnFbKBS3kw5/vkVLbmtKrnGAl21J0eSUucppa405cx2zw4a2Ydhh++0prqbncfZpv77jsWCHxah54gMUoyoFt5sxvr2R4dsbGV6tJlEdv7N80uLBhRwPzue4OJM+sh1iWoz0pSPI3Bu3HhjRvm8JFtErWoL1ChGJm2jbTgG0xReTIQSmBZYtsGwDoynANZB1I2gfM4xQ5BhBNMcwQqETPBc8P2DHjOgwHosffdt1qSRNp8HG6t1AcKysUFtbo1EsBcnUHoRlYhbSWPk0Zj4TCI904uCvrlsCpFjFLNe7BYgClNuOfPjbFaieLDrTMJZ5hE7gniS+XA0FSAW73J0b8hMWjfnAdOyo1n504smwDsQfvBW3Ra3VkjtZZjnR3ZLbmpJ7cC25ioJR5bS5yilrlSmz21xww0rzavY0r2VOcys5jTxgozATI2jJNfIUxO5TJL6ClytJXihmeGEjy2Kz+wv4QtrlvlNT3HdmloX8IXxvDYGj/ck/FnSomX0LG2iJE9OQWEJimTJIHXsiKKhqGshOwSN6ljE6hE33H1qwyohEijBE9FgYBsoMjcfiEhlXCNsATPBMhDLBNBGmiThkpz9f+jjSwfEdXOniSpdms051bS0QHOtF3PUyfqXW93cvbCuKdAT3GYxU/PD+gJXEqDaxSn0EiPR6xMfJ6njZiSOXhlEKq+IEqZfFCvHlKqI3+jEddL405zO4haMd/YD9teK2aLfklnk5V+tqyX2wmOLR9SznqvEDack1kMyYRU6bQf1H2mhHLpWCu1aBV/JneDV3htVY7lD+PbIkmB9wPkzdN/huKc23NzJ8p5im7LdP36ZQXM5WeWKizsWzZzGnLx/5z1UvWoyMCQKwDB9LeJjCb3/OFFFV9573bARiRfWIGCUEUjh4ooEvGkjDC9NDYUSFjugKRvThD0RMIExaAqX1WJhGe134uPU8Vsd6w0QJ8KTXLTR8NxIcrXW+6+IVq/gbVbxiBb9YxS/X+/+0MQurkAlERz6NWUhjJA9ReLTYSoBIvzv1MmYdL4eBFaZhYkckDSM8SXypSnyxTGKxglXtjlh5SStKvTRn06gjlKffilYrbtOVuHtoxW1Rsj3+fGJzS+58R0tu4gBacm1cTllrnDbXmLfWiIn2l6SnDG4ZU7yaP8PLU2epWsMZLrkTFmZYjJons8ti1DXH4oVikH55sZzC6yjczZg+b8tXeLJQ4ZFcFZk7w0rhXfjm0Zw9sxN7EiPPPfcczz77LIuLizz88MN87nOf4z3vec+W2zebTT772c/yla98hTt37nDmzBn+p//pf+Knf/qn93zgJwFTKEzhYQkf0ziok5LqSlMofHxVxSe4qUGUThhtUUZ43ytUouWg1sZD4SPxlMIPl30pg2Ul8ZEoM9iHMgTKNJBS4jRdvLqDW3dwqw38Rv+2CBG3Q8GRwQojHyIRG90VtJKY1SZmqRrUgPgyEBudkQ9ddNoXQwhiRyUNoxRWqRmlXuLLNUTn35AhaE6nAgEyn8HLHz3fj37stxW3Rasl9zthS24r2JHwDR7eSIdTcoffkpsW9aD7xVxlxix2NRvVpc0tNc1rmdO8PHeGZvzwnGgnRJp5kWN6F/NhlII36/FIgLxR6xZKc3GHJ/MVniyUuT9TxxTg2lmWJ/4S9cRo/EGGxcBi5Ktf/Sqf/OQnee6553j3u9/NF77wBd73vvfx0ksvce7cub6v+Ymf+Anu3r3Lb/7mb3Lp0iWWlpbwPF1s14uAUHgEEZDD+v6SqoFPFY8qkv4RhV2hFAoPPyxo85GhqFB4SkYCw1e7+6LzpcD1wHElrhvc+1v0wZumwI5ZxOIWdtwmFrMCO3VhgPRRpSqUayjTCAWOQBkGmKHQMUwww3Wh8Gk/H9bY7KXwS0nMagOzWMMsh0WovgtSF53uhqOShhGuT/xuu/bDqvVEP9J2VPtxFF1PtyJqxXUVju/vKwm4Ene4MlHhe31ach9bz3D/0FtyFVNGmdPWKqfMVQpmdyPChp/itj/FG7F5rs4tUD/EguAENnNGjnmx83wYT8JLlRTf3sjyQjHDSkfXkEBxb7rOE4UKT+YrnE44HWWGBmu5B9jIPYDaZarnKCOUGqwM+h3veAePP/44n//856N1Dz74IB/60Id45plnNm3/h3/4h/zNv/k3eeONN5icnNzTQZZKJfL5PMVikVxuuJa6V/7Bb1C929tNc3gYqHb65cCiH71IvK7ox+5OiD4SqUJh0RIaXY9V5JsxKL5UuK7CcQnuPUWfhhYATBNilsC2BTEbbEtg7ni1vIs6mug+TD9t2jYUMqbZIVa2EDBCYNaCKIhwO9ptpS463YkjkYZRCqvYJLFYJnGnQmy5RkejRRD9mE3RmM/SXMjgZWNjEf2AQIB4vqThyT17gbRwDMlL+Sp/PtHdkptxTR45gJZcE585cyMSIEmj/fckFSz7eW75U9yS09yZnKQyk8JNHo63wCDzYSqewZVSEP24UkxT70hVxYTkkVyVJwsV3pavULA3fxHWE3MsT7wN1x6eg/MP3zd9ID4juz1/DxQZcRyHF154gU996lNd69/73vfyrW99q+9rvva1r/Hkk0/yj/7RP+LLX/4y6XSaD3zgA/z9v//3SSb75+qazSbNjnHmpVJpkMM88ljCxzJ8TOEdmm+RxMFXFTwqm6IfqpUWUe0Uide674hoDAvfVziuCqMegQjxt9i9aQZXx7YVCg9bYO7pl6baduf7uPxr9z61a2SiNusuwUNQ/6GLTneF0dENs7OwPBiE4xO/GxSeJu5UMOvdIj2aeDufwZlNow6pbX5YtFpxm65k70mYdkvulckyL+W7W3LvLSV5bD3LPUNsyY0LJ+x+WWPeXMcS7S8LR5ksepPc9qe47U1QSyQpL6SoTiaDC4dDYLfzYZaaNt/eCPw/flBO4Xe231peFP24nKsSN/r/+/hmgpXCo1TS/bMQ48xAYmRlZQXf95mb685Nzc3NcefOnb6veeONN/jTP/1TEokEv/d7v8fKygp/9+/+XdbW1vit3/qtvq955pln+MxnPjPIoR1pWtEPM4x+HM5XrcRXNRzKNFUZTzWj1ImnOtMoe49m7IRSgcgIIh5t8bFVPZxlEkY7BLbFkSlQ7I/s6I7SdR57ZaRpGKWw1xuR62lstTv6IU2BM5uOBIh/xCbe7oZhtOK2CFpyK1yZrHS15E42LR5dy/LIRprMUFpyFTmjFnW/TBnlrqBTVca55U1xy59i2c8jhUmtEKc8k6aZPRyPgWA+TOCMutV8GKngjVoiEiDX692FpWcSTZ4slHkiX+FSurHjhWkpcw+r+cvIA/Y7GRV7+uT0fmkopbb8IpFSIoTgn//zf04+nwfgV37lV/gbf+Nv8Ou//ut9oyOf/vSnefrpp6PHpVKJs2fP7uVQR4YpfGzDxxQ+hhj+lXHLNjwQE4FVuEeTuizRkCXqqoy/vzabwY5HBWkV1wuEh+MGy1sKD6sz1RKIj6MrPDTDJErDxMThOAJ3YDQ94neqJO6Uid+pYja6ox9uNkZzIRu03s6k4JCuroeJH7biOvtoxW2hUFzNNLgyUeaVPi25j61lOVvbf0uuQDJjliIBkjG6Z/Gs+hluhwJkQ6YBgW+bVGaTVGZS+IdUo7PTfBhHCl4sh/UfGxk2OsSZgeKBTI0nCxWeKFSYj+8uZevECixNPE4zPjW0n+MoMpAYmZ6exjTNTVGQpaWlTdGSFgsLC5w+fToSIhDUmCiluHnzJvfee++m18TjceKHWO08DAxUWHgaFKDu509ThrUZMhQaMhQerWWp2nNKHFWhqYLoh8+QnD93oCU8gmhHu85jq+892wojHqH40MLj5DGyNIxU2Ov1IPWyWMFeq3f9bUrLoDmXjopP/fR4XnVKFI4b1IF4+2jFbVEMW3K/O1GmGGtf1MzXYjy2nuHhjQwJuT+hZuMxb61z2gzs1+OiLQx9JbjrT3ArTMHUVft80MzEKM+mqB2SR0t7PkyelNj8+Si5ZmA+1sd+PWn4PJqv8mQ+qP/IWLv/t1HCYrXwMMXMpbCG7XgzkBiJxWI88cQTPP/883z4wx+O1j///PN88IMf7Puad7/73fxf/9f/RaVSIZPJAPDKK69gGAZnzpzZx6GPHlME4sMSPsYWOb5OFBKp6BIZMkyTKIKT+U4pE4kbiQ9HVVAHlGKJjlkpPH9zqmWrsudWeqUtPo6IGZVmJMTDNIx1iGkYo+G1B87dqWA63RFCNx9v135Mj2f0A4KoRTMsQt1PK24LXyheyda4MlnmjUyjqyX38nqaR9f335KbEo2u9tsu+3Vlc9ub5JY3xV1/Ao92tEMZgupUkvIhFaQG82HSLIg8EyK9KYJ3qxHYr7+wkeGVHvv1KdsNox9lHsrUsXdxbuilmjzNysRjeNboBtcdNgOnaZ5++mk+9rGP8eSTT/LUU0/xxS9+kevXr/OJT3wCCFIst27d4ktf+hIAP/mTP8nf//t/n7/1t/4Wn/nMZ1hZWeHnf/7n+emf/uktC1iPKttFP6SSHeKiO5rho6LJq3vBVTWaqhTUftDY+QV7pCU8WkWlLfGxpfCwN6datPDQtNIw8Zg4nM+DVMRW68TvBKZjsfXuvxFpGzTnAvHRWMggU2M2ubEDhcLxJU1X4e6zFbfFcjglt7cl90IlwaNrGe4v7aclVzFhVCIBMmFWu54tyWRQ/+FNsSpzXSd1ADdhUZ5JUZ06nILUJDHmjdym+TC+gldC+/Vvb2RYbHaLsoupBk/kyzxZqHAh2dxzwMazUixPvI1a8tR+foyxZGAx8pGPfITV1VU++9nPsri4yOXLl/n617/O+fPnAVhcXOT69evR9plMhueff56/9/f+Hk8++SRTU1P8xE/8BL/0S780vJ/iADHwMQwPQ3gEza2KplJIX4ZdKAw9OiHxeqIfw6/9UCoQGq02Wncb4SEIhEdvqkULD00Lw2gPpzuMNIxRd6PIR+JOBcPt/ht0ConQ9TSDM5Xi0NrWhownFZ6UeH54v88akBatltwrExVu9bTktqbkTuyxJddAMmducCoUICmjbUooFazIHLe8KW57U5RV/yv/WiFBeTZF8xCKhreaD9PwBX9eSvPtjezW9uv5oP5jOrZPvyBhsJG9l7XcQyjjZBqjD+wzMgoO0mfkPz7zv1G724wKQpVQCFyEcEG4cMBpkBauqndEP/ZhPNaHLuERiY/+2wqxOdVymCF2zXgRtwXxuIFlHvBnRCpiKzUSdyrEFyvENnqiHzGTxlw6cj2Vh+QtMUwkCs+XeD54UuL6e4+m9iNoyW1yZbLS1ZJrKLhUSvHYembPLbkx3EB8WGvMm2vYHe23rjK440+GAmQSh/7/Nr5tUplOUplO4R+CZX6/+TDb2a+nTZ/HQ/HxaK5KyhzOuaERn2J54gmcWH7njQ+QsfIZOY6UvQa+amIID2F4CA7H+VTi44TRj6YqDy360RIevamWfnQKj5b4OPCTimbssa22KdlBflbMmhvMe7lTIX632hX9UIA7mYwKT53J5FhFPxSB0Z/nS9ww+rHf1tutqJk+3ytU+PM+LbmPrWV56x5bcrOiFqRfrFWmjFLXr78mY4H5mDfFkl9AsnWKpZmJUZ5JUTsEh9Te+TB7sV8fFtKIsVp4K6X0xbExzDtITrwYSSTqOObhOLB6qhFFP1z2/55SBRGO3lRLP4RoRTvadR5aeGh2y6GkYXxJfKUWTby1S93dYX7cpDkfRD6a8xlkYny+vqRSuGG6xfUVvtx/wel2dLbkvpyrITtach8qpnh0Dy25AsWUUYoESM7ojuCu++lIgKzLDGyz78MuSJ0Ii1GnRBqpDF4qp3hhI8MLxSzLu7VfHyLl9AVWCo8gzfHqGj1Ixuev+YA4yHOxQtJUlTACUkLu0na9H1L2RDw8xVbjfQxB2yo9TLWYWnhoBkQAsZhBPBY44B4EZsWJUi/xpSqG1xH9EOBMJsPUSxZ38vBmi+wHhQpqPXwV1Xr4h5QNL0ZTcsuUOlpyF2oxHlvP8tBGeqCWXAufeXM9sF+3Vje13y75BW75QfqlpnaeFusmLCozKSqHUJDaOR/G9+NcKQb1H1dKaWr+Zvv1JwoVHt/Cfn1YBEPtHqeemD2w9xhXTrwYGTaeaoaFpyUcqju/oA+bhIcbdLn0wzDociwN7NK18NDsnQNNw/iS+FItMB1brGCXuycu+wmrPXBuLo2MH/2vKF+FtR5S4fpBuuUwC/G8cEruppZcz+DyRprH1rPMNXbvn5IUTU6F5mNz5kZX+21TWSyG7beL/gTeLk8htUKCykyKRu5gIwECwYzIMCfyuE6eF4pZvryF/frj+QpPFiq8dRv79WGhhMl67gE2cvcfi6F2B8HR/0s/4ihUaDxWwlFlfAYbhCbl5jktWwkP0+hnl66Fh2b/RGmY2F5n/2yNWW5G815iS1WMjsnLSoAznYoEiHtIRlZ7RRFEO1zZKjZV+5rzsh+W4w5XJit8r1Ch3tOS+1jYkmvtqiVXUTCq4fyXVabMStezFZngpjfFbX+KZT+H2qb+oxNpGZRnUodSkJomzpzIU63PcqWY39J+PUi/lHdlvz4s6ok5liYex7Mzh/OGY4oWI3vAx+lqvd3tILSBJtN2Co+wzmNUA8Q0x5ODSsMITxJbqkYCxKr0RD+SVuj5kaU5l0YdQufEXjmo1tpBUShcQ9E0JK9n61yZrHAr1a6pybam5K5lmHB3rsEwkMyYxch+PW2096UUrMoct8IISEml2K7+o5fDKki1MCioAhvV0/yn4gT/qZhhveNn77Jfz1eYTxzuxOxgqN1jVNLjNcpkVGgxskvatuslfJwdt/f9dgvtribTRuZh+5lMq9HszNDTMEphlZ2g82WxQny5hug4aStDBNGPsO3Wy8ePZPRjWK21vlA4QuKaCseQOEb73u18bPY87nne7Vxvbj4OQ8G9YUvuW3bRkhvDZcFa47S5xry1Rky0r4Q8ZXDHnwjab/1JmmowW3xlCKqTYUHqAZvK2V6e5fJZXi3O8N1ypst+PWH4PBbarz+Wr5AdwH59mBSzl1jLX0Ya49diPiq0GNkCHzcqPHVUdUtjM6WCYXDOsZxMqzkumEZ7ON0whK5wfeJh9CO+WMGqdV91eik7Mh1rzqZRhzTIbLcowJWShvSp4lPDpyFkIAasbhHhdoiHXpHQ7/E+R7Zsi1Aw1bR5ZD3DWzcyZLztf68ZUY/Mx2bMYldqoi5tbofdL3f9Aj6D/xt5cYvy7MEXpFYaeZbL53itOMdr1dQm+/UnwujHw9nanuzXh0UzNsHyxOM045MjO4ZxRYuRDhxVDVMv/W3XlQqiG5F/hwvOdpNpzQ7zMD0gTnPIDDUNoxRWsaP2Y2Vz9KM5E0Q/mgtZvGxsqNEPn+BE3xSSpiFpivBxuM7pXBdu015WNIWkIXyccLklHvY5bHZbTAkxaRCTgpg0sMP7mN/zWArsfttJQczvft5SYod2XMWUUQ66X8xVCj22BRt+Kmq/XZNZ9voLqBcSlGdSNIb879xCKrhbneJO6Syvl+ZYanbXf1xINniyEPh/7Md+fVgow2I1f5li5p4TMdTuIDjxYsQzG2zI6ziq3BX9iCbTet11HluljC1r85wWLTw0o2BYaRjh+MTvVkmExmNmvbuX3MvEonkvzmwaZRlBbYNQOMLvKxqahsRpCYRouWNdS1hEIiNY5x3w1W7M7xYEXQLB30IoRI83b2tLA/MglU4HJj5z5kYkQJJGO0olFSyH7be3vEmqau/zwKRlUJ5OUZk5mIJU1ze5Xp7lZukUV0tz1Px2isMUioezQfplKPbrQ6SSOsNK4VH8EzTU7iA48WLEtWo05EZ7Mm1Hncd2k2nboiMoMO2d6qjRHCZmRzfMbkWwRHUJgyY+9kaD3O0ak7fqTCw16dQAngnXz9i8et7iLy6YLE4KHKNJU9RxjKVIQKgD/FMwFCSkQUwZxGVws5UIxIRvYPkCWwrsPtGHXpFhh1EKe8dow9EjLpyo+HTO3MDqsF93lMmiN8ltf4rb3gTuFvbru6WZjlGeTVErJIbuclt1E1wrzXGtuMCtyjSeaoucg7JfHxaelQ6H2i2M+lCOBSdajPzH/++/5NXvvkm17G8rPHpTLbqVVnOYKFRQFGkoHLNdo+CaChVTyJjCs1R3ymLLtEU7IuEainRd8ehVxWNvBLdCjzXOrUm4co/gylsEL50VuLYC3PC2PS0BEA+FQ0yJQEAoI1gfPSe6xEW0HG4f63iNCUemtfZwUeSNKqfNtaD91ih3pSaqMh5Mv/WnWPbz29qv7+rdwoLUykwKZ4gFqUrBaiPHtdI8V4vzLNcnup6fizlB90uhwv2ZGgfktbc/hMFG9j7Wcg+e2KF2B8GJ/k0uX3uDSqmdU43ZQYi7JT608NDslbrpU7f8js6JzYWQLUERFT+asl0Qacqu9cOKNgileMsiofiQ3HubruhH04ZXz5q8dt7i2rkY9ZxFTBrklcF/VukVFy0BIdoCQ7WjFXsZuNZLZ2ttRboja60dBSY+s2aRU9Yqp8y1rvZbgFU/G7Tf+lMUZZphFMB4cZPyTJrqVBJpDaf2wVeC25VprpXmuVacp+y20xkCxaV0gyfyZd5eODj79WHRiE+zNPk4rj3aoXbHkRMtRi7/yHup3n2Z5tqSnkyr2TcrcYeXC1VeKVS7xrIPE7vzxN8jAmJ9oglxaZCtKRZuNpm72WDqVh272R3udnPxsPA0Q3M6xYRp8Hbg7Q3oU8d9YEhCB9MDmlo7DgTup0H0ozf94imDu36B22EKpq6G52Zazycozw6vILXpW1wvzXG1NM/10hyObEdX7NB+/clDsF8fFtKMs5J/K+XMxVEfyrHlRIuRc5cf4bW5Asvl5VEfimYMUShupZu8nK/ycqHKWo+pUtw3ugsjfaNdtxAu2z0Fkp0FkHFpkDFMMpZJWgSRiF1FG6QitlYPBs7dqWCv1bteJS2D5nw6dD3N4h+wL0Q/FEHUwz+EqbVHG8WkUea0tcYpc5UJsztPVpVxbvuT3PYmWdpj++1WSMugMp2iPKSC1JKT5FpxgWuleW5XprpSRTnL5Yl89dDs14eJHmp3OJxoMaLRDIonJNey9SACkq9R7biqMyVcKKe4fyPNvcUU2T2MZYegRikepgqjaN0O391G3Y0GziXuVjGc7qtNp5AIB85lcKZTQy9E3InDnlp7lLHwmDfXOWWtccpcI9HR/dJyP73tTXLLnxxa+qUTJ20HqZiJ/RWkKgVL9QLXivNcLc2z1uhOXSwk6vxQKEAO0359WDh2juWJx2kkZkZ9KCcCLUY0mh2omz6v5Wq8Uqjyeq6G0+GGGfcMLpUCAXJPKUV8j45Xphl2w+zWBE8qYqu1yHQsttGdT5G2EUQ+wtZbeQhj2lt0Tq1tCY/Dmlp7VGmZj50y15gxi13D5xxlcseb4LY/xaI3QZPB3E93gxKC2mSC8mx6XwWpnjS4WZnhWnGea6V5al7b/0OgeEumxDvydX6oUGXhkO3Xh4USFmv5B9nI3qc9Qw4RLUY0mj4UbY9Xw/TLm9k6skMfZB2T+4pp7t9Ic76SxNxjdakQ7XZcaxdzh4yaG5mOxe9WMNzu2g9nIhGZjjmTyUOLfngyEByjmlp7FBFIZsxSVP+RM+pdz5dkMqr9GGT43KAMoyC17sV4M6z/uFGexZPt04ZteNyXXeedhRpPFZojs18fFrXkPMsTj+NZ6VEfyolDixGNhuBqfjnh8HKhxiv5Kos9BagzdZv7NtLcX0yzUIvvy5ciZgviMWPnbi1fElupkbhTIbFYwS52H5MfM6PIR3M+g0wc/J+zROGfyNbanYnhRqmX3tkvUgmW/Ty3/EkWvUnK6mANsur5OOWZNI3c3gpS1xuZoPulNM+d6mSX/XrarnN/boV3FGq8MytJHIMBnr6ZYHniMaqpkzXUzjRgOhNnLpdgKj3ampiTLUa28nHXnAgkipvpRlj/UWU90eHqqOBsNcH9G2nuK6aZbO4vzWGZbVfU7dIwZtUJ6j4WK8SXqhhehysw4E4lI9dTd+Jgox+KYNJ0q7PFH+HU2qOJIm/UOGWucspaY9oodZ33G8pm0ZvkljfJXX8C94C/bqVlUJkKhtX58cHeK7Bfn+RqaZ5rpQU2mt3j7qeTG9yTW+Lt+QpPpC1SRiuVNOZCRAiKmUus5R8+EUPthIDJdIy5XIL5XILpTAzrAGcKDcLJFiPfeIZ3bPwL7qRSLHt5lr0CNZnY+XWascUVkqu5Oi/nq7yar1Kz2yd7UwreUk5y30aa+4op0nssQG1hiGAw3bZpGF8SX27VfpSxy90Tof24GRaeZmnOp5EDnmQGQbfW7kzk/REKkF7vj3U/HXa/BLNf1CGcrIOC1BTVAcWp65vcqMxytTjPm6U5Gn77ytgQktPpFS7m7/C2fIn74wkmRBpDHB/L82ZskuXJx2nGJnbeeIzJJ23m83FmswnmcgliQ/KPGTYnWozUvvlv8a82mBENZsUaCEUTm5JMsyHTbMgsVZVACQGGQBE4EyI6l4PiMCUI1nctcyRHpZ80aqbPa/kaLxeqvJGtdY1jT3gG9xZT3F9M85ZSitg+R64KgjRMbJs0jFluRqmX2FIVw+8YOCfAmUpFqRd3InEgn6FWa63ny8hY7GS21u7Mbr0/Fv1JaupwLmaigtSZFE569wWvLfv1q8V5blVm8Dvs1+Omw7nsXS7m73B/doPzdpo5kSMmpg/iRxgZ0rBZi4baHb/v53TcjCIfc7kEyQOYI3QQnGgxsr7yEKVv3e77XJIGySE5PqkdBIsSO4uaaBtDBFdb0TKBUBIE643Wfvrts/e9tttmgGMLr8aUEO3l8JhAoIzufQIH/iWwEXN5JSxAvZ5pdDmY5poW94cFqGcriaEMNIuF4wL6DacTniS2VI2KT61KT/QjaUWpl+ZcBnUAXx6+ajuZnvTW2p0JvD9a9R+TZqXr2ZqMccufOhDvj20RAidlUSskqEyndlWQ2mm/fq04z1KP/XouVuVC7g4X84ucTm8wb2SYN3LkxfGsnaikzrIy8Si+ufeBgUeNuGUE4iMf1H5kE+OZbjrRYiR2/4M0X/wOXrUGKIRU2PhY+FjKx0CCBBQoJYL5NUogZXAL1nfbafdDqMCGO0CfAmDIAi183LAUpbhPMeahYpJ7BFw0wDcg4ZvkXJuCa5PyTZRQIKooUd2zQDMtA8sWWDGBcA2UEWyPABTElmsk7pSJL9UQsif6MZMKBUgWLx8fqjjrba31fKmLTHdgt94ft/1JNg7A+6MfyhA0MzGamRiNjI2TjkVifzt8JVisTIX1H/OUne7OkLnUWihA7jARL5MXCeaNPDPiLVjHtJXVszIsTbyNenJ+1IeybyxDMJOLMx9GPwop+1i4h59oMTLzd/8u3/Ovsvzaq32fF0gmzDIzVpEZq8i0tUHc6B5dLZVg3c+w7OZZcfOsuDlcGQOlEEohFOEyPY+7n+vcBhluS8dy7zZdy3vZpt/222wjAx8BFAipInEVLG+xn21+9wch0PLA3JbPeuGtvuUWB4mXstudL3NplD28q+lWa22rw0W31u6OwPsjcD4dhfdHL9IyaGRjkQBxktauRWrLfv1aKaj/6LRfN4XP2ewSF3J3uJC7S8puYmMyJ3LMGxdIi2PsLCoM1nMPsJ69f2yH2hkCpjKB+JjLx5lOx3c9mXucGM9/nUNCYbDm51nz87zcDNbkjBoz1kYgTuwN0kaTKavMlFWG5E0Ain6KFa+gi2IPQCj5SO4mmtxJNlhKNPGEwlSB+6nlw1wtxlw1xmzdJuaLobyvoRRGkAHr2qYlzDoFJR3izO1wPfVyw4l+9LbW6iLT3bN7749JVoYw+XYnvLgVig+bZiaGN2Bx8nb260mryfncHS7m7nAms4xtBm3GkyLNvJhiSmQwjsHV9HbUE7MsT7wN186N+lAGZiJlM5cPIh8z2Tj2Eel4OUi0GBkIQUmmKTlpXndOA5AyGm1xYm2QN2vR7Z54UI9SlXFW3LY4KckUY98StxtaKQ/Evk6XVcsPDMjyVa7m6nhRXkyQ9MzI/+NiKYmdNiANxX0eumEEbqixXRqSHQS6tXb/xHBZCFMvCyP2/nBSgehoZGM00zZywOiYVIKlWoE3wwjIao/9+kS8HNV/zKbWo8aaBDbzxgRzIkdCjGc9wSBIM85K4RHK6QujPpRdk0lYUdplNhcnMcTI6bigxcg+qckEbzrzvOkEuciYcLrSOhNmhbTRJB2/y/n4XQCa0uqInORZ97MH5sA4rqzFXF4pBALkZk8BaqFpRf4fZyuJoYyqh9ARNRQgtnX4AqSrtTYcIKejHoPS7f0xZZS6ul1b3h+3vUnuHKD3hzIEzXQ76tFM26gBr26VgpV6nluVaW5VprldncLtSL8IFAvpVS7k73Ahd4dCvD1kz0AwLbLMixwFkToWNQW7oZS5yGr+EaR58Gm1/ZCMGcxlE1H0I32ALfvjgv4NDBlHxbjlznDLDYYrWXhMWSWmrSIz1gZTVom44XE6tsLp2AoQtAeuejmWQ4Gy6uUPr0L/iKBQ3Ek6vFwIOmCWk90dJ/O1GPdvBB0wM43YvhxQOxFALGYQs8XOjqhDRLfWDg8DyZy5MVLvD2kZNDKteg87mP8y4GdJKVhvZrlVmeZmZZrblWmafvdJNW46nM6scDG3yPncXRJW9/yXDHHmjTyzIoctTs53iGPnWZ58nEb8aLYh26ZgLmy1nc8lyI9gUvZRR4uRA8bD4q43yV1vEgi+OAt9imLn7A3m7A2goyg2FCcrXgFHHb8Pr4/izWw9asEtx9ohdKHgfDnJ/cU0922kyLvD/fn7TsY9QHzVFh66tXb/JEWTBXONU9Ya8+b6oXt/eHEz7HIJBIi3Byt+paDkpLkZRj5uVaape93HahseC+kVzmRWOJ1ZYSpZ3ORrZmEwK3LMG3my4mTVpwVD7R5iI3vvkRpqZxowk41HAmQyFTuWRafDRIuRQ0YOWBT7ADeAoCh22SuwMuZFsU1D8nouMCB7LVej2TFYy/YFl0op7iumuVRMkfSHe2VnW4Ebqm2LAy3eC6IeUrfWDpXRen+06j1aaRd/jzn9spOMhMetyjQVt7tOxRQ+C+lVTofiYya10dXl00lBpJgXeaZFBvMInYgPi1ryFMsTb8OzRu8K27JZn88lmM8nmM7EMbX4GAgtRkbOYEWxl8awKLZiebySr/FKocrVbA2/43sz7ZqBA+pGmovlJJYa7pfqbmfC7BeJwvEkrqdwfKlrPYZA4P2xwSlrdQvvjyy3vamhe38E9R521GK7l3qPFjU3HgmPm5VpSk73zBdDSOZSa5zOBNGPudQ6ptE9M0sgsDGxMbEwyYsk80aOpDjadREHhW8mWZ54G9XU6ZEex7jYrI8LWowcQfZWFGuHKZ2jURS7GneiAXQ3082u88Rkww7TL2lOV+NDK0BtYRqhAImJA7s6aUU/HE/i+rrLZViMwvtDWkZHymVv9R4tGp7dFflYb3a3lQoUs6kNzmdWuZDZ4EKmRNoIxYYwsZnpWG4LkJNSgLotQrCRuZe1/EOoEQy1G1eb9XFBi5ExYHdFsS5nYiucGVFRrEJxO9WMClBXE92Fdaeq8aAAtZhmqmEPrQC1RasVNx4TmAfUiitVEPVwvKDrRUc/9o9AMm2UAgFirZI/BO+PVr1HS4Dspd6jheNb3K3OcLsyw83yFHcbWTqVt0BxJlnngWyFy9kaD2caZKO3y4Q3zU4045MsTTyBEysc2nseF5v1cUGLkTGkX1HshFmOxMm0VTyUolhPBAWoL+eDCEilowDVkHChVYBaTJN1h/9Ri6bi2gLrAFpxFUGLrePr6Mcw6fb+WCcm2q7GB+H94SbttrlYOoa/zRVtKyUS64pOWNGykjZvVnK8WsnxcjnL1VoS2SOszySaPJyt8XC2ykPZGllLbvFump2Qhs1q/q2UMm858HlWx9VmfVzQYuQYIDFY9fOs+nlebp4j8FqoBuLEDmpPUjsVxbqFXXUdNIxgAu4r+Rqv5as4HRNwY77gUjHN/cUU9xRTJOTwIzFCiKgTxjqAVlxfBVEPHf0YJrvx/pjgtje1b+8PJQRO2g5SLtkYfjqOZdrYmCSFSQ5rUxqkKyXS0w7rSsGr1QTfKad5sZzi1WoSX3V/5ubiThD1yFZ5OFujYPto9k8lfY6VwiMHNtTupNisjwtajBxLBEWZoehkwqJYFRbFFqPC2Fy/olg/3tWx0yqKLdle4IBaqHItU0d2RMozjsl94QTc85UkljqYP+aYLYjHjKF7gejox8HQ8v5YMNc4ba0OxftDIDAxMKN7A2FZqEwSsilEJo2RSmKbdhDNwMAYsMvEV/B6NcH3y2m+X07xF5Ukbk9R9ZTtcjnXFh/TMW+LvWn2gmtnWZ54G/XE1pOm9spk2ma25XSajWOdAJv1cUGLkROBoCaTvOkko6LYuHDCtE5HUazZJG0GRbFv2Bb/Jpnhj1NpXkl2Xy1O1+1IgJyqxYde/9Gi1YobG7IXSDv6oWe7DJOEaIZzX3bn/WGEoiIeCotIaIiOxx3LBgLiMVQmicokkdkUJGL7Ct9LBdfq8UB8lALxUe+J6OUtj4ezNS6H4mMu7h50xuBkIgzWcg+wkXsANSTDtmzCYj6fYC57cm3WxwUtRk4ozZ6iWAOXcm6dVwtlvpPzudWRVxdK8WjT4T+r1nm0ZJGo51j2DFY9G3/IQsS2AvExzFZchcLzw+JTXzudDo/tvT/qMsGKP8OqP0fJn0EQwxQGc8LAFAJjF8WoKhlHZQPh4WWSENtfnZNScLMR48Uw8vFSOUW1x88mbfpRzcflbI3TCUeLjwOmnpgLh9pl97UfbbM+vuh/qROMJyRXs3VeLlR5NV+jGuW6TUwJ91VsnqpIfqxW5j4ROMUCkFwDWkWx2cjrZMXL76ko1jJDATLEVlxfKVyvnX7R8mMwNqdETEwEMSGZMVeZNpeYspaIiXb6RSmoyilK/gJF/xR1lafVWZLZzT+rIVCpBDKbRGVSQfrF2t+VrFJwp2nz/XKKF8tpXiqnKHrdX3tJw+fBbD1Ku5xPNje5nGoOBt9MsFJ4lEr63J5er23Wjw9ajJww6mZQgPpyvsrruRpuRwFqwjO4VAoMyN5SShEPi0NeAl7asii2xJRV6iiKTbf9TrYpijWNdifMMFpxFYHbqaujH33prLPouon+6ZHOqEVMVMibt8mbi2SMZYyO9IuvLEr+PEX/FCV/Ho8BnIFNo0t4qHQCjP3n8FccixdLQeTj++UUqz2jBGJC8kCmzkPZKpdzNd6SajCiwcwnmlLmHlbzlwcaaqdt1o8vWoycAIq2yyuFQIC8ma13TcDNOmbk/3GunMTcMu2y26LYKnmz2rcodsWfoGmlidvGUFpxgzkvMrqdFPnRr5CzLSxE33qLwep6JBljiby5SM5cJGmUup5tyEwoPhaoyGnULv1rVMxCZQPhITNJSMaH0q654ZpR5OP7pRR3ne6TmyUk96YbUdrlUrqBbZyUT8vRw4kVWJp4nGZ8asdttc36yUGLkWOIQrGUdAL/j0KVO6nuCbiz9Rj3hRNw5+t7nYC7XVFsIE4KHUWxF0KnWEfZbFAIbxOU2b1TbBD9CIpOj1P0w+gbtdi6kNM8AGddkyY58w558zY58w6W6LReF1TkDMUw/dJUu8vrR/UemSD6QXw4IfSyZ/BSWPPxYjnFrUa863kDxT2h+Hg4W+P+TJ24Fh8jZ7dD7Vo263O5BLNZbbN+UtiTGHnuued49tlnWVxc5OGHH+Zzn/sc73nPe/pu+41vfIMf+ZEf2bT+Bz/4AQ888MBe3l7TB4niRqYRTcDdiLfbDYWCM5VEZME+6RxMXrVVFHvbnSFmC5IxybRVZEIE0iNPkZhwmWWZWZYB8JRJkXwkTorkkR1X2uMa/TAxiAVG3mG9RVC02S89sptCzuGjSIhSlH5JG6uIDut1V8Wj9EvZn8PfyXpdgEonh1rv0aLmG/ygnIzabd+sx7tagQWK88kml3OB+HggUydlaqOxo0Q1eZqVicf6DrXTNusa2IMY+epXv8onP/lJnnvuOd797nfzhS98gfe973289NJLnDu3dRHSyy+/TC7XntMwMzOztyPWRLhC8kauVYBapd7h9GhJwVtKSe7bSHNvKU3aO/g/8FYXTLsV12SdadaZBgLr75wqhXGRdQpsYAuPKdaYol0UWyTLip/nrp9jyc3hMD5FaSYGkyJNXiRHJDK2RuCTNZbImYvkzdvEjVrX8zWZp+SfougvUJWTsN3xm0aUblHZFCqVCBL6Q6DhC16uJqN229driU0+JC2X08uhy2lGu5weSTwrxfLE26glT0XrtM26ph8Di5Ff+ZVf4Wd+5mf423/7bwPwuc99jj/6oz/i85//PM8888yWr5udnaVQKOz5QDUBNdOPDMjeyNXxOsLPSc/g3tD/42I5SUwe/MkwasWNCYwd8v8KgyIFihR4kwuAIq0q5NQGebXOpNggZTSZoMSEVeJeC4jDhp9iWeZZ8fMs+7ldOcUeNiaCCZGmIFJHSoTYok7OCMRH1ryLKdruoFKZlOVsmH5ZwFXpbXZkIrOpIOqRTaKGVO8BbZfTVrttP5fT+bgT1Xw8pF1Ojz5CsJG9j7XcQ5iWzYK2WdfswEBixHEcXnjhBT71qU91rX/ve9/Lt771rW1f+7a3vY1Go8FDDz3E//w//899Uzctms0mzWa7ZbBUKm257UlgPebySqHKy/kqNzKNrgLUfNOK0i/nKomhT8Dth2W2O2H2UsmuCGo+XE+x7sfx1SwwCyjSosmMWWTGLDJtFskbdQpmjYJZ4157EYCqjLPs56NbSSUZ1vj4QTEQFEgzYaQOpJZjcBQpYz1Kv6SM9a5nHZmMaj/Kcha1xVeASsYi4SEzSYgPb1y9p+CN0OX0xXKKl7d0Oa2Gfh/a5XSccBJTeGee4tTMPG/TNuuaXTKQGFlZWcH3febmum165+bmuHPnTt/XLCws8MUvfpEnnniCZrPJl7/8ZX70R3+Ub3zjG/zwD/9w39c888wzfOYznxnk0I4VCsWdpBMJkKWeAtS5WizqgJndcwHqYOy3FTeq/fAkrtyq9kNQVQmqXoJrXvAZi+MwbZYigTJhVEgbTdLGEhfsJSCYbbLi5yJxsi4zu7IX3w8CQYEkE0Ya6wCnIe8GA5eseZe8uUjeXMQWjei5bu+PBeqqwCbhJkClEqhsKqj5SCfBHl5te+RyWgrEx19UkjT6uJxezlZ5OBeIj7mYdjkdJ9Jxk0w6TeriO5g89zDWkOqFNCeHPX3j9IbYlFJbht3uv/9+7r///ujxU089xY0bN/jH//gfbylGPv3pT/P0009Hj0ulEmfPnt3LoY4NPorrmXrUglvqKUA9V0lw/0YwAbdwQAWovRgitGOPCawBBUhn9MP1Jb7aW+lpkxi3/Glu+UHdiYXHlFkOxIlRZMoskxAuZ6xVzlirALjKYLUlTmSeVT+LPyTBIBDkSDI1YhGyL++PznqPTCg+hjijQym40YhFBaf9XE4zph91uzysXU7HjoRtkk9aZBM2uYSFPXsfnHkS7IMZaqc5/gwkRqanpzFNc1MUZGlpaVO0ZDve+c538pWvfGXL5+PxOPF4fMvnjwuOIXk9F4iP1/I1Gh1FeLYvuKeU4r5imnuLKZL+4Zz4hAiiH7GYwB7QC8STLQEi8baMfuwPD4u7/gR3/QkgGMg2YVSiyMmMWSImPOatDeatDQB8JViXmShysuIPXhQrEGRJMGWksUfSES/JGKth6+1W3h8LlPxTm70/onqPsNMlGWeYFqMtl9NOi/WtXE5b813OaZfTsSJmCXIJm1zSJpewibfabRN5OPcU5BZGe4CasWegb9VYLMYTTzzB888/z4c//OFo/fPPP88HP/jBXe/nO9/5DgsLJ/PDW7E8Xs3XeKVQ5Y1sHb+jADXlGtxXDKIfF0tJbHU4NQiCsBNmwKm4ktbAucB6fRSNtxKDVZljVeb4C/csrXH1M0YxEigpw2HaLDNtlnmQm8BgRbEtERI75K6etvdHYD62W+8PlYgFRaaZVFDvkRhevUeL5aYViY/vl1Os9XM5DaMel7M1LmqX07HCNAS5pB1FP1K9A+YME+Yfgfm3BssazT4Z+BLv6aef5mMf+xhPPvkkTz31FF/84he5fv06n/jEJ4AgxXLr1i2+9KUvAUG3zYULF3j44YdxHIevfOUr/O7v/i6/+7u/O9yf5AizFnd4OV/j5UKVm+lGV8p+otEuQD1TPZwC1BabW3F3phX9cDyJf0DRj/0hKMo0RZnmNe8UQVFsg5mOupPcLotiMySYNDIkDk2EbO/94alYu/i05f3RWe8Rpl2GWe/RYt01g4LTUhD56Odyel+H0Zh2OR0vDAOycSuKfKTj1tbfRLnTcO6dkMhttYVGMzADf2t95CMfYXV1lc9+9rMsLi5y+fJlvv71r3P+/HkAFhcXuX79erS94zj8j//j/8itW7dIJpM8/PDD/Ot//a/5a3/trw3vpzhiKBSLqSYvhwWoK0m36/mFapz7wxbc6YZ9KAWoLWwrqAOx7Z1bcSGMfngSxx9d9GN/CKoqSdVLtotihcOM0RYnhT5Fsa6KUfVnqMhpKnKGmiywre/Gno+u0/tjkbhR7Xp+k/eHYQb1Hq20y5DrPVqUPJMfhA6nL5ZT3O7jcnopXY9qPu7P1Ilp8TE2CAHpuEUuEUQ/MnF757SZnYKzb4fJtxzKMWpOFkKpPVYWHiKlUol8Pk+xWOwyThsG//bXfp7l117d9358obiWqfNKocor+SrlWNsHwVBwvpyMClBz7uHWHFhmWIi6y1ZcV7bSL0Htx3HHwmPaLLFgVpgzK+SMdQzR7WPhK4uqnKLiB+KkKie3bIvdie29PwzKcq7t/WHlIjv1wN8jMdR6jxaBy2kgPAKX0+60lUBxIdWMaj4eyNRJapfTsSIZC4pOcwmbbMLGGuRzNPsgnHocrOGn/DTHm92ev/Vsmn3QNCSv5Wu8EhagNju+nGNhAer9G2kulVIkDqkAtYVptDthdhosNf7Rj/1hksCU01RVgje8IFqRMtbJGCtkzGXSxgqWcMmZd8mZwYwdqQxqcoJqGDmp+FP4bFV03fb+yBmLpM3+3h8leYqSfRo/m4uiHwdR7wEdLqdhu+0bfVxOzyYaQc1HrsaDGe1yOm7EbSMoOk3Y5JIWsb1E0FJTcP5dkJ4e/gFqNB1oMTIgZcuLoh9Xs3U6TU7TrhkMoCumuFBOYanDrdgzjHYnzE6tuCct+tGPGBZTIkNGxLtSZQqTqpymKqe56z1AUMtRJGOukDGWyRgrxIw6GXOVjLnKHC8DUJf5KHJSkxMkjOKW3h81OUlRnmLDPk8tdxqZSwcpl9jB/Ek6UvBqNRkVnPZzOV0IXU4f1i6nY4ltCrJh2iWXtEnsx+vDtINIyMwDQUGJRnPAaDGyC1biDi8Xggm4t9LNruemGnaQftlIc7oWP9T6Dwi8QGK7aMU96dGPTmxMpkSGrEjs8t9L0FAFGl6BFS4BipioRpGTjLFCwiiTNIokjSIzvL5pD76yKMl5ivZ51jOXcPPTB1bvAYHL6esd4qOfy+l0zI0s1h/O1pjSLqdjhWkIsgkrarlNxczhfPtMnIez74DYNuMBNJoho8WIsblTQqG4lW7ycj4QIKuJ7gLU05V41AEz3Tz8HKoAYjGDuC2wtmjFVYAnA9Mxx/fx5MkVHy0szGiI3f5Eo8BRGdb8DGv+hXDfDTLmCukwcpIyNmiSpmidZz19D+XCJWQ6fSD1HhC6nNbiUbvtDyopmj2ziQqWx8O5tviY1S6nY0cmYVFI2mSTFpnYLopOByGeDURI4XgbTGqOJlqMZKYhOYnnV7mW2ODlfIVXClWqHSFqU8KFcor7iinu20iT9Ubza4vZQRrG3qIVV6JwvJYAkagTHP1QhkAJAQIMw2TSyDBhZDAMAwwjmO9jGME2hgjaC3qW1XbrDSNQhcIAQ+AJQUM8wHK0nYJYbGjD5HqRCm424ny/nOLFUiA+el1Os6bHQy2vj1yNU3HtcjpumIagkLIpJGPkkxb2QUTShAFzl2HhUTD1KUEzGk70J6/klPg2r/Pt06/zemYDx2jXTsR9g0vFoAD1nlKK+CFMwO1HqxW3nxfIUYh+qOik3Vrut06gxPbr6dpmkG03b9M641oYnDEmOS0msISBBMa1OkYpWGzakcX698spSn1cTlviQ7ucji/JmEkhaVNI2btrud0PmTk4/xQkJw7wTTSanTnRYuS/++P/jitcgbDbKOvGuK8ywf2VCc6Xk5heE7zmtvs4CPq24hrBSdoHHKloSoXjK6QAFRMoYUcnckR4sg6jA9ud9INtOwTANvvY6qR/1DAxOCUKnDUmscX4ukPu5HIaNyQPZNriQ7ucjieGAdmEzUTKJp+MkbAO4cLHSgSzZKYuHdm/Y83J4kSLkR8+88PcWb7GPatp7q9MsNBII0QYuo8LVDwFKJRXB68epD3C0H9QctA6cQfLGCJIjLRO/K0TdudjROCdFa0LXmMmTKysjZW3UTGThoB6KASaUlJzfOqOj+OP67X9wWMgIhESE+P30V5zrEh4fL+cYqnH5dQWknvTdS6Hk20vpeocxnlLM3xilhGmX4LiU/MwBcH0vXD6SbC3HoGg0Rw24/eNPUR++vJPc3E9xw3jFZiB+nYtbEqCU4XGBnjOUN5fxAzsnI2VtTHi7St4VyrqrketIWm4PvLo+9KNFIFgXuQ5Z0ySEIc7P2Y/lDyTlzqMxnpdTk0U94Qup5ezNe7TLqdjS8vxtJC0mUjFSMVGELFL5APPkOz84b+3RrMDJ1qMmIYJprm7PnphBNXm8Sy4DWgWwakwaI2osARWzsbKxTATwReSAuquH9x09GMg5kSO88YUSXH0nSGrnsEPKoHweLGc4nofl9OLqUaUdtEup+PNoRSf7gbDhIXHgiJV7RmiOaKcaDGyZ+xEcJNT0CxBowRyG4MoU2BlbeycjZE0EULgSkW16VJ3dPRjL0yLLBeMKdJiK9fT0dPwBX9Raadd+rmcnku2xYd2OR1/ouLTtD381tu9kD8TtOvqoXaaI44WI/vBsCA5CYmJIErSLAVREwADrIyNlbMx0xYIQcP1qdddHf3YB1MizXljmqw4uvnu75dT/M7tKV6upPB7xMdCvBn5fDyUrZHXLqdjzUiKT3eDnYKzPwSTF0d9JBrNrtBiZBgIEaRvElnMlIkVq2FZFTzpU3N96pUmDVfq6Mc+KIgUF4wp8iI16kPZkuWmxZdvzvIfNtpXoTM9LqeT2uV07ImKT1MxcgnrcItPd0IImHkQTr1ND7XTjBVajAwBM5fEmsxh5NNUPMVy3aFYrSNqSyQbyxhKX/3ulRwJLhjTTBhH15q6KQVfuzPFv7oziasMBIr/fGaD/3Jujbm4u/MONEeaVvHpRFj/MZLi092QnoZz74L01KiPRKMZGC1G9oiRTmBNZvFzKUquYqPuUr5dou07ZkBynlpijri7QbKxjO2VR3nIY0WaOBeNaaaMzKgPZUuUgn+/keUrN2dZcYIunocyVT5+donzqcP3p9EMj8MuPpUIHGzYy5gCw4K5h2HyLYFyajR2fo1GMyRs28Y09y/QtRgZAJGIYU5kqKdTFH1Fse7RvFvd4UWCZmyCZmwCy6uTbC6RaK4T2JdpekkS44IxxYzI9rW8Pyq8WYvzz27M8lIliNhMx1w+dmaJdxTK2kNqTEnFTAqpGPnUAcx92QYHm6vGBaQRY2AxYlpgxqBkQOnaQRyeRrMjhUKB+fn5fX1nazGyAyJm4efSVJMJShiUGy5yY29XHp6VpGydp5I8Q7K5QrK5jCH1FTRAApvzxhSzIodxhM/mZc/gX9ye4fnlAgqBLSQfnF/lA/NrxLUHyFhhGJBL2FEEJD6C4lMFLIo5zGSes7NTu//sCxEUqeq6EM0IUUpRq9VYWloCYGFhYc/70mKkD8o0aKSSVJMJisKg6Suo+wwrmqEMk1pyjlpijphbJNVYwvZKQ9n3uBHD4rwxxbzIH2kR4iv44+UC/+L2DJVwIN07J0p89PQSM3FdlDouxG2DQtImf0SKTz1MamaOU5MFUondCAsRWguktI275kiQTCYBWFpaYnZ2ds8pGy1GQlwF9WSCciJOybQCq3YJA7uaDYIAJ5bHieUx/QbJxjJJZxVOQMGrjclZY5JTooApjkg75BZ8v5zin92YjUzKziUbfPzsEg9nayM+Ms1OCAGZuBVFP45a8amPCRjE7G2OS4ggFdO6aRGiOWKkUkGXo+u6WozslXUsrqbS1BPxkboT+maCSvos1eQpEs4qyeYypn/8CtF6J+keZZabFl+5Ncu/Xw9adTOmz0+cWubHZjb0QLojjGWKIPoxaufTXSHC//d8oLQA0YwRw6jvO/FipJHOUa8mR30YEcowqSdmqcdnibklks0lYm5x1Ie1b8Zpkm7QqjvJv7oz1dWq+xOnlslqh9QjyaiKT4eKFiCaE8yJFyNHFgFOLIcTy2H6TZKNZRLOCmLMUjginKR7bgwm6epW3fHhKBSfDgXDDjpiYmlIZrUA0ZxYjvbZQQOAb8appM+EKZw1ks0lTL8+6sPalmCSbo5zxtRYTNK9Xo/zT693t+p+9MwS79StukeGo1Z8umfMGBTOwcQFiE3Bm28GXTHj+vNoNENAi5ExQhkG9cQ09cQ0MbccpHCcIgdaZLsHxmmSbiVs1f03Xa26a3xgflW36o6Yo158OhBWvC1Asqfa9WnHwKDs13/913n22We5efMmP/dzP8ezzz476kPSjCFajIwpjp3FsbOYvkOyuUyiuYJQo20xHYdJui10q+7RZLyKT3fASnQIkIWRFsgfFC+++CKf/OQn+f3f/30ef/xx8vn8qA9JM6ZoMTLm+GaMSuo01cQCcXedZGMJyz/cltNJkebCEZ+k24lu1T1aHIvi0xZWAibOBwIkM38sBUgnX/va13jiiSf48R//8VEfimbM0WLkmKAMg0Z8ikZ8CtutkGwuE3fWOcgUTp4UF82jPUm3kxUnmKrbatVNmz4f0a26h86xKT5tYSeh0BIgc8degLS45557eOONN4CgtfOjH/0oX/7yl0d8VJpxRYuRY4hrZ3DtDIY8E6RwGisYanjTY3MkOG9MM3mEJ+l24kjBv7ozydfuTOHoVt2RcGyKT1t0CpDs/NCKT5VS1N3RdMwlbXMgv4g/+7M/46mnnuLv/J2/w0c/+lHS6fH4PtAcTbQYOcZIw6aaPBWkcJx1Us0lLG+HwX7bMA6TdDtRCv7DRpYvd7Xq1vj42bu6VfeAOVbFpy3sVEcKZu5Aul/qrs9Dv/hHQ9/vbnjps3+VVGz3p4RMJsO1a9f4S3/pLzE/P8+HP/xhvvGNb/CjP/qj/M7v/M4BHqnmOKLFyElACJrxSZrxSSy3GkRLnDV2m8IZl0m6nfS26k7ZLh87q1t1D5JjVXzawk4F4mPiAmRmdfttB9/97ncBeOtb3wrAz/7sz/LTP/3T/PZv//YoD0szpmgxcsLw7DRlO01VnibRXCXZWMZQTt9tx2WSbie6VfdwOVbFpy1i6bYASc8cqgBJ2iYvffavHtr79b73IFy5coVLly5F6Zkf+ZEf4Rvf+MYBHJnmJKDFyAlFGja15Dy1xBxxd4NkYxnbKwPBJN1zxiQLIo9xxOfHtJAK/nilwFdvtVt131Eo8bEzulV3mBy74tMWsUyHAJkeWQRECDFQqmSUXLlyhUcffXTUh6E5JozHp15zcAhBMzZBMzZBwve45AsuNZpYjE9h50vlJP/0xlxXq+5/c3aJy7pVdygcu+LTFp0CJDMz6qMZO65cucIHPvCBUR+G5pigxYgGE4vZxEVm4ucxhc0N3yFXvUq+8vq+Cl4PmhXH4is3Z/kz3ao7VI5l8WmLeLY7AqLZE1JKvve97/ELv/ALoz4UzTFBi5ETjIHJTPw8s/GLWEbbul2aMTZy97ORvY9UY5FC+TWSjbsjPNJu+rXq/tjMBh/Rrbp75lgWn7aIZ2HiYihApkZ9NMcCwzCoVo/uhYpm/NBi5AQiMJiOnWUu8RZsYxvXVCGoJU9RS57CdkvkK6+Tq15DyNHUYLRadb9yc5bljlbd/+bsXS7oVt2BOZbFpy0SuTACchFSk6M+mhPBX/2rf5X/9J/+E9VqlTNnzvB7v/d7vP3tbx/1YWnGBC1GThACwWTsDPOJe4gZyYFe69o5Vibexlr+MtnqNfKV17Hd8gEd6Wau1+P8sxuzfL/c0ap7Zol3TuhW3d1ybItPWyTy7RSMFiCHzh/90Wj8UTTHAy1GTggT9inmE5dImPtzSZSGTTF7L8XMJZKNuxQqr5GqLw7pKDejW3X3x7EtPm2hBYhGcyzQYuSYk7fnWEjcS9LMDnfHQlBPzlNPzmO5FQqV18hWr2HI4djOb9Wq+9Ezy8zGh2dtf9w41sWnLZKFtgBJToz4YDQazTDQYuSYkrNmWEjcS8o6+JHenp1hZeIxVvMPk61eJ195jZhb2vP+elt1zyYafPycbtXdiq7i05SFfRwHtSUnOgRIYcQHo9Foho0WI8eMjDnJQvJeMtbhh6yVYVPK3kMpew/JxhL58mukG7eDytNdoFt1d8+xLj5tkZoMxEfhvBYgGs0xR4uRY0LKzLOQuI+cfTS8E+qJWeqJWSyvFnThVN7AkP1t5x0p+NqdSf6VbtXdkmNffNoiNRVGQM4H9SAajeZEsCcx8txzz/Hss8+yuLjIww8/zOc+9zne85737Pi6b37zm/zlv/yXuXz5MleuXNnLW2t6SBpZ5hP3UojNjfpQ+uJZKVYLb2Ut9yCZ2g3yldeJO+tA/1bdB8OpurpV9wQUn7aIBMiFoCVXo9GcOAYWI1/96lf55Cc/yXPPPce73/1uvvCFL/C+972Pl156iXPnzm35umKxyE/91E/xoz/6o9y9e3QMtMaVuJFmPnGJCXthLCbpKsOinLlIOXORRHOF8p3X+RevwvfLKUC36sIJKT5tkZ5up2C0ANFoTjwDi5Ff+ZVf4Wd+5mf423/7bwPwuc99jj/6oz/i85//PM8888yWr/tv/9v/lp/8yZ/ENE1+//d/f88HfNKJiSTziXuYjJ1GjMkQu05qjsfX/sLhP7yRQgG2Ae+f3+BDc3dPZKvuiSg+bZGeaadg4kPu7tJoNGPNQGLEcRxeeOEFPvWpT3Wtf+9738u3vvWtLV/3T//pP+X111/nK1/5Cr/0S7+04/s0m02azXaYvlTae2fGccEWceYS9zAVO4Mhxu+KWSrF/+/qGn/8g7vUHB+Ah0/l+GuXF5hMmWzUblGovEq8uTbiIz14OotPs3GbYx0IyswG0Y+JCxDPjPpoNBrNEWUgMbKysoLv+8zNddcnzM3NcefOnb6vefXVV/nUpz7Fv/t3/w7L2t3bPfPMM3zmM58Z5NCOLaawmYu/hZn4+bEUIQBXV6r839+9zWKxAcBcLs5/+cgp7pkJTk4KqKTPUUmfI95cI195jWztBqjjUbxqGoJswjr+xactMnNB9KNwXgsQjUazK/ZUwNpbo6CU6lu34Ps+P/mTP8lnPvMZ7rvvvl3v/9Of/jRPP/109LhUKnH27Nm9HOrYYmIxk7jAbPwCprBHfTh7YqPm8Acv3uF7t4oAJG2TH3twlh+6OIW5RS9qMz7JUvyHWC08Qq4STA42/fphHvZQsE3BZDp2/ItPW2Tm2imY2P5cfjXjxa//+q/z7LPPcvPmTX7u536OZ599dtSHpBlDBhIj09PTmKa5KQqytLS0KVoCUC6X+fa3v813vvMd/vv//r8HgtHTSiksy+Lf/Jt/w1/5K39l0+vi8TjxeHyQQzs2GJhMx88xF39L1yTdccL1JX/y6jJ/8soyrq8QwA9dnOTHHpwjHd/dR843E6znH2Q9dz/p+i0K5ddINFcO9sCHRCpmct98lvhxmnzbj+x8uwg1lhr10WhGwIsvvsgnP/lJfv/3f5/HH3+cfP7otGP/yZ/8Cc8++ywvvPACi4uL/N7v/R4f+tCHRn1Ymi0YSIzEYjGeeOIJnn/+eT784Q9H659//nk++MEPbto+l8vxve99r2vdc889x7/9t/+W3/md3+HixYt7POzjx64n6R5hlFJ8/3aJr7+4yEYtsGy/MJXm/Y8usJAfbDBfhDCops5STZ0l5mwEKZzqDYQazeTgncgnLS7NZrGOowuZEJBpCZBzWoBo+NrXvsYTTzzBj//4j4/6UDZRrVZ59NFH+Vt/62/x1//6Xx/14Wh2YOA0zdNPP83HPvYxnnzySZ566im++MUvcv36dT7xiU8AQYrl1q1bfOlLX8IwDC5fvtz1+tnZWRKJxKb1J5X9TNI9StwpNvi/v3ubN1aqAOSTNu+7PM9bT+eH1nrsxAosTz7Jav4RctUghWN51aHsexhMZ2JcnM4cLzdUISC7EBahngd7fD+jmuFyzz338MYbbwBB6v6jH/0oX/7yl/e8vz/8wz/kl37pl3jxxRcxTZOnnnqKX/3VX+Wee+7Z0/7e97738b73vW/Px6M5XAYWIx/5yEdYXV3ls5/9LIuLi1y+fJmvf/3rnD9/HoDFxUWuX78+9AM9jgxrku4oqTkef/yDJf7DG6sowDIEP3zfDD987wyxAyrUlGaMjdz9bGTvI12/Tb7yOsnGaL1rThWSnJ04JidqISB7ql2Eao9npG4sUQrcEc1gslMMYvLzZ3/2Zzz11FP8nb/zd/joRz9KOr2/77FqtcrTTz/NW9/6VqrVKr/4i7/Ihz/8Ya5cuYJhGPzyL/8yv/zLv7ztPv7gD/5gVwacmqOHUGqXg0NGSKlUIp/PUywWyeWGa5D05e/8W7679OpQ97kTBzZJ9xDZrlV3In34tS62W6JQfo1s9c1DTeEIAeenUsxlx/yELYwgAtJKwWgBcig0Gg2uXr3KxYsXSSQS4FThl0+N5mD+P7cHKj6u1Wpks1m++c1v8s53vpPl5WV+6qd+iqWlJZrNJp/73Of4sR/7sT0fzvLyMrOzs3zve9/j8uXLrK2tsba2fev/6dOnSSY3XxQIIXTNyAGy6XPcwW7P33o2zSGStaY5lbjvUCbpHiS9rbqz2Tjvf7TdqjsKXDvH8uTjrBbeSrZ6jUL5NSyvcqDvaRqCe2bSTKTGs9AYYUDuVCBA8me1ANEMxHe/+10A3vrWtwLwf/6f/ycPPvggf/AHfwBAvT5YF9zrr7/OL/zCL/Dv//2/Z2VlBSmD1v7r169z+fJlJicnmZw8/AGgmsNBi5FDIGNOsJC8bySTdIdJb6tuwjb4zx+c27ZV97CRhk0xey/FzCVSjbvkK6+Rqi8O/X1sU3DvXJbsLruDjgzCgNzpMAJyFqyT2bV2ZLFTQYRiVO89AFeuXOHSpUtReubtb387/+Sf/BO++c1v8rGPfSzqoNwt73//+zl79iz/+//+v3Pq1CmklFy+fBnHCQZs6jTN8WbMvknHi6M2SXev9GvVffvFSf7zAVp1Dx0hqCXnqSXnsdwKhcprZKvXMKS7710nbJP75zMkrDExoesSIOfAGtNIzklAiLHxably5QqPPvooAOvr6/wv/8v/wve//30A3va2t/EjP/IjPPzww7va1+rqKj/4wQ/4whe+EImJP/3TP+3a5hOf+AQ/8RM/se1+Tp8+PeiPoTkiHNEzyXhz1Cfp7patWnX/y0cWOFUYn2JNz86wMvEYq/mHyVavk6+8Rszd24iBTMLivtkM9lH3EDHMMAVzMUjBaAGiGTJXrlzhAx/4AACf//zn+cAHPkAqFURXHnvsMe7evbtrMTIxMcHU1BRf/OIXWVhY4Pr165vGjgyapqlUKrz22mvR46tXr3LlyhUmJye3HeqqGQ1ajAyRuJFiPnHv2EzS3Y47pbBVd/ngWnUPG2XYlLL3UMreQ7KxRL78GunG7aCDYRdMpGPcM5M+um6qhtmOgGgBojlApJR873vf4xd+4RcA+M53vsPf+3t/L3r+xf9/e/ceF3WdL378NcPAcL/McFcEBERxEFQ0CRTM23q89LDa2trIWmvLTY1ocSvT3Pa4urgWWskRjx7pcTatU9lxH+uNn0Z5QRKSBOFsiRBdSNBcQFQg5/v7g5xEAWe4OFzez8djHg9nPt/5fN/fT5/m++bz/X6+n+JiIiIiTO+3bdvGY489RnvzJdRqNTt27GDJkiUYDAbCw8PZsGEDiYmJnY4xPz+fyZMnm95fe6r3/Pnz2bZtW6frFT1DkpFu0NdX0r3etam6n5afx6i0TNWdGOZFwrCem6prDZftvbls743mxwbcLpbherEctbGp3e19XLUE6p1636J2ahtwG/xzAmLTN5cOEH2LWq2moeHnZ/zodDo+//xzJk2axNatWxk5ciS+vr6m8oqKChISEjqsc+rUqZSUlLT6rCuTPRMTE7v0fXF7STLSBRqVFl/7oejtAvrsInbXGBWF4xU/kF3SO6bq3i4/apw47z6KH1wjcL70Ne4XT2PX9K9W2wToHPDv7BNke4Jac10CMlgSEGF1qamp/OpXv2LLli0YDAYyMzNble/bt4/169dbKTrRF0gy0gktK+kG46UN6vNJCPTOqbq3m6LWUO8cTL1zMPaN53Cr/xKXy98y1NMJT+dekIy1SkACwEb+1xW9R2hoKPn5+e2W5+bm3sZoRF8kv2gW6A8r6V6vL0zVtYYrWk+uOnphCHbE88pXcO4LaLbCysFqTcv0W48gcB0sCYgQot+SXzcz9IeVdK/XfNXIoS9r+LgvTdW9jRztbEgM98Ld0Q7Qg18UXKiA6lJoqOnZndvYtox8eAS13IwqCYgQYgCQX7oO9IeVdK93barunuIqLpim6joye5R/n5qq25PcHW1JDPfC0e66/zXUNqAPaXldrIGaUvihHBRj9+zUxu66EZBBLfsTQogBRJKRNrSspDsIX/vQPr2S7vX641Td7ubjqmXirRb4c/ZqeQ0eBzX/bHl1ZmEzG7uWB5B5BLU8D0QSECHEACbJyA08bP3wtQ/r0yvpXm+gTNXtqiC9IxOG6lGbe6+MrQP4R4PvKPjXVy2XcC7eYuVgSUCEEKJNkoz8pD+spHu99qbqzjT4oevHU3U7I8LflegA9859Wa0GXXDL69IPLUnJD2VgbGlzNNqfExAX/5bthRBCtDLgkxFXWw+GOcfipHG3dijdpq2purNH+RPqPXCm6ppDpYKYQA/CfLopAXXUQVAcDBoLP5wBezdw8ZMERAghbmHAJyO+jkNaPUmwL2trqu7UET7cMcCn6rZFo1ZxZ6iewR6WrVRqFlt78Im49XZCCCEASUb6hTan6gbpmBYhU3XbotWoSQj3wtNZa+1QhBBCIMlIn9bWVN1AvSNzZKpuu5ztNUwO98LFvu8/tE4IIfoLSUb6qLam6v7C4MsomarbLr2zHQnDvLC3lVksQgjRm8iddX3M5aar/P3z73jj4JecqWlAo1YxOdybZ6cOI2qwuyQi7Rjk4cCU4d6SiAjRzd58802CgoLQaDSkpqZaO5xeITExkeTkZGuH0afIyEgfIVN1Oy/Mx5mxQzzMf4aIEMIsxcXFJCcn8+GHHzJmzBjc3NysHVKv8MEHH2BrK5eCLSHJSB8gU3U7LyrAjZH+8gMpRE/YtWsXY8eOZdasWd1ed1NTE3Z2PfOHVk/WDaDT6Xqs7v5KLtP0Yv+61MSO45VsPnSGqtor2NuqmT3Kj8V3hUkicgtqFcSG6CUREaKHhISEsGzZMvLy8lCpVCQlJXWpvsTERBYtWkRKSgqenp5MmzYNRVFIS0tj6NChODg4EBUVxXvvvdfqe/X19fz617/GyckJPz8/XnvttZsuk3S27vfee4/IyEgcHBzQ6/VMnTrV9CiIjspu3H9jYyNLlizB29sbe3t74uPjOX78+E3Hv2TJEpYuXYpOp8PX15eVK1d22GZ79+4lPj4ed3d39Ho9s2fPpqyszMKW7x1kZKQXapmqe46Pv6huNVV3aoQPzjJV95ZsbVoeee/r1vcXNxQDi6IoXP7xslX27aBxsOies9zcXGJjY1m4cCEPP/wwTk5dX0IjKyuLhQsXcuTIERRF4aWXXuKDDz4gIyODsLAwPvnkEx5++GG8vLxISEgAICUlhSNHjrBr1y58fHxYsWIFn332GdHR0V2qu6qqigcffJC0tDTmzZtHfX09hw4dQlGUDsvasnTpUt5//32ysrIIDAwkLS2NGTNmcPr06VajKFlZWaSkpJCXl0dubi6PPvoocXFxTJs2rc16GxoaSElJITIykoaGBlasWMG8efMoLCxE3ccetqhS2mu9XqSurg43Nzdqa2txdXXt1rrzzpynrKZ3PPRMpup2naOdDYnhXrg7yn00ove7cuUK5eXlBAcHY29vz6XmS9zx9h1WiSXvoTwcbc1/COClS5dwcXHhyJEjTJgwgZqaGh555BGqq6tpbGwkPT2dqVOnml1fYmIitbW1nDhxAmg50Xp6enLw4EFiY2NN2z3++ONcunSJt99+m/r6evR6PW+//Tb33XcfALW1tfj7+/PEE0+Qnp7e6bo/++wzxo4dS0VFBYGBga1i7ajs2v6io6NJT0+noaEBDw8Ptm3bxkMPPQRAc3MzQUFBJCcnm276TUxM5OrVqxw6dMhUz/jx47nrrrtYs2aNWW1YU1ODt7c3RUVFGAwGs77THW7sx9cz9/wtf2b3EjdO1XW11zAz0k+m6lrA3dGWxHAvHO2kWwvR006ePAlAZGQkANu3b2fEiBHs2bMHgMuXLR/hiYmJMf27pKSEK1eu3DQq0NTUxOjRowE4c+YMzc3NjB8/3lTu5uZGeHh4l+uOiopiypQpREZGMmPGDKZPn859992Hh4dHh2U3Kisro7m5mbi4ONNntra2jB8/ntLS0lbbjho1qtV7Pz8/qqur226sn+pevnw5x44d49y5cxiNRgAqKytvazLSHeRX28ouN13l/5WeJa/VqrqeJAzzllV1LeDjqmVimKxELPo2B40DeQ/lWW3fligsLCQ0NNR0eWbcuHG89tprHDlyhKSkJBYtWmRxDNdf6rl2Yv3HP/7BoEGDWm2n1bY8PfnawP6Nf7C1NeBvad02NjZkZ2dz9OhR9u/fz+uvv266RyY4OLjDsrZiaSvGGz+7cQaOSqUyxdqWOXPmEBAQwObNm/H398doNGIwGGhqamr3O72VJCNW0tZU3Qg/V/4tUqbqWipI78gdQ2X9HdH3qVQqiy6VWFNhYSFRUVEAXLhwgVWrVnHq1CkARo8ezeTJkxk5cmSn64+IiECr1VJZWWm6P+RGISEh2Nra8umnnxIQEAC0XBb48ssv2/2OuXVDy3+PuLg44uLiWLFiBYGBgezcuZOUlJQOy64XGhqKnZ0dhw8fbnWZJj8/v0vPIjl//jylpaVs2rSJiRMnAnD48OFO12dtkoxYgUzV7T4R/q5EDZZLWULcboWFhcydOxeAjIwM5s6di6NjSyIVHR3N2bNnu5SMuLi48Pvf/55nn30Wo9FIfHw8dXV1HD16FGdnZ+bPn4+Liwvz588nNTUVnU6Ht7c3L7/8Mmq1usPfBHPqzsvL48CBA0yfPh1vb2/y8vKoqalhxIgRHZbdyMnJiYULF5piHDJkCGlpaVy6dIkFCxZ0un08PDzQ6/VkZmbi5+dHZWUlzz//fKfrszZJRm6jf11qYu+p7zn5jayq21UqFcQEehDm42LtUIQYcIxGI0VFRSxfvhyAEydOsHjxYlN5cXExERE/r1y9bds2HnvssXZnm7TnT3/6E97e3qxevZozZ87g7u7OmDFjePHFF03bvPrqqzz11FPMnj0bV1dXli5dytdff33TjZSW1u3q6sonn3xCeno6dXV1BAYGsm7dOmbOnElpaWm7ZW1Zs2YNRqORpKQk6uvriYmJYd++fW3eY2IutVrNjh07WLJkCQaDgfDwcDZs2EBiYmKn67QmmU1zG2bTtDVVN+anVXVlqq7lNGoVsSF6AnR9YzhbiPZ0NAuhL3nyyScxGAwsXryYrVu3snfvXt59911T+cqVK8nJySEnJ6fHY2loaGDQoEGsW7euSyMPwnwym6aXUxSFkqo6dhfJVN3uotWoSQj3wtNZa+1QhBA/SU1N5Ve/+hVbtmzBYDCQmZnZqnzfvn2sX7++R/Z94sQJ/u///o/x48dTW1vLK6+8AsDdd9/dI/sTPUOSkR5y9qepumXXT9U1+DFK7m/oNGd7DYnhXrjay5oPQvQmoaGh5Ofnt1uem5vbo/v/61//yj//+U/s7OwYO3Yshw4dwtPTs0f3KbqXJCPd7HLTVf7f/50l78zPU3XjwzxJlKm6XaJ3tiNhmJesuiuEaGX06NEUFBRYOwzRRZKMdBOZqttzBnk4EBeiR2MjyZwQQvRHkox0g4pzDfxdpur2iDAfZ8YO8UAts42EEKLfkmSkC2ovN7OnuEqm6vaQqAA3WXVXCCEGAElGOkGm6vYstQruGKon2LPrq4AKIYTo/eTMaYE2p+rqHJkd5c8gmarbLWxtVEwM88LXre8+c0EIIYRlJBkxk0zV7XmOdjYkhnvh7ig3/AohxEAiycgtyFTd28PNwZbEcC+c5DKXEEIMOPLL3w6jopBfcYH9Jd/LVN0e5uOqZWKYlyR3QggxQEky0oaKn1bV/U6m6va4IL0jdwyV2UdCCDGQdepP0Y0bN5oWxLn26N32HD58mLi4OPR6PQ4ODgwfPpzXXnut0wH3pNrLzew4XknmoTN8V3sFe1s1syL9WHxXmCQiPSDC35XYEElEhOjL3nzzTYKCgtBoNKSmpna6nsTERJKTk7svMCvoD8dgLRaPjLzzzjskJyezceNG4uLi2LRpEzNnzqSkpIQhQ4bctL2TkxOLFi1i1KhRODk5cfjwYZ588kmcnJz47W9/2y0H0VXNV40cPn2OnH/KVN3bJSbIg2E+LtYOQwjRBcXFxSQnJ/Phhx8yZswY3NzkuUCicyw+07766qssWLCAxx9/HID09HT27dtHRkYGq1evvmn70aNHM3r0aNP7oKAgPvjgAw4dOmT1ZERRFD4t/4GtR8plqu5tYqOGO0M8CdA5WjsUIUQX7dq1i7FjxzJr1ixrh3JLTU1N2NnJ/X69lUWXaZqamigoKGD69OmtPp8+fTpHjx41q44TJ05w9OhREhIS2t2msbGRurq6Vq/upigKT7xVwLrsL7hwqRlXew33xwTw20lDJRHpIVqNmruG+0giIkQ/EBISwrJly8jLy0OlUpGUlNRtdSuKQlpaGkOHDsXBwYGoqCjee++9Vtvs3buX+Ph43N3d0ev1zJ49m7KyMlN5YmIiixYtIiUlBU9PT6ZNm2b6fMmSJSxduhSdToevry8rV660eP8NDQ088sgjODs74+fnx7p16255XLeKeSCzKBk5d+4cV69excfHp9XnPj4+fP/99x1+d/DgwWi1WmJiYnj66adNIyttWb16NW5ubqZXQECAJWGaRaVSMdLfFVsbFYnhXjw7bRjRAe7yzJAe4myvYdpIH7xctNYORYheS1EUjJcuWeWlKIpFsebm5jJ06FDWrl1LVVUVGzdu7LZ2eOmll/iv//ovMjIyOHXqFM8++ywPP/wwH3/8sWmbhoYGUlJSOH78OAcOHECtVjNv3jyMRqNpm6ysLDQaDUeOHGHTpk2tPndyciIvL4+0tDReeeUVsrOzLdp/amoqH330ETt37mT//v3k5OTccvVgc2IeqDp1Q8SNJ2xFUW55Ej906BAXL17k2LFjPP/884SGhvLggw+2ue0LL7xASkqK6X1dXV2PJCRPJYQQ4uXExcar3V63+JnOyY7EcC/sbW2sHYoQvZpy+TL/HDPWKvsO/6wAlaP5o5bOzs5UVFQQHx+Pr68vNTU13H///VRXV9PY2Eh6ejpTp061OI6GhgZeffVVDh48SGxsLABDhw7l8OHDbNq0yTSqfu+997b63pYtW/D29qakpASDwQBAaGgoaWlpN+1j1KhRvPzyywCEhYXxxhtvcODAAaZNm2bW/i9evMiWLVt46623TCMuWVlZDB48uMNjMyfmgcqiZMTT0xMbG5ubRkGqq6tvGi25UXBwMACRkZGcPXuWlStXtpuMaLVatNqe/wvawc4GH1d7Lv70VFXR/fzd7YkP9URjI88QEaI/OXnyJNDymw6wfft2RowYwZ49ewC4fPlyp+otKSnhypUrppP8NU1NTa3uPywrK2P58uUcO3aMc+fOmUYXKisrTSf2mJiYNvcxatSoVu/9/Pyorq42e/9lZWU0NTWZkhUAnU5HeHh4h8dmTswDlUXJiJ2dHWPHjiU7O5t58+aZPs/Ozubuu+82ux5FUWhsbLRk16IPCvV2JibQA7VM3RXCLCoHB8I/63iovyf3bYnCwkJCQ0NxcmpZ0HLcuHG89tprHDlyhKSkJBYtWtSpOK6doP/xj38waNCgVmXX/5E6Z84cAgIC2Lx5M/7+/hiNRgwGA01NTaZtrsV2I1tb21bvVSqVab/m7N/SS1qWxDxQWXyZJiUlhaSkJGJiYoiNjSUzM5PKykqeeuopoOUSy7fffstbb70FtMxBHzJkCMOHDwdanjvy17/+lcWLF3fjYYjeZtRgNwyDZJqfEJZQqVQWXSqxpsLCQqKiogC4cOECq1at4tSpU0DLLMrJkyczcuRIi+uNiIhAq9VSWVnZ7kSH8+fPU1payqZNm5g4cSLQcm7pDubsPzQ0FFtbW44dO2Z6pMWFCxf44osvrBJzf2BxMvLAAw9w/vx5XnnlFaqqqjAYDOzevZvAwEAAqqqqqKysNG1vNBp54YUXKC8vR6PREBISwpo1a3jyySe77yhEr6FWwR1D9QR7tv0XiRCifygsLGTu3LkAZGRkMHfuXBx/SqSio6M5e/Zsp5IRFxcXfv/73/Pss89iNBqJj4+nrq6Oo0eP4uzszPz58/Hw8ECv15OZmYmfnx+VlZU8//zz3XJc5uzf2dmZBQsWkJqail6vx8fHh2XLlqFWt385uidj7g86dQPr7373O373u9+1WbZt27ZW7xcvXiyjIAOExkbFpDAvfN3srR2KEKIHGY1GioqKWL58OdDyyIbrf+eLi4uJiIgwvd+2bRuPPfaY2Zc3/vSnP+Ht7c3q1as5c+YM7u7ujBkzhhdffBEAtVrNjh07WLJkCQaDgfDwcDZs2EBiYmK3HN+t9g+wdu1aLl68yNy5c3FxceG5556jtra23Tp7Oua+TqV09uLXbVRXV4ebmxu1tbW4urp2a915Z85TJjewdpmDnZrEYd54yCKCQpjtypUrlJeXm5bX6KuefPJJDAYDixcvZuvWrezdu5d3333XVL5y5UpycnLIycmxXpCix3TUj809f8sUB9Flbg62TI/wlUREiAEqNTWVrKwsoqOjOXjwIJmZma3K9+3b1+YUWyGukYVXRJf4uGqZGOaFnUbyWiEGqtDQUPLz89stz83NvY3RiL5IkhHRaYF6RyYMlVV3hRBCdI0kI6JTRvi5yOPzhRBCdAtJRoTFYoI8GObjYu0whBBC9BOSjAiz2ajhzhBPWXVXCCFEt5JkRJhFq1EzaZiXrLorhBCi20kyIm7JSWvD5OHeuNrb3npjIYQQwkKSjIgO6ZzsSAz3wt7WxtqhCCGE6KckGRHt8ne3Jz7UE42NPENECCFEz5FkRLQpxMuJcUE61PIMESGEED1MkhFxk1GD3TAMcrN2GEIIIQYISUaEiVoF44N1DPVytnYoQgghBhC5GUAAoLFRkRDuJYmIEMIib775JkFBQWg0GlJTU2/rvhMTE0lOTm73veg7ZGRE4GCnJnGYt6y6K4SwSHFxMcnJyXz44YeMGTMGNze5vCs6R5KRAc7NwZbEcC+ctNIVhBCW2bVrF2PHjmXWrFnWDkX0cXKZZgDzdtEyNcJbEhEhhMVCQkJYtmwZeXl5qFQqkpKSun0fe/fuJT4+Hnd3d/R6PbNnz6asrKzb9yOsT85CA1Sg3pEJQ/XYyNRdIXoNRVH4sclolX1r7NQWrcKdm5tLbGwsCxcu5OGHH8bJyanbY2poaCAlJYXIyEgaGhpYsWIF8+bNo7CwELVa/pbuTyQZGYBG+LkQHeBu0Q+PEKLn/dhkJPOZj62y79+uT8BWa/6Tlp2dnamoqCA+Ph5fX19qamq4//77qa6uprGxkfT0dKZOndqlmO69995W77ds2YK3tzclJSUYDIYu1S16F0lGBpixgR6E+7pYOwwhRB938uRJACIjIwHYvn07I0aMYM+ePQBcvny5y/soKytj+fLlHDt2jHPnzmE0towaVVZWSjLSz0gyMkDYqOHOEE8CdI7WDkUI0Q6NnZrfrk+w2r4tUVhYSGhoqOnyzLhx43jttdc4cuQISUlJLFq0CICvvvqKp59+mm+++Ybm5mb279/PoEGDzNrHnDlzCAgIYPPmzfj7+2M0GjEYDDQ1NVl2cKLXk2RkALDTqEkY5oWXi9baoQghOqBSqSy6VGJNhYWFREVFAXDhwgVWrVrFqVOnABg9ejSTJ08mLCyMWbNmsXHjRiZNmsQPP/yAq6urWfWfP3+e0tJSNm3axMSJEwE4fPhwzxyMsDpJRvo5J60NieHeuDnYWjsUIUQ/UlhYyNy5cwHIyMhg7ty5ODq2jLxGR0dz9uxZiouLmTBhApMmTQJAp9OZXb+Hhwd6vZ7MzEz8/PyorKzk+eef7/4DEb2C3I7cj+mcbJkx0lcSESFEtzIajRQVFZlGRk6cOMHw4cNN5cXFxURERFBUVMS4ceParGPbtm0d3kSvVqvZsWMHBQUFGAwGnn32WdauXdu9ByJ6DRkZ6af83e2JC/XE1kbyTSFE91Kr1TQ0NJje63Q6Pv/8cyZNmsTWrVsZOXIkvr6++Pj4UFxcDMDVq1epra01jY5UVFSQkNDx/TFTp06lpKSk1WeKopj+nZOT06rsxvei75AzVT8U4uXEpDAvSUSEELdFamoqWVlZREdHc/DgQTIzMwF49NFHKSsrw2AwEBMTw+nTp03f2bdvH2lpadYKWfQyMjLSz4wa7IZhkKwPIYS4fUJDQ8nPz7/pcxcXF3bv3t3md3Jzc3s6LNGHSDLST6hVMD5YJ6vuCiGE6HMkGekHNDYqJoZ54ufmYO1QhBBCCItJMtLHOdipSRzmjYeTnbVDEUIIITpFkpE+zNVBw+RwWXVXCCFE3yZnsT7K20XLxGGeaDV942mNQgghRHskGemDhugciQ3RY6OWVXeFEEL0fZKM9DHD/VwYHeDe4ZMLhRBCiL5EkpE+ZGygB+G+LtYOQwghhOhWkoz0ATZquDPEkwCdo7VDEUIIIbqdJCO9nJ1GzaRhnni72Fs7FCGEEKJHyOIlvZiT1oZpET6SiAgheq0333yToKAgNBoNqamp3Vp3YmIiycnJ3Vqn6J1kZKSX0jnZkjDMGwc7mborhOidiouLSU5O5sMPP2TMmDG4ucm6WKJzJBnphfzc7YkP9ZRVd4UQvdquXbsYO3Yss2bNsnYooo+Ts10vE+LlREKYlyQiQoheLSQkhGXLlpGXl4dKpSIpKanH97l3717i4+Nxd3dHr9cze/ZsysrKTOXvvfcekZGRODg4oNfrmTp1Kg0NDWaVNzY2smTJEry9vbG3tyc+Pp7jx4/3+DGJFjIy0ouMGuyGYZAMcwoxUCmKwo+NjVbZt0artej5Rbm5ucTGxrJw4UIefvhhnJycejC6Fg0NDaSkpBAZGUlDQwMrVqxg3rx5FBYWcvbsWR588EHS0tKYN28e9fX1HDp0CEVRAKiqquqwfOnSpbz//vtkZWURGBhIWloaM2bM4PTp0+h0uh4/toGuU8nIxo0bWbt2LVVVVYwcOZL09HQmTpzY5rYffPABGRkZFBYW0tjYyMiRI1m5ciUzZszoUuD9iVoF44N1DPVytnYoQggr+rGxkQ3z77PKvpdkvYetvfk3yzs7O1NRUUF8fDy+vr7U1NRw//33U11dTWNjI+np6UydOrVbY7z33ntbvd+yZQve3t6UlJTQ1NTEjz/+yD333ENgYCAAkZGRpm2rqqraLW9oaCAjI4Nt27Yxc+ZMADZv3kx2djZbtmzp9htzxc0svhbwzjvvkJyczLJlyzhx4gQTJ05k5syZVFZWtrn9J598wrRp09i9ezcFBQVMnjyZOXPmcOLEiS4H3x9obFQkhHtJIiKE6FNOnjwJ/HxC3759OyNGjKCgoIDi4mLi4uK6fZ9lZWU89NBDDB06FFdXV4KDgwGorKwkKiqKKVOmEBkZyS9/+Us2b97MhQsXTN/tqLysrIzm5uZWMdva2jJ+/HhKS0u7/TjEzSweGXn11VdZsGABjz/+OADp6ens27ePjIwMVq9efdP26enprd7/+c9/5n//93/5+9//zujRozsXdT/hYKcmYZg3Oic7a4cihOgFNFotS7Les9q+LVFYWEhoaKjp8sy4ceN47bXXOHLkCElJSSxatAiAr776iqeffppvvvmG5uZm9u/fz6BBgzoV45w5cwgICGDz5s34+/tjNBoxGAw0NTVhY2NDdnY2R48eZf/+/bz++uume1qCg4M7LL92qebGy1SKosjSG7eJRSMjTU1NFBQUMH369FafT58+naNHj5pVh9FopL6+vsNrcI2NjdTV1bV69TeuDhqmR/hKIiKEMFGpVNja21vlZelJt7CwkKioKAAuXLjAqlWrOHXqFB999BGvv/46p06doqmpiVmzZrF06VIKCws5dOgQPj4+nWqb8+fPU1payksvvcSUKVMYMWJEq5GPa+0XFxfHH//4R06cOIGdnR07d+68ZXloaCh2dnYcPnzYtG1zczP5+fmMGDGiU/EKy1g0MnLu3DmuXr16U2fy8fHh+++/N6uOdevW0dDQwP3339/uNqtXr+aPf/yjJaH1KV4uWiYN80SrkWeICCH6psLCQubOnQtARkYGc+fOxdGxZcmK6Ohozp49S3FxMRMmTGDSpEkAXboR1MPDA71eT2ZmJn5+flRWVvL888+byvPy8jhw4ADTp0/H29ubvLw8ampqTMlER+VOTk4sXLiQ1NRUdDodQ4YMIS0tjUuXLrFgwYJOxyzM16n5o50dytq+fTsrV67knXfewdvbu93tXnjhBWpra02vr7/+ujNh9kpDdI7cNdxbEhEhRJ9lNBopKioyjYycOHGC4cOHm8qLi4uJiIigqKiIcePGtVnHtm3bLBqNUavV7Nixg4KCAgwGA88++yxr1641lbu6uvLJJ5/wb//2bwwbNoyXXnqJdevWmW5IvVX5mjVruPfee0lKSmLMmDGcPn2affv24eHhYXH7CMtZNDLi6emJjY3NTaMg1dXVtxx6e+edd1iwYAH/8z//c8s7rLVaLVoLr1/2BcP9XBgd4C7XIIUQfZparW71/A6dTsfnn3/OpEmT2Lp1KyNHjsTX1xcfHx+Ki4sBuHr1KrW1tabRkYqKChISEjrcT05OTqv3U6dOpaSkpNVn1+73gJbnkLRnxIgRHZbb29uzYcMGNmzY0GFMomdYNDJiZ2fH2LFjyc7ObvV5dnY2d955Z7vf2759O48++ihvv/32gH1S35hAd8YM8ZBERAjR76SmppKVlUV0dDQHDx4kMzMTgEcffZSysjIMBgMxMTGcPn3a9J19+/aRlpZmrZBFL2PxbJqUlBSSkpKIiYkhNjaWzMxMKisreeqpp4CWSyzffvstb731FtCSiDzyyCOsX7+eCRMmmEZVHBwcBsQ6BjZqiB3qyRC9o7VDEUKIHhEaGkp+fv5Nn7u4uLB79+42v5Obm9vTYYk+xOJk5IEHHuD8+fO88sorVFVVYTAY2L17t+khMlVVVa2eObJp0yZ+/PFHnn76aZ5++mnT5/Pnz2fbtm1dP4JezE6jZtIwT1l1VwghhOiASrn+glsvVVdXh5ubG7W1tbi6unZr3XlnzlNW03DrDS3kpLUhMdwbNwfbbq9bCNE/XLlyhfLycoKDg7G34OmnQvQmHfVjc8/fsjZND9A52ZIwzBsHO5kxI4QQQtyKJCPdzM/dnvhQT1l1VwghhDCTJCPdaKiXE+ODdKjVMmNGCCGEMJckI90kcpAbkYP7/+wgIYQQortJMtJFKhWMD9YRIqvuCiGEEJ0iyUgXaGxUTAzzxM/NwdqhCCGEEH2WJCOd5GCnJmGYt6y6K4QQQnSRJCOd4OqgITHcG2etNJ8QQgjRVTL/1EJeLlqmRfhIIiKEEMCbb75JUFAQGo2G1NTUbq07MTGR5OTkbq2zO3VHfDfW0duPuafIGdUCQ3SOxIbosZGpu0IIQXFxMcnJyXz44YeMGTNmQKw3dr0PPvgAW9vufcp2d9eZmJhIdHQ06enp3VZnT5BkxEzhvi6MGeIuq+4KIcRPdu3axdixYwfsauw6na5P1NkdmpqasLPruXsk5TKNGcYEujM20EMSESGE+ElISAjLli0jLy8PlUpFUlJSj+9z7969xMfH4+7ujl6vZ/bs2ZSVlZnK33vvPSIjI3FwcECv1zN16lQaGhrMKm9sbGTJkiV4e3tjb29PfHw8x48f7zCeti6xLFmyhKVLl6LT6fD19WXlypWm8oaGBh555BGcnZ3x8/Nj3bp1t6zTaDTyl7/8hdDQULRaLUOGDGHVqlVmtcmjjz7Kxx9/zPr161GpVKhUKioqKsw63sTERBYtWkRKSgqenp5Mmzatw7boKklGOmCjhvhQT4b7du/ifEII0dfl5uYydOhQ1q5dS1VVFRs3buzxfTY0NJCSksLx48c5cOAAarWaefPmYTQaqaqq4sEHH+Q3v/kNpaWl5OTkcM8993BtLdhblS9dupT333+frKwsPvvsM0JDQ5kxYwY//PCDRTFmZWXh5OREXl4eaWlpvPLKK2RnZwOQmprKRx99xM6dO9m/fz85OTkUFBR0WN8LL7zAX/7yF5YvX05JSQlvv/02Pj4+ZrXJ+vXriY2N5YknnqCqqoqqqioCAgLMPt6srCw0Gg1Hjhxh06ZNFrWDxZQ+oLa2VgGU2trabq/7WNk55W/Hvrrp9T/5Xytn6y53+/6EEOKay5cvKyUlJcrlyy2/NUajUbna+KNVXkaj0aLYGxoaFLVareTm5iqKoijV1dXKL37xC2XMmDHKyJEjlezs7C63T0JCgvLMM8+0W15dXa0ASlFRkVJQUKAASkVFRZvbdlR+8eJFxdbWVvnb3/5m+qypqUnx9/dX0tLSzI4vISFBiY+Pb7XNuHHjlD/84Q9KfX29Ymdnp+zYscNUdv78ecXBweGmOq69r6urU7RarbJ58+Z2Y7jR9W3SVozmHm9CQoISHR1t1j5v7MfXM/f8LfeMtMFJa0NiuDduDt17Y5IQQnREaTby3YqjVtm3/yt3orJgpfGTJ08CEBkZCcD27dsZMWIEe/bsAeDy5cvdHmNZWRnLly/n2LFjnDt3DqPRCEBlZSUzZsxgypQpREZGMmPGDKZPn859992Hh4cHAFFRUe2Wl5WV0dzcTFxcnGlftra2jB8/ntLSUotiHDVqVKv3fn5+VFdXU1ZWRlNTE7GxsaYynU5HeHh4u3WVlpbS2NjIlClTOtUmBoOh3e+Yc7wxMTEdH2w3kss0N9A52TI9wlcSESGE6EBhYSGhoaE4OTkBMG7cOHbu3Mkdd9zBG2+8gYNDy5Opv/rqK2bPnk10dDQjR47k22+/7fQ+58yZw/nz59m8eTN5eXnk5eUBLTdX2tjYkJ2dzZ49e4iIiOD1118nPDyc8vJygA7LlZ8u1dx4X6CiKBbfK3jjTBiVSoXRaDTtwxLX2rAjHbVJe8w93mv/bW8HGRm5jp+bPfFhntjaSI4mhLj9VLZq/F+502r7tkRhYSFRUVEAXLhwgVWrVnHq1CkARo8ezeTJkwkLC2PWrFls3LiRSZMm8cMPP+Dq2rl78M6fP09paSmbNm1i4sSJABw+fLj1MahUxMXFERcXx4oVKwgMDGTnzp2kpKR0WP7kk09iZ2fH4cOHeeihhwBobm4mPz+/2575ERoaiq2tLceOHWPIkCFAS7t98cUXJCQktPmdsLAwHBwcOHDgAI8//nin2sTOzo6rV6/eFEtPH6+lJBn5yVAvJ8YH6VDLM0SEEFaiUqksulRiTYWFhcydOxeAjIwM5s6di6OjIwDR0dGcPXuW4uJiJkyYwKRJk4CuTVv18PBAr9eTmZmJn58flZWVPP/886byvLw8Dhw4wPTp0/H29iYvL4+amhpGjBhxy3InJycWLlxIamoqOp2OIUOGkJaWxqVLl1iwYEGnY76es7MzCxYsIDU1Fb1ej4+PD8uWLUOtbj8JtLe35w9/+ANLly7Fzs6OuLg4ampqOHXqFAsWLLhlmwAEBQWRl5dHRUUFzs7O6HS623K8lpJkBIgc5Ebk4IH1sB4hhOgso9FIUVERy5cvB+DEiRMsXrzYVF5cXExERARvvPEG48aNa7OObdu28dhjj5l9+UKtVrNjxw6WLFmCwWAgPDycDRs2kJiYCICrqyuffPIJ6enp1NXVERgYyLp165g5c6ZZ5WvWrMFoNJKUlER9fT0xMTHs27fPdM9Jd1i7di0XL15k7ty5uLi48Nxzz1FbW9vhd5YvX45Go2HFihV89913+Pn58dRTT5nVJgC///3vmT9/PhEREVy+fJny8nKCgoJuy/FaQqV05kLWbVZXV4ebmxu1tbWdHuJrz8XGH+XR7kIIq7hy5Qrl5eUEBwdjb29v7XA67cknn8RgMLB48WK2bt3K3r17effdd3n99df54osveP3117l69Sq1tbWm0ZGVK1eSk5NDTk6OdYMXXdZRPzb3/D3gb46QREQIIbomNTWVrKwsoqOjOXjwIJmZmUDLQ7fKysowGAzExMRw+vRp03f27dtHWlqatUIWvYyciYUQQnRJaGgo+fn5N33u4uLC7t272/xObm5uT4cl+pABPzIihBBCCOuSZEQIIYQQViXJiBBCCCGsSpIRIYQQQliVJCNCCCGEsCpJRoQQwsr6wOOehGjXtcX5ukKm9gohhJXY2tqiUqmoqanBy8vL4kXZhLAmRVFoamqipqYGtVqNnZ1dp+uSZEQIIazExsaGwYMH880331BRUWHtcIToFEdHR4YMGdLhOju3IsmIEEJYkbOzM2FhYTQ3N1s7FCEsZmNjg0aj6fKoniQjQghhZTY2NtjY9I3VeoXoCXIDqxBCCCGsSpIRIYQQQliVJCNCCCGEsKo+cc/ItTn4dXV1Vo5ECCGEEOa6dt6+1bN0+kQyUl9fD0BAQICVIxFCCCGEperr63Fzc2u3XKX0gUf/GY1GvvvuO1xcXLr1oUB1dXUEBATw9ddf4+rq2m319lfSXuaTtjKftJX5pK3MJ21lvp5sK0VRqK+vx9/fv8PnkPSJkRG1Ws3gwYN7rH5XV1fprBaQ9jKftJX5pK3MJ21lPmkr8/VUW3U0InKN3MAqhBBCCKuSZEQIIYQQVjWgkxGtVsvLL7+MVqu1dih9grSX+aStzCdtZT5pK/NJW5mvN7RVn7iBVQghhBD914AeGRFCCCGE9UkyIoQQQgirkmRECCGEEFYlyYgQQgghrEqSESGEEEJY1YBIRjIyMhg1apTp6XKxsbHs2bPHVK4oCitXrsTf3x8HBwcSExM5deqUFSO2nlu11aOPPopKpWr1mjBhghUj7j1Wr16NSqUiOTnZ9Jn0rba11VbSt1qsXLnypnbw9fU1lUuf+tmt2kr6VGvffvstDz/8MHq9HkdHR6KjoykoKDCVW7NvDYhkZPDgwaxZs4b8/Hzy8/O56667uPvuu02NnJaWxquvvsobb7zB8ePH8fX1Zdq0aaYF+gaSW7UVwC9+8QuqqqpMr927d1sx4t7h+PHjZGZmMmrUqFafS9+6WXttBdK3rhk5cmSrdigqKjKVSZ9qraO2AulT11y4cIG4uDhsbW3Zs2cPJSUlrFu3Dnd3d9M2Vu1bygDl4eGh/Od//qdiNBoVX19fZc2aNaayK1euKG5ubsp//Md/WDHC3uNaWymKosyfP1+5++67rRtQL1NfX6+EhYUp2dnZSkJCgvLMM88oiqJI32pDe22lKNK3rnn55ZeVqKioNsukT7XWUVspivSp6/3hD39Q4uPj2y23dt8aECMj17t69So7duygoaGB2NhYysvL+f7775k+fbppG61WS0JCAkePHrVipNZ3Y1tdk5OTg7e3N8OGDeOJJ56gurrailFa39NPP82sWbOYOnVqq8+lb92svba6RvpWiy+//BJ/f3+Cg4P51a9+xZkzZwDpU21pr62ukT7VYteuXcTExPDLX/4Sb29vRo8ezebNm03l1u5bfWLV3u5QVFREbGwsV65cwdnZmZ07dxIREWFqZB8fn1bb+/j48NVXX1kjVKtrr60AZs6cyS9/+UsCAwMpLy9n+fLl3HXXXRQUFAzIxy7v2LGDzz77jOPHj99U9v333wPSt67pqK1A+tY1d9xxB2+99RbDhg3j7Nmz/Pu//zt33nknp06dkj51g47aSq/XS5+6zpkzZ8jIyCAlJYUXX3yRTz/9lCVLlqDVannkkUes3rcGTDISHh5OYWEh//rXv3j//feZP38+H3/8salcpVK12l5RlJs+Gyjaa6uIiAgeeOAB03YGg4GYmBgCAwP5xz/+wT333GPFqG+/r7/+mmeeeYb9+/djb2/f7nbSt8xrK+lbLWbOnGn6d2RkJLGxsYSEhJCVlWW6+VL6VIuO2iolJUX61HWMRiMxMTH8+c9/BmD06NGcOnWKjIwMHnnkEdN21upbA+YyjZ2dHaGhocTExLB69WqioqJYv3696c7ra1nhNdXV1TdliANFe23VFj8/PwIDA/nyyy9vc5TWV1BQQHV1NWPHjkWj0aDRaPj444/ZsGEDGo3G1H+kb926ra5evXrTdwZy37qek5MTkZGRfPnll/J7dQvXt1VbBnKf8vPzM41wXzNixAgqKysBrN63BkwyciNFUWhsbCQ4OBhfX1+ys7NNZU1NTXz88cfceeedVoyw97jWVm05f/48X3/9NX5+frc5KuubMmUKRUVFFBYWml4xMTH8+te/prCwkKFDh0rf+smt2srGxuam7wzkvnW9xsZGSktL8fPzk9+rW7i+rdoykPtUXFwc//znP1t99sUXXxAYGAhg/b7V47fI9gIvvPCC8sknnyjl5eXKyZMnlRdffFFRq9XK/v37FUVRlDVr1ihubm7KBx98oBQVFSkPPvig4ufnp9TV1Vk58tuvo7aqr69XnnvuOeXo0aNKeXm58tFHHymxsbHKoEGDBmRbteXGGSLSt9p3fVtJ3/rZc889p+Tk5ChnzpxRjh07psyePVtxcXFRKioqFEWRPnW9jtpK+lRrn376qaLRaJRVq1YpX375pfK3v/1NcXR0VP77v//btI01+9aASEZ+85vfKIGBgYqdnZ3i5eWlTJkyxZSIKErLlKaXX35Z8fX1VbRarTJp0iSlqKjIihFbT0dtdenSJWX69OmKl5eXYmtrqwwZMkSZP3++UllZaeWoe48bkxHpW+27vq2kb/3sgQceUPz8/BRbW1vF399fueeee5RTp06ZyqVP/ayjtpI+dbO///3visFgULRarTJ8+HAlMzOzVbk1+5ZKURSl58dfhBBCCCHaNmDvGRFCCCFE7yDJiBBCCCGsSpIRIYQQQliVJCNCCCGEsCpJRoQQQghhVZKMCCGEEMKqJBkRQgghhFVJMiKEEEIIq5JkRAghhBBWJcmIEEIIIaxKkhEhhBBCWNX/B3MVaXi5Hs26AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "o=2\n",
    "lim=2\n",
    "y_lim=[0.7,1.01]\n",
    "plt.plot(nn[lim:],R2.mean(axis=3)[:,lim:,o].T)\n",
    "#plt.ylim(y_lim)\n",
    "plt.legend(['$f_1$','$f_\\delta$, a=1','$f_\\delta$, regression a','$f_\\delta$, learned a','$f_{\\delta c}$, all','$f_{\\delta c}$, lasso','$f_{\\delta c}$, lasso indicator'])\n",
    "for i in range(7):\n",
    "    plt.fill_between(nn[lim:], R2.mean(axis=3)[i,lim:,o]+R2.std(axis=3)[i,lim:,o], R2.mean(axis=3)[i,lim:,o]-R2.std(axis=3)[i,lim:,o],alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d325f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_save = R2.reshape(7,len(nn)*reps*y_train.shape[1])\n",
    "\n",
    "np.savetxt(\"DiscrepR2TrainNVaryDefinitiveAtria.csv\", R2_save.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d731ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
